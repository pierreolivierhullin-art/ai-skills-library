---
name: search-engineering
version: 1.0.0
description: >
  Search engineering pgvector PostgreSQL vector similarity semantic search embeddings,
  Elasticsearch OpenSearch full-text search relevance scoring BM25 query DSL,
  Algolia InstantSearch faceted navigation typo tolerance,
  hybrid search dense sparse vectors RRF reciprocal rank fusion,
  embedding models OpenAI text-embedding cohere open-source,
  search analytics query understanding autocomplete,
  RAG retrieval augmented generation search layer pipeline
---

# Search Engineering â€” Recherche Full-Text, Vectorielle et SÃ©mantique

## Quand Activer ce Skill

Ce skill est pertinent quand :
- Tu intÃ¨gres une barre de recherche dans une application (full-text, facettÃ©e, sÃ©mantique)
- Tu veux utiliser pgvector pour de la recherche par similaritÃ© vectorielle dans PostgreSQL
- Les rÃ©sultats de recherche sont peu pertinents et tu dois amÃ©liorer le ranking
- Tu construis un pipeline RAG (Retrieval-Augmented Generation) pour un chatbot ou assistant
- Tu veux comparer Elasticsearch, Algolia, Typesense ou une solution interne
- Tu as des besoins de recherche hybride combinant lexical et sÃ©mantique

---

## pgvector â€” Recherche Vectorielle dans PostgreSQL

```sql
-- Activer l'extension pgvector
CREATE EXTENSION IF NOT EXISTS vector;

-- Table avec embeddings (1536 dimensions pour text-embedding-3-small)
CREATE TABLE documents (
  id          BIGSERIAL PRIMARY KEY,
  titre       TEXT NOT NULL,
  contenu     TEXT NOT NULL,
  embedding   vector(1536),
  metadata    JSONB DEFAULT '{}',
  crÃ©Ã©_Ã       TIMESTAMPTZ DEFAULT NOW()
);

-- Index HNSW â€” recommandÃ© en production (vitesse > prÃ©cision vs IVFFlat)
CREATE INDEX idx_documents_hnsw
ON documents USING hnsw (embedding vector_cosine_ops)
WITH (m = 16, ef_construction = 64);

-- Index IVFFlat â€” alternative plus Ã©conome en RAM
CREATE INDEX idx_documents_ivfflat
ON documents USING ivfflat (embedding vector_l2_ops)
WITH (lists = 100);  -- RÃ¨gle : sqrt(nombre_lignes)
```

```typescript
import OpenAI from 'openai';

const openai = new OpenAI();

// GÃ©nÃ©rer un embedding
async function gÃ©nÃ©rerEmbedding(texte: string): Promise<number[]> {
  const response = await openai.embeddings.create({
    model: 'text-embedding-3-small',
    input: texte,
    dimensions: 1536,
  });
  return response.data[0].embedding;
}

// Indexer un document
async function indexerDocument(
  titre: string,
  contenu: string,
  metadata: Record<string, unknown> = {}
) {
  const embedding = await gÃ©nÃ©rerEmbedding(contenu);

  await prisma.$executeRaw`
    INSERT INTO documents (titre, contenu, embedding, metadata)
    VALUES (
      ${titre},
      ${contenu},
      ${JSON.stringify(embedding)}::vector,
      ${JSON.stringify(metadata)}::jsonb
    )
  `;
}

// Recherche par similaritÃ© cosinus
async function rechercherSimilaires(
  requÃªte: string,
  limite = 10,
  seuilScore = 0.7
) {
  const queryEmbedding = await gÃ©nÃ©rerEmbedding(requÃªte);

  return prisma.$queryRaw<Array<{
    id: number;
    titre: string;
    contenu: string;
    score: number;
  }>>`
    SELECT
      id,
      titre,
      contenu,
      1 - (embedding <=> ${JSON.stringify(queryEmbedding)}::vector) AS score
    FROM documents
    WHERE 1 - (embedding <=> ${JSON.stringify(queryEmbedding)}::vector) >= ${seuilScore}
    ORDER BY embedding <=> ${JSON.stringify(queryEmbedding)}::vector
    LIMIT ${limite}
  `;
}
```

### Recherche Hybride â€” RRF (Reciprocal Rank Fusion)

```sql
-- Combiner full-text BM25 + similaritÃ© vectorielle avec Reciprocal Rank Fusion
WITH
  -- RÃ©sultats full-text
  fulltext AS (
    SELECT
      id,
      ts_rank_cd(
        to_tsvector('french', titre || ' ' || contenu),
        plainto_tsquery('french', 'intelligence artificielle')
      ) AS score,
      ROW_NUMBER() OVER (
        ORDER BY ts_rank_cd(
          to_tsvector('french', titre || ' ' || contenu),
          plainto_tsquery('french', 'intelligence artificielle')
        ) DESC
      ) AS rang
    FROM documents
    WHERE to_tsvector('french', titre || ' ' || contenu)
      @@ plainto_tsquery('french', 'intelligence artificielle')
    LIMIT 50
  ),
  -- RÃ©sultats vectoriels (embedding de la requÃªte passÃ© en paramÃ¨tre)
  vectoriel AS (
    SELECT
      id,
      1 - (embedding <=> '[0.1, 0.2, ...]'::vector) AS score,
      ROW_NUMBER() OVER (
        ORDER BY embedding <=> '[0.1, 0.2, ...]'::vector
      ) AS rang
    FROM documents
    LIMIT 50
  ),
  -- Fusion RRF : 1/(k + rang) avec k=60 par convention
  rrf AS (
    SELECT
      COALESCE(f.id, v.id) AS id,
      COALESCE(1.0 / (60 + f.rang), 0) +
      COALESCE(1.0 / (60 + v.rang), 0) AS rrf_score
    FROM
      fulltext f
      FULL OUTER JOIN vectoriel v ON f.id = v.id
  )
SELECT d.id, d.titre, r.rrf_score
FROM rrf r
JOIN documents d ON d.id = r.id
ORDER BY rrf_score DESC
LIMIT 10;
```

---

## Elasticsearch / OpenSearch â€” Recherche DistribuÃ©e

```typescript
import { Client } from '@elastic/elasticsearch';

const client = new Client({ node: process.env.ELASTICSEARCH_URL });

// CrÃ©er un index avec mappings et analyseur franÃ§ais
await client.indices.create({
  index: 'produits',
  body: {
    settings: {
      analysis: {
        analyzer: {
          analyseur_fr: {
            type: 'custom',
            tokenizer: 'standard',
            filter: ['lowercase', 'elision_fr', 'stop_fr', 'stemmer_fr'],
          },
        },
        filter: {
          elision_fr: {
            type: 'elision',
            articles_case: true,
            articles: ['l', 'm', 't', 'qu', 'n', 's', 'j', 'd', 'c'],
          },
          stop_fr: { type: 'stop', stopwords: '_french_' },
          stemmer_fr: { type: 'stemmer', language: 'light_french' },
        },
      },
    },
    mappings: {
      properties: {
        nom: {
          type: 'text',
          analyzer: 'analyseur_fr',
          fields: { keyword: { type: 'keyword' } },
        },
        description: { type: 'text', analyzer: 'analyseur_fr' },
        prix: { type: 'float' },
        categorie: { type: 'keyword' },
        tags: { type: 'keyword' },
        note_moyenne: { type: 'float' },
        en_stock: { type: 'boolean' },
      },
    },
  },
});

// RequÃªte full-text avec filtres et agrÃ©gations
const { hits } = await client.search({
  index: 'produits',
  body: {
    query: {
      bool: {
        must: [
          {
            multi_match: {
              query: 'smartphone 5G',
              fields: ['nom^3', 'description'],   // nom a 3Ã— le poids
              type: 'best_fields',
              fuzziness: 'AUTO',                  // TolÃ©rance aux fautes
              operator: 'and',
            },
          },
        ],
        filter: [
          { range: { prix: { gte: 200, lte: 1000 } } },
          { term: { en_stock: true } },
        ],
        should: [
          { range: { note_moyenne: { gte: 4.0, boost: 1.5 } } },
        ],
      },
    },
    aggs: {
      par_categorie: { terms: { field: 'categorie', size: 10 } },
      plage_prix: {
        range: {
          field: 'prix',
          ranges: [
            { key: '< 200â‚¬', to: 200 },
            { key: '200â€“500â‚¬', from: 200, to: 500 },
            { key: '> 500â‚¬', from: 500 },
          ],
        },
      },
    },
    highlight: {
      fields: {
        nom: {},
        description: { fragment_size: 150, number_of_fragments: 2 },
      },
      pre_tags: ['<mark>'],
      post_tags: ['</mark>'],
    },
    from: 0,
    size: 20,
  },
});
```

---

## Algolia â€” Recherche SaaS ClÃ©-en-Main

```typescript
import algoliasearch from 'algoliasearch';
import { liteClient } from 'algoliasearch/lite';
import InstantSearch from 'react-instantsearch';
import { SearchBox, Hits, RefinementList, RangeInput } from 'react-instantsearch';

// Configuration cÃ´tÃ© serveur (admin)
const client = algoliasearch(
  process.env.ALGOLIA_APP_ID!,
  process.env.ALGOLIA_ADMIN_KEY!
);
const index = client.initIndex('produits');

// Configurer le ranking et les attributs
await index.setSettings({
  searchableAttributes: [
    'unordered(nom)',
    'description',
    'tags',
  ],
  attributesForFaceting: [
    'categorie',
    'marque',
    'filterOnly(prix)',
    'filterOnly(en_stock)',
  ],
  customRanking: ['desc(ventes_30j)', 'desc(note_moyenne)'],
  typoTolerance: 'min',
  removeStopWords: ['fr', 'en'],
  queryLanguages: ['fr'],
  distinct: true,
  attributeForDistinct: 'rÃ©fÃ©rence_produit',
});

// Interface React avec InstantSearch
function PageRecherche() {
  const searchClient = liteClient(
    process.env.NEXT_PUBLIC_ALGOLIA_APP_ID!,
    process.env.NEXT_PUBLIC_ALGOLIA_SEARCH_KEY!
  );

  return (
    <InstantSearch indexName="produits" searchClient={searchClient}>
      <SearchBox placeholder="Rechercher un produit..." />
      <RefinementList attribute="categorie" />
      <RangeInput attribute="prix" />
      <Hits hitComponent={({ hit }) => (
        <div>
          <h3>{hit.nom}</h3>
          <p>{hit.description}</p>
          <span>{hit.prix}â‚¬</span>
        </div>
      )} />
    </InstantSearch>
  );
}
```

---

## Pipeline RAG â€” Couche de Recherche pour LLM

```typescript
// Architecture RAG : Retrieve â†’ Augment â†’ Generate
class PipelineRAG {
  constructor(
    private readonly openai: OpenAI,
    private readonly prisma: PrismaClient
  ) {}

  async rÃ©pondre(question: string): Promise<{ rÃ©ponse: string; sources: string[] }> {
    // 1. RÃ©cupÃ©rer les documents pertinents
    const documents = await this.rÃ©cupÃ©rerContexte(question, 5);

    if (documents.length === 0) {
      return {
        rÃ©ponse: "Je n'ai pas trouvÃ© d'informations pertinentes pour rÃ©pondre Ã  cette question.",
        sources: [],
      };
    }

    // 2. Construire le prompt avec le contexte
    const contexte = documents
      .map((d, i) => `[Source ${i + 1}: ${d.titre}]\n${d.contenu}`)
      .join('\n\n---\n\n');

    // 3. GÃ©nÃ©rer la rÃ©ponse
    const rÃ©ponse = await this.openai.chat.completions.create({
      model: 'claude-sonnet-4-6',
      messages: [
        {
          role: 'system',
          content: `Tu es un assistant expert. RÃ©ponds en te basant uniquement sur les sources fournies.
Si la rÃ©ponse n'est pas dans les sources, dis-le clairement. Cite les sources utilisÃ©es.`,
        },
        {
          role: 'user',
          content: `Question : ${question}\n\nSources disponibles :\n${contexte}`,
        },
      ],
      temperature: 0.1,  // RÃ©ponses plus dÃ©terministes
    });

    return {
      rÃ©ponse: rÃ©ponse.choices[0].message.content ?? '',
      sources: documents.map(d => d.titre),
    };
  }

  private async rÃ©cupÃ©rerContexte(question: string, k: number) {
    const embedding = await gÃ©nÃ©rerEmbedding(question);

    return prisma.$queryRaw<Array<{ id: number; titre: string; contenu: string; score: number }>>`
      SELECT id, titre, contenu,
        1 - (embedding <=> ${JSON.stringify(embedding)}::vector) AS score
      FROM documents
      WHERE 1 - (embedding <=> ${JSON.stringify(embedding)}::vector) >= 0.65
      ORDER BY embedding <=> ${JSON.stringify(embedding)}::vector
      LIMIT ${k}
    `;
  }
}
```

---

## MÃ©triques et Tuning de Relevance

```
Signaux de ranking Ã  combiner :
â”œâ”€â”€ Score textuel (BM25, TF-IDF) â€” Pertinence lexicale
â”œâ”€â”€ Score vectoriel (cosinus) â€” Pertinence sÃ©mantique
â”œâ”€â”€ Freshness boost â€” Contenu rÃ©cent favorisÃ©
â”œâ”€â”€ PopularitÃ© â€” Clics, conversions, dwell time
â””â”€â”€ Personnalisation â€” Historique et prÃ©fÃ©rences utilisateur

MÃ©triques d'Ã©valuation :
â”œâ”€â”€ NDCG@K  â€” Normalized Discounted Cumulative Gain
â”‚             Mesure la qualitÃ© du ranking (0 â†’ 1, 1 = parfait)
â”œâ”€â”€ MRR     â€” Mean Reciprocal Rank
â”‚             Position moyenne du premier rÃ©sultat pertinent
â”œâ”€â”€ P@K / R@K â€” PrÃ©cision et rappel parmi les K premiers
â””â”€â”€ CTR     â€” Click-through rate (proxy de satisfaction)

Outils d'Ã©valuation search :
â”œâ”€â”€ Elasticsearch Learning to Rank (LTR plugin)
â”œâ”€â”€ Qdrant / Weaviate â€” benchmarks vectoriels
â”œâ”€â”€ Arize AI / Ragas â€” Ã©valuation RAG
â””â”€â”€ A/B testing de ranking : Optimizely, GrowthBook
```

---

## Comparatif des Solutions

```
              pgvector      Elasticsearch    Algolia       Typesense
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Vectoriel     âœ… Natif       ğŸ”¶ Dense vector  âŒ             ğŸ”¶
Full-text     âœ… PostgreSQL   âœ… BM25 avancÃ©   âœ… PropriÃ©taire âœ…
Facettes      ğŸ”¶ Manuel       âœ… Aggregations  âœ… InstantSearch âœ…
Typo tolerance âŒ            ğŸ”¶ fuzziness     âœ… IntÃ©grÃ©e     âœ…
Infrastructure Self-hosted    Self/Cloud      SaaS           Self/Cloud
CoÃ»t          0 (PostgreSQL) Moyen           Ã‰levÃ© (SaaS)   Faible
IdÃ©al pour    RAG/AI apps    Gros volumes    E-commerce     App B2B
```

---

## RÃ©fÃ©rences

- `references/vector-search.md` â€” pgvector avancÃ© (quantification, filtrage prÃ©/post), modÃ¨les d'embedding comparÃ©s (OpenAI vs Cohere vs open-source), Qdrant, Weaviate, FAISS, chunking strategies pour RAG
- `references/elasticsearch.md` â€” Elasticsearch en profondeur (aggregations, percolator, mapping explosions, snapshot), OpenSearch, Learning to Rank, relevance tuning avancÃ©
- `references/search-ux.md` â€” Algolia InstantSearch avancÃ©, Typesense, autocomplete, search-as-you-type, analytics de recherche, A/B testing de ranking, query understanding NLP
- `references/case-studies.md` â€” 4 cas : migration PostgreSQL + pgvector pour RAG, Elasticsearch 50M docs optimisÃ©, Algolia e-commerce avec personnalisation, recherche hybride production
