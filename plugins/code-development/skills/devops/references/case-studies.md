# Études de cas — DevOps

## Cas 1 : Construction d'une Internal Developer Platform

### Contexte
CloudScale, scale-up SaaS B2B de 120 personnes (40 développeurs répartis en 8 squads), opère 35 microservices sur Kubernetes (EKS) pour une plateforme de gestion documentaire utilisée par 500 entreprises. L'entreprise est positionnée sur le segment mid-market européen avec une croissance de 50% par an, ce qui accélère les besoins en recrutement et en autonomie des squads. Chaque squad gère son propre pipeline CI/CD (mélange de GitHub Actions et Jenkins), ses Helm charts, et ses configurations Terraform. L'équipe plateforme (3 SREs) passe 80% de son temps à répondre aux tickets des développeurs : "mon déploiement a échoué", "comment créer un nouveau service", "je ne trouve pas les logs de staging". Le stack technique comprend EKS, PostgreSQL RDS, Redis ElastiCache, S3, Datadog pour le monitoring, et un mélange hétérogène de pipelines CI hérité de l'évolution organique de la plateforme.

### Problème
Le temps moyen de création d'un nouveau service est de 2 semaines (config K8s, pipeline CI, monitoring, base de données, secrets, DNS, certificates), bloquant le lancement de nouvelles initiatives produit. Les développeurs passent 30% de leur temps sur des tâches d'infrastructure au lieu de coder des features — soit l'équivalent de 12 développeurs à plein temps consacrés à de l'ops. La charge cognitive est énorme : chaque développeur doit maîtriser Terraform, Helm, ArgoCD, Datadog, et les spécificités de 3 environnements (dev, staging, production) avec des configurations divergentes. Les métriques DORA sont médiocres : deployment frequency de 2/semaine par squad, lead time de 5 jours, change failure rate de 18%. Le recrutement est impacté : 3 candidats seniors ont décliné l'offre après avoir vu la complexité de la stack ops pendant l'entretien technique. L'équipe SRE de 3 personnes est en burnout, avec un turnover de 33% sur l'année passée.

### Approche
1. **Backstage comme portail développeur** : Déploiement de Backstage (Spotify) comme Internal Developer Platform après une évaluation de 3 semaines contre Port et Cortex. Catalogue de services unifié avec ownership (chaque service a un owner squad), documentation auto-générée depuis les README, SLOs affichés en temps réel, et graphe de dépendances inter-services. Chaque service est visible et navigable depuis un seul endroit — un développeur peut en 2 clics voir le owner, les SLOs, les derniers déploiements, et les incidents en cours. L'intégration avec Datadog, GitHub et ArgoCD permet d'avoir une vue 360 de chaque service sans naviguer entre 5 outils.
2. **Software Templates** : Création de 4 templates Backstage pour les nouveaux services : "API Service" (Node.js/Fastify avec Prisma), "Worker" (processeur de queue Kafka), "Frontend" (Next.js avec Tailwind), "Data Pipeline" (Python/Dagster). Chaque template provisionne automatiquement : repo GitHub avec CI pre-configuré, pipeline GitHub Actions avec tests, lint et build, Helm chart standardisé, namespace K8s dédié, base de données PostgreSQL (si nécessaire), monitoring Datadog avec dashboards et alertes par défaut, et documentation squelette. Le développeur remplit un formulaire (nom du service, squad owner, type de base de données) et le template génère tout en 30 minutes. Les templates sont versionnés et maintenus par l'équipe plateforme comme des produits internes.
3. **Golden paths** : Définition de "chemins pavés" standardisés pour les opérations courantes : déploiement (merge sur main → build → staging auto → promotion manuelle en prod), rollback (un bouton dans Backstage), feature flags (LaunchDarkly intégré dans les templates), database migration (script automatisé avec expand-contract), secrets rotation (AWS Secrets Manager avec rotation automatique). Les développeurs suivent le golden path par défaut et ne dévient que s'ils acceptent la charge de maintenance — les déviations sont trackées dans Backstage avec un score de "platform alignment". L'adoption des golden paths est mesurée hebdomadairement et les frictions remontées sont traitées comme des bugs du produit plateforme.
4. **GitOps avec ArgoCD** : Centralisation de tous les manifestes K8s dans un monorepo GitOps (`infra-manifests`). ArgoCD réconcilie automatiquement l'état du cluster avec le repo — si un développeur modifie manuellement un déploiement K8s, ArgoCD le revert en 30 secondes. Les développeurs mergent un PR pour déployer — plus de `kubectl apply` manuel, plus de credentials K8s sur les postes développeurs. Chaque environnement (dev, staging, prod) a son propre directory dans le repo avec des overlays Kustomize. Le pipeline de promotion staging → prod est un PR automatisé par le template, validé par le squad lead.

### Résultat
- Temps de création d'un nouveau service passé de 2 semaines à 30 minutes (template Backstage), avec les 4 premiers services créés par des développeurs juniors sans aide SRE
- Temps développeur sur l'infra réduit de 30% à 8% (libère l'équivalent de 9 développeurs-jours/semaine pour les features)
- DORA metrics transformées : deployment frequency 12/semaine par squad (x6), lead time 1.2 jours (divisé par 4), change failure rate 6% (divisé par 3)
- Tickets infra pour l'équipe plateforme réduits de 75% — les SREs passent 60% de leur temps sur des projets d'amélioration au lieu de répondre à des tickets
- Onboarding d'un nouveau développeur : premier déploiement en production en 2 jours (vs 3 semaines auparavant), mesuré sur les 8 derniers recrutements
- Le score de satisfaction développeur (survey interne) est passé de 4/10 à 8.5/10 sur les questions liées à l'infrastructure

### Leçons apprises
- Le platform engineering n'est pas un projet mais un produit — il faut l'itérer avec du feedback développeur, comme n'importe quel produit. Chez CloudScale, l'équipe plateforme a un backlog priorisé par les votes des développeurs et fait des "user interviews" mensuelles avec les squads.
- Commencer par les golden paths avant de déployer un portail — la valeur vient de la standardisation, pas de l'outil. Chez CloudScale, les golden paths étaient fonctionnels 2 mois avant Backstage, via de la documentation et des scripts. Backstage n'a fait que rendre le tout découvrable.
- L'adoption se fait par la valeur, pas par le mandat — si le golden path est plus rapide que le DIY, les développeurs l'adoptent naturellement. Chez CloudScale, le taux d'adoption des templates est de 95% — les 5% restants sont des cas légitimes nécessitant une configuration custom.
- Le monorepo GitOps est un game changer pour la cohérence — mais il nécessite un investissement initial significatif en tooling (validation des manifestes en CI, preview des changements, protection du main branch). Sans ce tooling, le repo devient un point de friction au lieu d'un accélérateur.

---

## Cas 2 : Pipeline CI/CD zero-downtime pour une app critique

### Contexte
PayEase, fintech de 50 personnes, traite 200K transactions/jour via une API déployée sur AWS ECS Fargate. L'entreprise est régulée par l'ACPR et soumise à des obligations de disponibilité contractuelles (SLA de 99.95%). Le pipeline CI/CD utilise GitHub Actions avec un déploiement direct en production après les tests unitaires et d'intégration. L'application est un monolithe Node.js (Express) avec une base PostgreSQL (RDS) et Redis pour le caching. L'équipe de 12 développeurs déploie via un workflow GitHub Actions qui build l'image Docker, pousse sur ECR, et met à jour le service ECS avec un rolling update. Les migrations de base de données sont exécutées au démarrage de l'application via un script intégré à l'entrypoint Docker.

### Problème
3 incidents de production en 2 mois directement liés aux déploiements : une migration de base qui ajoute un index sur une table de 50M lignes locke les tables pendant 8 minutes (toutes les transactions en timeout, 45K EUR de perte estimée), un bug de régression non détecté par les tests déployé à 100% des utilisateurs simultanément (1h20 de latence dégradée avant détection), et un rollback qui prend 25 minutes car il nécessite un revert de migration impossible sans perte de données. Le taux d'échec des déploiements est de 15%, générant un stress significatif pour l'équipe. Ce stress conduit l'équipe à ne déployer que les mardis et jeudis ("deploy days"), créant des batchs de 15-20 changements par déploiement — ce qui augmente paradoxalement le risque. Le VP Engineering constate que les développeurs évitent de déployer leurs changements, accumulant du code non déployé pendant des jours.

### Approche
1. **Blue-green deployments** : Migration de ECS rolling update vers un déploiement blue-green avec ALB (Application Load Balancer) et deux target groups. Le nouveau code est déployé sur un nouveau target group ("green"), validé par des smoke tests automatiques (15 scénarios critiques : création de transaction, vérification de solde, consultation d'historique), puis le trafic bascule instantanément via un changement de listener rule ALB. Rollback en 30 secondes (rebascule vers "blue") sans aucun impact — l'ancien code reste actif et sain pendant 24h avant d'être décommissionné. Le temps de déploiement total (build + deploy + smoke tests + switch) est de 8 minutes.
2. **Zero-downtime migrations** : Adoption du pattern "expand and contract" pour les migrations de base, formalisé dans un ADR et un template de migration : Phase 1 (expand) — ajouter la nouvelle colonne/table/index sans supprimer l'ancienne, avec `CREATE INDEX CONCURRENTLY` pour éviter les locks ; Phase 2 (migrate) — migrer les données en batch (1000 lignes à la fois) dans un script séparé ; Phase 3 (contract) — supprimer l'ancien schéma uniquement après validation en production pendant 1 semaine. Chaque phase est un déploiement séparé, espacé d'au moins 24h. Un linter de migration (squawk-cli) bloque en CI toute migration qui prend un lock exclusif sur une table.
3. **Canary analysis automatisée** : Avant la bascule blue-green complète, 5% du trafic est envoyé au "green" pendant 10 minutes via un weighted target group. Un job GitHub Actions compare les métriques (error rate, latence p95, status codes 5xx, taux de succès des transactions) entre blue et green via l'API Datadog. Si les métriques divergent de >10% ou si le error rate dépasse 0.5%, rollback automatique sans intervention humaine. Le canary a détecté 3 régressions en 4 mois qui auraient autrement impacté 100% des utilisateurs.
4. **Feature flags systématiques** : Intégration de LaunchDarkly dans le template de base de chaque feature. Chaque nouvelle feature derrière un flag, activable par pourcentage d'utilisateurs, par segment, ou par tenant. Les déploiements ne changent que le code disponible — l'activation se fait séparément via le dashboard LaunchDarkly. Rollback d'une feature en 1 clic sans redéploiement, en < 5 secondes. Les flags sont automatiquement nettoyés (supprimés du code) 30 jours après activation à 100%, via un bot qui ouvre une PR de cleanup.

### Résultat
- Zéro downtime sur les 6 derniers mois (vs 3 incidents en 2 mois avant), respectant le SLA de 99.95% avec une marge confortable (99.98% mesuré)
- Taux d'échec de déploiement réduit de 15% à 2%, les 2% restants étant détectés par le canary et rollbackés automatiquement
- Temps de rollback passé de 25 minutes à 30 secondes (blue-green switch), éliminant le stress associé aux déploiements
- Fréquence de déploiement passée de 2/semaine à 3/jour — les développeurs déploient avec confiance dès que leur code est prêt
- Lead time réduit de 5 jours à 4 heures (du commit au production), avec des features livrées aux clients en heures au lieu de semaines
- Migrations de base : zéro lock de table en production grâce au pattern expand-contract et au linter squawk-cli

### Leçons apprises
- Le blue-green deployment est le meilleur investissement pour la confiance en production — le rollback instantané change la psychologie de l'équipe. Chez PayEase, les développeurs sont passés de "j'ai peur de déployer" à "je déploie 2 fois par jour". Ce changement culturel est le bénéfice le plus impactant.
- Les migrations zero-downtime exigent de la discipline (3 phases au lieu d'1, minimum 24h entre chaque phase) mais éliminent la classe entière de bugs de migration. Le linter squawk-cli est le garde-fou qui rend cette discipline automatique — sans lui, les développeurs pressés contournent le process.
- Les feature flags découplent le déploiement technique de la release fonctionnelle — c'est le catalyseur du "deploy small, deploy often". Attention cependant à la dette technique des flags non nettoyés : chez PayEase, 40 flags abandonnés ont été retrouvés lors d'un audit, d'où le bot de cleanup automatique.
- Le canary analysis automatisé est le filet de sécurité ultime — il détecte les régressions que les tests ne couvrent pas (problèmes de performance, edge cases spécifiques à certains tenants, interactions avec des services tiers). Chez PayEase, 3 incidents potentiels sur 4 mois ont été évités grâce au canary.

---

## Cas 3 : Sécurisation de la supply chain logicielle

### Contexte
SecureOps, éditeur SaaS de compliance (30 personnes, 8 développeurs), aide les entreprises à gérer leur conformité réglementaire (SOC 2, ISO 27001, RGPD). L'entreprise est elle-même certifiée SOC 2 Type II et vise la certification ISO 27001 pour répondre aux exigences de ses clients enterprise qui représentent 60% du revenu. L'application utilise 450 dépendances npm, 30 images Docker, et déploie via GitHub Actions sur AWS EKS. Le pipeline CI exécute des tests unitaires et d'intégration mais aucun scan de sécurité — un comble pour un éditeur de compliance. L'équipe de 8 développeurs utilise des images Docker basées sur Ubuntu 22.04 "par habitude" et stocke des credentials AWS sous forme de secrets GitHub Actions longue durée (créés il y a 2 ans, jamais rotés).

### Problème
Un audit ISO 27001 pré-certification conduit par un auditeur externe révèle des non-conformités critiques qui bloquent la certification : aucune traçabilité de la chaîne de build (impossible de prouver que l'image en production correspond au code audité — un pré-requis ISO 27001 Annex A.8.25), 23 vulnérabilités CVE critiques et hautes dans les dépendances npm (dont 3 avec des exploits publics), des images Docker basées sur Ubuntu 22.04 avec 150+ packages inutiles (surface d'attaque estimée à 3x celle d'une image minimale), et des credentials AWS longue durée dans GitHub Actions pour l'accès au cluster EKS (accès root-level, jamais rotés, utilisés par tous les workflows). L'auditeur note que "l'ironie d'un éditeur de compliance non-conforme à ses propres standards est un risque réputationnel majeur". Le délai pour corriger et repasser l'audit est de 4 mois.

### Approche
1. **SLSA Level 2 provenance** : Intégration de Sigstore (cosign) dans le pipeline GitHub Actions. Chaque image Docker est signée avec une clé éphémère via Fulcio (certificate authority keyless) et une attestation de provenance SLSA Level 2 est générée et stockée dans Rekor (transparency log). La provenance lie cryptographiquement l'image au commit Git, au pipeline GitHub Actions, et au runner de build. Un admission controller Kubernetes (Kyverno) vérifie la signature et la provenance avant tout déploiement — une image non signée ou dont la provenance ne correspond pas au pipeline officiel est rejetée. Cette chaîne de confiance permet à l'auditeur de vérifier que chaque image en production correspond exactement au code audité.
2. **Scanning automatisé en CI** : Trivy scanne les images Docker (vulnérabilités OS + application) et les manifestes IaC (Terraform, Kubernetes), Snyk analyse les dépendances npm (`snyk test` + `snyk monitor` pour le suivi continu), et Checkov valide les configurations Terraform contre les benchmarks CIS. Quality gate : zéro CVE critique ou haute autorisée, avec une exception process documenté pour les faux positifs (review + approval par le tech lead). Dependabot activé avec auto-merge pour les patches de sécurité (minor et patch versions) après validation CI — les updates majeures nécessitent une review manuelle. Un rapport de vulnérabilités hebdomadaire est généré et envoyé au RSSI.
3. **Images minimales** : Migration de toutes les images Docker de Ubuntu 22.04 vers des images distroless (Google) pour les services Node.js et Alpine pour les outils nécessitant un shell. Les images passent de 800MB à 45MB — une réduction de 94% de la taille. La surface d'attaque est réduite de 90% (moins de packages = moins de CVEs potentielles) : l'image Ubuntu contenait 1200+ packages installés dont 150+ avec des CVEs connues, l'image distroless en contient 12. Le Dockerfile est standardisé en multi-stage build : stage de build (Node.js + npm) pour la compilation, stage final (distroless) avec uniquement le binaire et les dépendances runtime.
4. **OIDC pour l'authentification cloud** : Remplacement de tous les AWS access keys dans GitHub Actions par l'OIDC federation (GitHub → AWS STS). Zéro credential longue durée stockée dans les secrets GitHub — chaque job de pipeline obtient un token temporaire (1h max) avec des permissions minimales via des rôles IAM dédiés par workflow (build, deploy-staging, deploy-prod). Le rôle deploy-prod est conditionné à la branche main et au passage de tous les quality gates. Cette migration a nécessité la création de 5 rôles IAM avec des policies least-privilege, testées avec IAM Access Analyzer pour vérifier qu'aucune permission excessive n'est accordée.

### Résultat
- Certification ISO 27001 obtenue dès le second audit — la traçabilité de la supply chain (SLSA + Sigstore) est citée comme point fort dans le rapport d'audit
- CVE critiques/hautes réduites de 23 à 0 (et maintenues à 0 via le scanning continu et Dependabot auto-merge), avec un temps moyen de remédiation de 48h pour les nouvelles CVEs
- Taille des images Docker réduite de 800MB à 45MB (divisé par 18) — les déploiements sont 4x plus rapides (pull d'image en 3s vs 12s)
- Zéro credential longue durée dans le CI — suppression d'un vecteur d'attaque majeur, validé par un pentest qui a spécifiquement testé l'exfiltration de secrets CI
- Chaque image en production est traçable jusqu'au commit source avec preuve cryptographique, consultable via `cosign verify` par n'importe quel auditeur
- L'argument "supply chain sécurisée" est devenu un avantage commercial : 5 prospects enterprise ont cité la certification ISO 27001 comme facteur de décision

### Leçons apprises
- La sécurité de la supply chain est devenue un prérequis de certification (ISO 27001, SOC 2), pas un "nice-to-have" — intégrer dès le premier pipeline. Le coût de la mise en conformité a posteriori (4 mois chez SecureOps) est 10x supérieur à celui d'une intégration dès le départ.
- Les images distroless/Alpine divisent la surface d'attaque ET le temps de déploiement — c'est un double bénéfice sans compromis. La seule difficulté est le debugging en production (pas de shell dans distroless) — prévoir des ephemeral containers K8s pour le diagnostic.
- L'OIDC federation élimine le problème de rotation des credentials cloud en CI — c'est la pratique standard qui devrait remplacer toute utilisation d'access keys. Chez SecureOps, les anciennes access keys avaient 2 ans et un accès root — une bombe à retardement désamorcée.
- Le scanning automatisé ne suffit pas sans un process de remédiation clair — chez SecureOps, les 23 CVEs existaient depuis des mois, détectées par des audits manuels mais jamais priorisées. La quality gate CI qui bloque le merge est le seul mécanisme qui force la remédiation avant qu'elle ne s'accumule.
