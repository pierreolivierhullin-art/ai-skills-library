# Études de cas — AI Engineering

## Cas 1 : Cursor — Éditeur de code augmenté par l'IA avec RAG sur codebase

### Contexte
Cursor, startup fondée en 2023, développe un éditeur de code basé sur VS Code qui intègre nativement l'IA pour l'autocomplétion, la génération de code et la conversation contextuelle. L'éditeur cible les développeurs professionnels travaillant sur des codebases de 100K à 10M+ lignes de code, dans tous les langages. L'équipe technique (environ 30 personnes) doit résoudre un problème fondamental : comment fournir des suggestions de code pertinentes quand le contexte utile est distribué dans des milliers de fichiers, alors que les modèles de langage ont une fenêtre de contexte limitée et que chaque token envoyé a un coût. Le produit sert des centaines de milliers de développeurs actifs quotidiens, générant des millions de requêtes d'autocomplétion par heure avec une contrainte de latence de moins de 300ms pour rester invisible dans le flux de travail du développeur.

### Problème
Le défi central de Cursor est un problème de RAG sur codebase à grande échelle avec des contraintes de latence extrêmes. Un développeur qui écrit du code dans un fichier a besoin que le modèle comprenne : les types et interfaces définis ailleurs dans le projet, les patterns et conventions du codebase, les dépendances et APIs utilisées, le contexte métier exprimé dans les noms de variables et les commentaires. Envoyer tout le codebase dans le prompt est impossible (un projet de 500K lignes dépasse toute fenêtre de contexte), mais sélectionner les mauvais fichiers produit des suggestions hors-sujet qui érodent la confiance du développeur. Le retrieval doit se faire en moins de 50ms pour ne pas impacter la latence perçue. De plus, les codebases évoluent en permanence — chaque sauvegarde de fichier invalide potentiellement l'index.

### Approche
1. **RAG multi-signal sur codebase** : Cursor construit un index hybride combinant plusieurs signaux de retrieval. L'indexation sémantique via embeddings capture la similarité conceptuelle entre fragments de code. L'analyse statique du langage (imports, définitions de types, appels de fonctions) fournit un graphe de dépendances structurel. La récence et la fréquence d'accès aux fichiers pondèrent les résultats vers le code activement travaillé par le développeur. Le chunking est adapté au code : découpage par fonctions, classes et blocs logiques plutôt que par taille fixe, préservant la cohérence sémantique de chaque fragment.
2. **Model routing par tâche** : Cursor implémente un routeur de modèles sophistiqué. L'autocomplétion inline (le cas d'usage le plus fréquent, des millions de requêtes/heure) utilise un modèle rapide et économique entraîné spécifiquement pour la complétion de code, avec une latence cible de moins de 150ms. Les requêtes de chat et les éditions complexes sont routées vers des modèles frontier (Claude Sonnet, GPT-4o) qui offrent un raisonnement supérieur. Les tâches de refactoring à grande échelle utilisent des modèles avec de longues fenêtres de contexte. Le routage est déterminé par le type d'interaction, pas par un classifieur : chaque surface UI est liée à un modèle prédéfini.
3. **Prompt caching agressif** : Les instructions système et le contexte du projet (fichiers ouverts, structure du répertoire, configurations) constituent souvent 80% des tokens envoyés et changent rarement entre deux requêtes consécutives. Cursor exploite le prompt caching (prefix caching) des providers pour réduire les coûts et la latence. Le contexte du projet est organisé en préfixe stable suivi du contexte variable (position du curseur, code récemment tapé), maximisant le taux de cache hit. Les économies de coût atteignent 50 à 90% sur les requêtes d'autocomplétion.
4. **Speculative decoding et streaming** : Pour l'autocomplétion, Cursor utilise des techniques de génération spéculative — le modèle rapide génère plusieurs candidats en parallèle, et les résultats sont affichés en streaming dès les premiers tokens disponibles. Le développeur voit la suggestion apparaître progressivement, réduisant la latence perçue même quand la génération complète prend plus de temps.

### Résultat
- Latence d'autocomplétion maintenue sous 300ms au p95, même sur des codebases de plusieurs millions de lignes
- Taux d'acceptation des suggestions supérieur à 30%, indiquant une pertinence élevée du retrieval contextuel
- Coût par suggestion maintenu à une fraction de centime grâce au model routing (modèle rapide pour l'autocomplétion) et au prompt caching (réduction de 50-90% des tokens facturés)
- Indexation incrémentale du codebase en temps réel — chaque sauvegarde de fichier met à jour l'index en moins de 100ms, garantissant que le retrieval reflète toujours l'état actuel du code
- Adoption massive : plusieurs centaines de milliers de développeurs actifs quotidiens, validant que l'approche RAG sur codebase produit une valeur perçue supérieure aux assistants de code sans contexte projet

### Leçons apprises
- Le model routing est la clé de la viabilité économique d'un produit IA à haute fréquence — utiliser un modèle frontier pour chaque autocomplétion rendrait le coût prohibitif. Router le bon modèle vers la bonne tâche (rapide et économique pour la complétion, puissant pour le raisonnement complexe) est un pattern fondamental de l'AI engineering en production.
- Le RAG sur du code nécessite un chunking spécialisé — les stratégies de chunking par taille fixe qui fonctionnent pour du texte naturel produisent des fragments incohérents quand ils coupent au milieu d'une fonction ou d'une classe. Le chunking structure-aware (par fonction, classe, bloc) est indispensable pour le code.
- Le prompt caching transforme l'économie des produits IA conversationnels — quand le préfixe du prompt (instructions système + contexte projet) représente 80% des tokens et change rarement, le caching réduit les coûts d'un ordre de grandeur. Concevoir l'architecture des prompts pour maximiser le préfixe stable est un investissement à haut rendement.
- La latence perçue est aussi importante que la latence réelle — le streaming et la génération spéculative permettent d'afficher des résultats partiels immédiatement, rendant une génération de 500ms perceptivement instantanée pour le développeur.

---

## Cas 2 : Notion AI — Intégration IA dans un outil de productivité à grande échelle

### Contexte
Notion, plateforme de productivité utilisée par plus de 100 millions d'utilisateurs, intègre des fonctionnalités IA depuis 2023 : rédaction assistée, résumés de pages, Q&A sur l'ensemble d'un workspace, traduction, et extraction d'informations structurées. L'équipe IA (environ 20-30 personnes au sein d'une engineering org de 300+) doit servir des fonctionnalités IA sur des données extrêmement hétérogènes : documents texte, tableaux de bases de données, kanban boards, wikis, fichiers uploadés, le tout organisé en arborescences de pages imbriquées avec des permissions granulaires. Le volume de données par workspace varie de quelques pages pour un utilisateur individuel à des millions de blocs de contenu pour une entreprise de 10 000 personnes.

### Problème
Le défi d'ingénierie IA de Notion est multi-dimensionnel. Premièrement, le RAG sur workspace : quand un utilisateur pose une question comme "quel est le processus d'onboarding ?", le système doit retrouver l'information pertinente parmi potentiellement des millions de pages, en respectant les permissions de l'utilisateur (un stagiaire ne doit pas recevoir de réponses basées sur des documents RH confidentiels). Deuxièmement, la diversité des formats : le contenu Notion n'est pas du texte plat — il contient des tableaux, des relations entre bases de données, des blocs de code, des embeds, des toggles imbriqués. Le chunking et l'embedding doivent préserver la structure sémantique de ces formats hétérogènes. Troisièmement, le coût à l'échelle : avec 100M+ d'utilisateurs, chaque optimisation de coût par requête a un impact massif sur la marge. Le coût IA doit rester soutenable sans compromettre la qualité.

### Approche
1. **Stratégie multi-modèles** : Notion utilise une approche multi-provider et multi-modèles, incluant Anthropic (Claude) et OpenAI. Les tâches simples (reformulation, correction grammaticale, traduction) sont routées vers des modèles rapides et économiques. Les tâches complexes (Q&A sur workspace, synthèse de documents longs, extraction de données structurées) utilisent des modèles frontier. Le routage est déterminé par le type de fonctionnalité activée par l'utilisateur plutôt que par un classifieur dynamique, simplifiant la logique et rendant le comportement prédictible.
2. **RAG avec permissions intégrées** : Le pipeline RAG de Notion indexe le contenu par workspace avec des metadata de permissions attachées à chaque chunk. Lors du retrieval, un filtre de permissions est appliqué avant le ranking — seuls les chunks accessibles à l'utilisateur courant sont considérés. L'indexation est incrémentale : chaque modification de page met à jour les embeddings correspondants via un système de queues asynchrones. Le chunking respecte la structure Notion : un bloc de type "table" est chunké différemment d'un bloc "paragraph", et les relations entre bases de données sont encodées dans les metadata pour préserver le contexte structurel.
3. **Structured output pour le formatage** : Les réponses de l'IA doivent s'intégrer nativement dans l'éditeur Notion — pas du texte brut, mais des blocs Notion formatés (headings, bullet lists, tables, toggles, callouts). Notion utilise le structured output (JSON schema contraint) pour que le modèle génère directement des blocs au format Notion. Un layer de validation vérifie que la structure JSON est conforme au schéma de blocs Notion avant l'insertion dans la page. En cas d'échec de validation, un retry avec un prompt corrigé est déclenché automatiquement.
4. **Gestion des coûts à l'échelle** : Notion implémente plusieurs mécanismes de cost management : quotas d'utilisation par plan (gratuit, Plus, Business, Enterprise), prompt caching pour les instructions système et les contextes workspace récurrents, batching des requêtes d'indexation pour amortir les coûts d'embedding, et monitoring granulaire du coût par feature et par tier de plan. Les fonctionnalités IA les plus coûteuses (Q&A sur tout le workspace) sont réservées aux plans payants, alignant le coût avec la valeur perçue et le revenu généré.

### Résultat
- Q&A sur workspace déployé pour des millions d'utilisateurs, avec un retrieval qui respecte les permissions en temps réel — aucune fuite de données cross-permission documentée
- Temps de réponse maintenu sous 3 secondes pour le Q&A sur workspace, même pour des workspaces contenant plusieurs centaines de milliers de pages
- Structured output permettant une intégration native dans l'éditeur — les réponses IA sont indistinguables du contenu créé manuellement, préservant l'expérience utilisateur cohérente
- Coût IA maîtrisé grâce au model routing : les tâches simples (80% du volume) coûtent une fraction des tâches complexes, rendant l'offre IA économiquement viable même sur le plan gratuit avec des limites d'usage
- Adoption mesurée par le taux d'engagement : les utilisateurs avec accès aux fonctionnalités IA montrent une rétention significativement supérieure

### Leçons apprises
- Le RAG avec permissions est un défi d'ingénierie majeur souvent sous-estimé — le retrieval sémantique standard ne gère pas les ACLs, et ajouter un filtre de permissions après le retrieval peut vider le résultat ou dégrader la qualité. Intégrer les permissions dès l'indexation et le filtering pré-retrieval est la seule approche scalable.
- Le structured output est indispensable quand l'IA doit produire du contenu qui s'intègre dans un format applicatif spécifique. Le texte libre nécessite un parsing fragile et produit des résultats inconsistants. Le JSON schema contraint élimine cette classe d'erreurs et rend le pipeline déterministe.
- La gestion des coûts IA doit être conçue dès le départ, pas ajoutée après le lancement. Chez Notion, l'alignement entre les tiers de pricing et les niveaux de fonctionnalités IA permet de couvrir les coûts d'inférence tout en offrant une expérience gratuite limitée qui sert l'acquisition.
- L'indexation incrémentale en temps réel est critique pour les outils de productivité — un utilisateur qui modifie une page et pose une question 30 secondes plus tard s'attend à ce que la réponse reflète la modification. Un index batch mis à jour toutes les heures crée un décalage perceptible qui érode la confiance.

---

## Cas 3 : Perplexity — Moteur de recherche IA-native avec RAG en temps réel

### Contexte
Perplexity, fondée en 2022, a construit un moteur de recherche qui remplace le paradigme "liste de liens" par des réponses synthétisées avec citations. Le produit traite des dizaines de millions de requêtes par jour, chacune déclenchant un pipeline RAG complet : recherche web en temps réel, extraction de contenu, synthèse par LLM, et grounding avec citations vérifiables. L'équipe technique (environ 50 personnes) opère sous des contraintes de latence sévères — l'utilisateur attend une réponse complète en moins de 5-10 secondes, alors que le pipeline traverse plusieurs étapes coûteuses (recherche web, scraping, embedding, retrieval, génération). Le produit est disponible en mode gratuit (limité) et Pro (abonnement), nécessitant une optimisation agressive des coûts pour maintenir la viabilité économique.

### Problème
Le défi central de Perplexity est le RAG en temps réel sur le web ouvert, qui diffère fondamentalement du RAG sur corpus statique. Les documents ne sont pas pré-indexés — ils doivent être trouvés, téléchargés, parsés et compris en quelques secondes. La qualité des sources web est extrêmement variable : articles de qualité, pages marketing, contenu obsolète, désinformation, pages mal formatées. L'hallucination est le risque numéro un — une réponse qui cite une source mais déforme son contenu détruit la crédibilité du produit. Le grounding (ancrage des affirmations dans des sources vérifiables) doit être rigoureux, car l'utilisateur peut cliquer sur chaque citation pour vérifier. La latence est un enjeu compétitif : chaque seconde supplémentaire augmente le taux d'abandon face aux moteurs de recherche classiques qui répondent en moins d'une seconde.

### Approche
1. **Pipeline RAG parallélisé** : Perplexity décompose le pipeline en étapes parallélisées pour minimiser la latence totale. Dès la réception de la requête, le query understanding (reformulation, décomposition en sous-questions) et la recherche web sont lancés en parallèle. Les résultats de recherche sont scrapés et parsés en concurrence. Le retrieval sémantique sur les contenus extraits est effectué pendant que le modèle commence déjà à générer les premières parties de la réponse en streaming. Le résultat visible par l'utilisateur est un flux progressif : les sources apparaissent d'abord, puis la réponse se construit mot par mot avec les citations insérées en temps réel.
2. **Citation grounding et vérification factuelle** : Chaque affirmation dans la réponse générée est associée à une citation numérotée renvoyant à une source spécifique. Le modèle est instruit via le prompt système pour ne générer que des affirmations supportées par les sources retrievées, et pour indiquer explicitement quand l'information est insuffisante plutôt que d'inventer. Un mécanisme de post-processing vérifie la cohérence entre les citations et le contenu des sources. Les réponses qui ne parviennent pas à s'ancrer dans des sources fiables sont signalées ou reformulées.
3. **Multi-step reasoning pour les questions complexes** : Pour les requêtes nécessitant une synthèse de plusieurs sources ou un raisonnement multi-étapes, Perplexity implémente un pipeline en plusieurs passes. La première passe identifie les sous-questions, la seconde recherche et synthétise les réponses partielles, la troisième compose la réponse finale intégrée. Ce pattern "agentic RAG" est réservé aux requêtes complexes détectées par un classifieur en amont — les requêtes simples suivent le pipeline direct pour minimiser la latence et le coût.
4. **Optimisation de latence multi-couche** : Perplexity combine plusieurs techniques pour maintenir la latence sous 10 secondes malgré la complexité du pipeline. Le streaming permet à l'utilisateur de lire le début de la réponse pendant que la suite est générée. Le caching sémantique (cache basé sur la similarité de la requête, pas uniquement le match exact) évite de relancer le pipeline complet pour des requêtes similaires. Le pré-compute d'embeddings pour les sources fréquemment citées réduit le temps de retrieval. L'infrastructure de scraping est distribuée géographiquement pour minimiser la latence réseau vers les sources web.

### Résultat
- Latence end-to-end de 3 à 8 secondes pour une réponse complète avec citations, avec le premier contenu visible en streaming en moins de 2 secondes
- Taux d'hallucination significativement réduit par rapport aux chatbots sans grounding — les utilisateurs peuvent vérifier chaque affirmation via les citations cliquables
- Dizaines de millions de requêtes traitées par jour avec une infrastructure optimisée pour le coût, rendant le modèle freemium viable
- Le raisonnement multi-étapes est déclenché pour environ 15-20% des requêtes complexes, produisant des réponses de qualité supérieure sur les sujets nécessitant une synthèse de plusieurs sources
- Acquisition utilisateur organique forte — la qualité des réponses citées génère un bouche-à-oreille qui réduit le coût d'acquisition

### Leçons apprises
- Le RAG en temps réel sur le web ouvert est un ordre de grandeur plus complexe que le RAG sur corpus statique — la variabilité de qualité des sources, la latence du scraping, et le parsing de formats hétérogènes (HTML, PDF, JavaScript-rendered pages) créent des défis absents du RAG interne. Prévoir un investissement significatif en infrastructure de scraping et en heuristiques de qualité des sources.
- Le citation grounding est le mécanisme de confiance fondamental — sans citations vérifiables, l'utilisateur ne peut pas distinguer un fait d'une hallucination. Concevoir le pipeline pour que chaque affirmation soit traçable à sa source, et investir dans la vérification post-génération de la cohérence citation-contenu.
- Le streaming transforme la perception de latence pour les pipelines longs — un utilisateur qui voit le contenu apparaître progressivement en 2 secondes tolère une réponse complète en 8 secondes, alors qu'un écran blanc pendant 8 secondes est inacceptable. Le streaming est un pattern UX, pas seulement un pattern technique.
- Le caching sémantique est indispensable pour les produits de recherche IA — les utilisateurs posent souvent des questions similaires (pas identiques), et le cache par exact match rate la majorité des opportunités de réutilisation. Un cache basé sur la distance cosinus des embeddings de requêtes capture ces similarités et réduit significativement les coûts d'inférence.

---

## Cas 4 : GitHub Copilot — Pair programming IA à l'échelle mondiale

### Contexte
GitHub Copilot, lancé en 2021 et développé en collaboration entre GitHub (Microsoft) et OpenAI, est le premier assistant de code IA déployé à l'échelle massive — plus de 1,8 million d'abonnés payants et des dizaines de millions de développeurs l'utilisant via des plans entreprise. Le produit fonctionne comme une extension de VS Code, JetBrains et Neovim, fournissant des suggestions d'autocomplétion en temps réel pendant que le développeur écrit du code. L'équipe Copilot (plusieurs centaines de personnes réparties entre GitHub et OpenAI) gère un pipeline qui traite des milliards de suggestions par mois, avec des contraintes de latence, de qualité et de coût à une échelle inégalée dans l'industrie de l'AI engineering.

### Problème
Le problème central de Copilot est l'optimisation simultanée de trois métriques en tension : la qualité des suggestions (mesurée par le taux d'acceptation), la latence (qui doit rester sous 300-500ms pour ne pas interrompre le flow du développeur), et le coût par suggestion (qui doit être viable quand chaque utilisateur génère des centaines de requêtes par jour). Le context window management est critique : le modèle ne voit qu'une fenêtre limitée du fichier courant et des fichiers adjacents, mais la pertinence de la suggestion dépend souvent de contexte distribué dans tout le projet (types, interfaces, conventions, tests existants). Un taux d'acceptation trop bas signifie que les suggestions sont du bruit qui ralentit le développeur plutôt que de l'aider — le seuil de valeur perçue se situe au-dessus de 25-30% d'acceptation.

### Approche
1. **Context window engineering** : Copilot utilise un algorithme sophistiqué de sélection de contexte pour remplir la fenêtre de contexte limitée avec les informations les plus pertinentes. Le fichier courant a la priorité maximale, avec le code avant et après le curseur. Les fichiers ouverts dans l'éditeur (onglets) sont inclus proportionnellement à leur pertinence estimée. Les imports et définitions de types référencés par le code courant sont extraits et injectés. Un mécanisme de "fill-in-the-middle" permet au modèle de voir à la fois le code avant et après le point d'insertion, améliorant significativement la pertinence par rapport à la complétion gauche-droite simple.
2. **Modèles optimisés pour le code** : Copilot utilise des modèles spécifiquement fine-tunés pour la complétion de code, dérivés de la famille GPT mais optimisés pour la latence et la qualité de code. Le fine-tuning sur des corpus massifs de code open-source calibre le modèle sur les patterns réels des développeurs. Les modèles sont distillés et quantifiés pour atteindre les latences cibles sans dégrader la qualité au-delà d'un seuil acceptable. Différentes tailles de modèles sont utilisées selon le type de suggestion — les complétions de quelques tokens utilisent un modèle plus compact que les générations de fonctions entières.
3. **Optimisation du taux d'acceptation** : GitHub mesure et optimise en continu le taux d'acceptation comme métrique north star. Les suggestions qui ont une faible probabilité d'être acceptées (estimée par un modèle de scoring) sont filtrées avant affichage pour éviter le bruit. Le timing d'affichage est calibré — afficher trop tôt (pendant que le développeur tape activement) ou trop tard (quand le développeur a déjà écrit le code) réduit les acceptations. Le système apprend les préférences par langage, par type de projet et par développeur via des signaux implicites (acceptations, rejets, éditions post-acceptation).
4. **Infrastructure de serving à l'échelle** : Le serving de Copilot s'appuie sur une infrastructure Azure massivement distribuée, avec du load balancing intelligent qui route les requêtes vers les instances avec la latence la plus faible. Le batching de requêtes côté serveur amortit le coût d'inférence GPU. Le debouncing côté client évite d'envoyer une requête à chaque frappe — un délai de quelques centaines de millisecondes après la dernière frappe évite les appels inutiles pendant la saisie active. La télémétrie granulaire (latence, tokens, acceptation, édition post-acceptation) alimente une boucle d'amélioration continue.

### Résultat
- Taux d'acceptation moyen de 30%+ sur l'ensemble des langages, avec des pics à 40%+ pour les langages bien représentés dans le corpus d'entraînement (Python, JavaScript, TypeScript)
- Latence médiane sous 300ms pour les suggestions d'autocomplétion, permettant une intégration fluide dans le flow de développement
- Plus de 1,8 million d'abonnés payants générant un revenu annuel estimé à plus de 200M USD, démontrant la viabilité économique d'un produit IA à haute fréquence d'utilisation
- Études internes montrant une amélioration de productivité de 55% sur les tâches de coding mesurées, avec une perception de réduction de la charge cognitive par les développeurs
- Le coût par suggestion est maintenu à une fraction de centime grâce au batching, au debouncing et à l'utilisation de modèles optimisés plutôt que de modèles frontier

### Leçons apprises
- Le taux d'acceptation est la métrique qui compte pour un assistant de code IA — pas la qualité objective des suggestions, mais la probabilité que le développeur les utilise effectivement. Une suggestion techniquement correcte mais mal timée, trop longue ou stylistiquement différente du code existant sera rejetée. Optimiser pour l'acceptation force à considérer le contexte complet de l'expérience développeur.
- Le context window engineering est un art d'ingénierie sous-estimé — la sélection de quel contexte inclure dans la fenêtre limitée a plus d'impact sur la qualité que le choix du modèle. Investir dans des heuristiques intelligentes de sélection de contexte (fichiers liés, types importés, patterns récents) est plus rentable que d'augmenter la taille du modèle.
- Le debouncing et le filtrage côté client sont essentiels pour la viabilité économique — sans eux, chaque frappe de clavier déclenche un appel au modèle, multipliant les coûts par un facteur 5-10x. Le compromis entre réactivité et coût est un paramètre de design critique.
- La boucle de feedback implicite (acceptation/rejet/édition) est le mécanisme d'amélioration le plus puissant à l'échelle — elle fournit des millions de signaux d'évaluation par jour sans aucun effort demandé à l'utilisateur, alimentant l'optimisation continue du modèle et de l'infrastructure.

---

## Cas 5 : Claude Code d'Anthropic — Agent de programmation avec architecture tool use

### Contexte
Claude Code, lancé par Anthropic en 2025, est un agent de programmation en ligne de commande (CLI) qui permet aux développeurs d'interagir avec leur codebase via le langage naturel. Contrairement aux assistants de complétion (Copilot, Cursor), Claude Code opère comme un agent autonome capable de lire des fichiers, exécuter des commandes shell, éditer du code, lancer des tests, créer des commits et interagir avec des APIs externes (GitHub, etc.). L'outil est conçu pour des tâches complexes multi-étapes : refactoring à grande échelle, résolution de bugs, implémentation de features complètes, revue de code, et migration de codebase. L'architecture repose sur le modèle Claude avec un système de tool use (function calling) qui permet au modèle d'appeler des outils définis (lecture de fichiers, recherche, édition, exécution bash) dans une boucle agentic.

### Problème
Le défi fondamental d'un agent de programmation est la tension entre autonomie et sécurité. L'agent doit avoir suffisamment de latitude pour accomplir des tâches complexes de manière autonome (explorer un codebase inconnu, identifier les fichiers pertinents, comprendre l'architecture, implémenter une solution, la tester), tout en garantissant qu'il ne cause jamais de dommages irréversibles (suppression de fichiers critiques, push force sur main, exécution de commandes destructives, exposition de secrets). La planification multi-étapes est un défi d'AI engineering : le modèle doit décomposer une demande vague ("corrige le bug de pagination") en une séquence d'actions concrètes (rechercher le code de pagination, comprendre le bug, identifier la cause, appliquer le fix, vérifier avec des tests), tout en gérant les échecs et les bifurcations imprévues. L'évaluation de la qualité est particulièrement difficile pour un agent — les métriques classiques (exact match, BLEU score) sont inadaptées quand la sortie est un ensemble d'actions sur un système de fichiers.

### Approche
1. **Architecture tool use structurée** : Claude Code définit un ensemble d'outils typés que le modèle peut appeler : lecture de fichiers (Read), recherche par pattern (Glob, Grep), édition de fichiers (Edit), exécution de commandes (Bash), écriture de fichiers (Write). Chaque outil a un schéma JSON strict qui contraint les paramètres acceptés, et un mécanisme de validation qui vérifie les pré-conditions avant l'exécution (par exemple, un fichier doit avoir été lu avant d'être édité). Le modèle alterne entre raisonnement (réflexion sur la prochaine étape) et appels d'outils dans une boucle agentic — le nombre d'itérations est plafonné pour éviter les boucles infinies.
2. **Planification multi-étapes avec adaptation** : Pour les tâches complexes, Claude Code utilise une approche plan-and-execute adaptative. Le modèle élabore un plan implicite basé sur la demande de l'utilisateur, exécute les premières étapes, observe les résultats, et ajuste le plan en fonction des découvertes. Par exemple, si la lecture d'un fichier révèle une architecture inattendue, le modèle adapte son approche plutôt que de suivre le plan initial aveuglément. Ce pattern est plus robuste que la planification rigide, car les codebases réels contiennent toujours des surprises que le modèle ne peut pas anticiper sans exploration.
3. **Safety guardrails multi-couches** : Claude Code implémente une défense en profondeur contre les actions dangereuses. Un système de permissions distingue les actions sûres (lecture de fichiers, recherche) qui s'exécutent sans confirmation, des actions sensibles (édition de fichiers, exécution bash) qui peuvent nécessiter l'approbation de l'utilisateur. Les commandes bash destructives (rm -rf, git push --force, git reset --hard) sont bloquées par défaut avec un avertissement explicite. Un mode sandbox restreint les commandes réseau et les accès fichiers en dehors du répertoire de travail. Les instructions système du modèle incluent des guardrails explicites interdisant les opérations irréversibles sans confirmation humaine.
4. **Évaluation agentic (evals)** : L'évaluation d'un agent de programmation nécessite des evals spécifiques. Anthropic utilise des benchmarks de type SWE-bench (résolution de vrais issues GitHub) où l'agent doit naviguer dans un codebase réel, comprendre un bug décrit en langage naturel, et produire un patch correct. Les evals mesurent non seulement si le patch résout le problème (tests passent), mais aussi la qualité du processus (nombre d'étapes, outils utilisés efficacement, absence d'actions inutiles). Des evals de sécurité vérifient que l'agent refuse les demandes dangereuses et demande confirmation pour les actions sensibles.

### Résultat
- Capable de résoudre des tâches de programmation complexes de bout en bout : lecture du codebase, compréhension du problème, implémentation de la solution, exécution des tests, création de commits — le tout en une seule session interactive
- Architecture tool use permettant une extensibilité via des outils MCP (Model Context Protocol), ouvrant l'agent à des intégrations avec des services externes (bases de données, APIs, systèmes de CI/CD) sans modification du modèle
- Guardrails de sécurité empêchant les actions destructives — les opérations irréversibles nécessitent une confirmation explicite, et les commandes connues comme dangereuses sont bloquées par défaut
- Performance élevée sur les benchmarks SWE-bench, démontrant la capacité à résoudre de vrais bugs dans de vrais codebases open-source, au-delà des exercices synthétiques
- Adoption rapide par les développeurs pour les tâches de refactoring, de migration et de résolution de bugs, réduisant significativement le temps consacré à la navigation et à la compréhension de codebases inconnus

### Leçons apprises
- Le human-in-the-loop n'est pas un compromis, c'est un design pattern de sécurité fondamental pour les agents IA. Demander confirmation pour les actions sensibles ne dégrade pas l'expérience utilisateur — au contraire, cela construit la confiance nécessaire pour que le développeur délègue des tâches de plus en plus complexes. L'objectif n'est pas l'autonomie totale mais l'autonomie calibrée au niveau de risque.
- La conception des outils (tool design) est aussi importante que le choix du modèle pour un agent. Des outils bien typés, avec des schémas stricts et des messages d'erreur informatifs, guident le modèle vers des comportements corrects. Des outils mal définis (paramètres ambigus, pas de validation) produisent des erreurs en cascade que le modèle ne sait pas récupérer.
- L'évaluation d'un agent nécessite des benchmarks end-to-end sur des tâches réelles — les evals unitaires (qualité de complétion, résolution d'exercices isolés) ne capturent pas les compétences critiques d'un agent : navigation dans un codebase inconnu, gestion des erreurs imprévues, adaptation du plan en cours de route. SWE-bench et ses variantes sont le standard émergent pour cette évaluation.
- L'architecture agentic (boucle tool use) est fondamentalement différente de la génération en une passe — elle requiert un raisonnement robuste sur plusieurs dizaines de tours, une gestion du contexte croissant (chaque appel d'outil ajoute de l'information), et des mécanismes d'arrêt pour éviter les boucles infinies. Les modèles qui excellent en génération de texte ne sont pas automatiquement de bons agents — la capacité de planification, de correction d'erreur et de raisonnement multi-étapes est un axe d'évaluation distinct.
