# Études de cas — Quality & Reliability

## Cas 1 : Transformation de la stratégie de tests avec Playwright

### Contexte
InvoiceCloud, SaaS de facturation B2B (55 personnes, 14 développeurs), gère 200K factures/mois pour 3000 clients PME et ETI. L'entreprise est un acteur de référence sur le marché français de la facturation électronique, avec une obligation de conformité aux normes Factur-X et à la réforme de la facturation électronique. L'application est développée en React/Next.js avec un backend Node.js (Fastify) et PostgreSQL. La suite de tests E2E utilise Cypress (150 tests) exécutée en nightly build sur Jenkins. L'équipe QA (2 personnes) effectue des tests manuels de régression de 3 jours avant chaque release bimensuelle. Le stack de test comprend également 200 tests unitaires (Jest) et 30 tests d'intégration (Supertest), formant une "pyramide inversée" avec trop de tests E2E et pas assez d'intégration.

### Problème
Les tests Cypress sont instables et compromettent la confiance de toute l'équipe dans l'automatisation : le taux de flakiness atteint 18% (27 tests flaky sur 150), principalement dû à des `cy.wait()` arbitraires (attentes de 2-5 secondes codées en dur), des sélecteurs CSS fragiles qui cassent à chaque modification de l'UI, et des tests non-isolés partageant un état de base de données global. Les développeurs ignorent systématiquement les résultats E2E car "c'est probablement flaky" — un vrai bug de régression sur le calcul de TVA passe en production en moyenne une fois par mois, coûtant 8-15K EUR en corrections manuelles de factures. Les tests manuels de 3 jours rallongent le cycle de release de 8 à 11 jours et bloquent les déploiements. Le temps d'exécution de la suite Cypress est de 45 minutes (exécution séquentielle dans un seul navigateur, pas de parallélisme). Le coût total de la QA (tests manuels + correction des bugs de régression + temps développeur perdu sur les faux positifs) est estimé à 30% du budget engineering.

### Approche
1. **Migration Cypress → Playwright** : Réécriture progressive des 150 tests en Playwright sur 8 semaines, en commençant par les 50 tests les plus flaky (triés par taux d'échec sur les 30 derniers jours). L'auto-waiting natif de Playwright élimine 80% de la flakiness : Playwright attend automatiquement qu'un élément soit visible, stable et actionnable avant d'interagir, supprimant les `cy.wait(3000)` arbitraires. Les browser contexts garantissent l'isolation entre tests — chaque test a son propre contexte navigateur avec un état de base de données frais (seed par test via un fixture). Exécution multi-navigateurs (Chromium, Firefox, WebKit) en parallèle sur 4 workers, réduisant le temps d'exécution de 4x.
2. **Test pyramid rebalancing** : La suite existante forme un "ice cream cone" inversé (150 E2E, 200 unitaires, 30 intégration) — trop de tests lents et fragiles en haut, pas assez de tests rapides et stables en bas. Migration vers une pyramide saine en 3 phases : extraction de la logique métier testable en unitaire (calcul de taxes TVA, proration, échéances de paiement, génération de numéros de facture — 150 tests unitaires ajoutés), ajout de 120 tests d'intégration API (Supertest) couvrant les endpoints critiques avec des assertions sur les réponses complètes, réduction des tests E2E de 150 à 80 scénarios critiques (happy paths des parcours utilisateur principaux uniquement : création de facture, paiement, avoir, export).
3. **Visual regression en CI** : Intégration de Playwright screenshot comparison pour les 15 pages critiques (facture PDF, dashboard, page de checkout, email de relance, tableau de bord comptable). Quality gate : zéro diff visuelle non approuvée. Les changements visuels intentionnels (modification de la charte graphique, ajout d'un champ) sont approuvés via un workflow de review dans GitHub — un reviewer voit la comparaison avant/après et approuve explicitement la diff. Le seuil de tolérance est configuré à 0.1% de pixels différents pour absorber le rendu anti-aliasing variable entre les navigateurs.
4. **Flaky test quarantine** : Processus automatisé via un script post-CI : un test qui échoue 2x sur les 10 dernières exécutions est automatiquement mis en quarantaine (taggé `@quarantine`, exécuté mais non-bloquant pour le pipeline). Un ticket Linear est créé automatiquement avec le label "flaky-test" et assigné à l'owner du test. SLA strict : correction ou suppression du test sous 1 sprint (2 semaines). Les tests en quarantaine > 2 semaines sont supprimés automatiquement avec une alerte au squad lead. Le taux de quarantaine est un KPI visible sur le dashboard d'équipe.

### Résultat
- Taux de flakiness passé de 18% à 0.5% (1 test flaky résiduel sur 200, en quarantaine), restaurant la confiance de l'équipe dans la suite de tests
- Temps d'exécution de la suite E2E réduit de 45 min à 8 min (parallélisme Playwright 4 workers + réduction du nombre de tests E2E de 150 à 80)
- Tests manuels de régression de 3 jours éliminés — les 2 QA se concentrent sur les tests exploratoires, les edge cases métier et la validation UX, à plus forte valeur ajoutée
- Bugs de régression en production réduits de 1/mois à 1/trimestre, soit une réduction de 75% des incidents liés au calcul de factures
- Cycle de release passé de 8 jours (avec testing manuel) à 1 jour (CI automatisé complet), permettant des releases quotidiennes
- Couverture multi-navigateurs : 100% des tests exécutés sur Chromium, Firefox et WebKit — un bug spécifique Safari sur l'export PDF détecté dès la première semaine

### Leçons apprises
- Playwright élimine mécaniquement la majorité de la flakiness grâce à l'auto-waiting et à l'isolation des browser contexts — c'est le choix par défaut pour tout nouveau projet E2E. Chez InvoiceCloud, le passage de Cypress à Playwright a réduit la flakiness de 18% à 0.5% sans aucune amélioration du code de l'application.
- La pyramide des tests n'est pas un dogme mais un guide — trop de tests E2E est un anti-pattern coûteux (lent, fragile, difficile à maintenir). Extraire la logique métier en tests unitaires rapides et réserver l'E2E aux 80% de valeur concentrés dans les 20% de parcours critiques. Chez InvoiceCloud, les 150 tests unitaires ajoutés s'exécutent en 4 secondes vs 8 minutes pour les 80 E2E.
- Le quarantine automatique des tests flaky est essentiel — un test flaky non traité érode la confiance de toute la suite (effet "boy who cried wolf"). Le SLA de 1 sprint et la suppression automatique après 2 semaines forcent le traitement rapide.
- La visual regression est un filet de sécurité précieux pour les applications B2B où l'apparence des documents (factures, exports) est contractuellement importante — un pixel mal placé sur une facture PDF peut causer un rejet par le client.

---

## Cas 2 : Chaos engineering et Game Days pour la résilience

### Contexte
StreamPay, fintech de paiement par streaming (70 personnes), traite 500K micro-paiements/jour (paiements de contenus digitaux, tips, subscriptions) via une architecture microservices sur Kubernetes (GKE). L'entreprise est un acteur innovant du paiement instantané, régulé par l'ACPR avec un SLA contractuel de 99.95%. Les 12 services communiquent via gRPC (synchrone) et Kafka (asynchrone). L'infrastructure est multi-AZ (3 zones GCP) mais n'a jamais été testée en conditions de panne réelle — la multi-AZ est une configuration théorique jamais validée. L'équipe technique comprend 15 développeurs, 3 SREs, et un responsable conformité. Le stack inclut Go (services de paiement), Node.js (API gateway), PostgreSQL (Cloud SQL), Redis (Memorystore) et Kafka (Confluent Cloud).

### Problème
Un incident majeur révèle la fragilité de l'architecture malgré les investissements en haute disponibilité : la perte d'une Availability Zone GCP pendant 35 minutes (incident GCP confirmé) provoque une cascade de défaillances non anticipée. Le service de paiement tente de reconnecter à la base PostgreSQL (instance Cloud SQL dans la zone perdue, le failover automatique prend 8 minutes), sature son pool de connexions (50 connexions, toutes en état "connecting"), et bloque tous les paiements pendant 2h15 — bien au-delà des 35 minutes de panne GCP. Le circuit breaker n'est pas configuré sur les appels base de données, les retries sont infinies (sans backoff ni limite), le pool de connexions n'a pas de timeout de connexion, et le failover DNS prend 20 minutes au lieu des 30 secondes théoriques. L'incident coûte 180K EUR en transactions perdues et 15 points de NPS. L'ACPR demande un rapport d'incident détaillé et un plan de remédiation sous 30 jours.

### Approche
1. **Steady-state hypothesis** : Définition de l'état normal du système sous forme de SLIs mesurables, servant de référence pour évaluer l'impact de chaque chaos experiment : latence p99 < 500ms (mesuré au niveau de l'API gateway), error rate < 0.1% (ratio 5xx / total requests), throughput > 100 transactions/seconde (mesuré côté Kafka consumer). Toute expérience de chaos est évaluée par son impact sur ces 3 SLIs — un experiment est considéré comme "réussi" (le système est résilient) si les SLIs restent dans les bornes, ou "révélateur" (vulnérabilité découverte) si un SLI dépasse la borne. Les SLIs sont affichés en temps réel sur un dashboard Grafana dédié pendant chaque experiment.
2. **Resilience patterns** : Implémentation systématique des patterns de résilience AVANT les chaos experiments (tester un système sans protection n'est pas du chaos engineering, c'est du sabotage) : circuit breaker (Resilience4j en Go) sur chaque appel inter-service et base de données avec un seuil de 50% d'échecs sur 10 secondes, retry with exponential backoff + jitter (base 100ms, max 5s, max 3 retries) pour les erreurs transitoires, bulkhead pattern (pools de connexions séparés par service consommé — un pool dédié PostgreSQL, un pool Redis, un pool par service gRPC), timeout strict sur chaque appel externe (200ms pour Redis, 500ms pour PostgreSQL, 1s pour gRPC), et graceful degradation (mode dégradé qui accepte les paiements en queue Kafka si le service de vérification de fraude est down, avec traitement différé).
3. **Chaos experiments progressifs** : Déploiement de Litmus (CNCF) en staging puis en production (après validation de tous les resilience patterns). Progression graduelle sur 3 mois : pod kill (service individuel, 1 pod à la fois) → network latency injection (200ms de latence ajoutée entre services spécifiques) → AZ failure simulation (drain complet d'un node pool dans une zone) → Kafka broker loss (arrêt d'1 broker sur 3) → PostgreSQL failover (trigger du failover Cloud SQL via maintenance simulée). Chaque experiment est documenté avec : hypothèse, impact observé sur les SLIs, vulnérabilités découvertes, et actions correctives. Les experiments en production sont limités aux heures creuses (2h-6h) et supervisés par un SRE avec un kill switch.
4. **Game Days trimestriels** : Organisation de Game Days de 4 heures avec toute l'équipe engineering (15 développeurs + 3 SREs). Scénario secret préparé par le SRE lead (seul informé) simulant un incident réaliste : "perte d'une AZ + Kafka broker down + pic de trafic x3". L'équipe on-call doit détecter, diagnostiquer et résoudre l'incident en temps réel sans savoir que c'est un exercice. Debriefing structuré de 2h après le Game Day avec timeline, what went well (détection en 3 min, communication claire), what went wrong (runbook Redis incomplet, escalade trop lente), et action items (mise à jour du runbook, automatisation de l'escalade). Les Game Days sont filmés et les replay sont utilisés pour l'onboarding des nouveaux développeurs.

### Résultat
- Circuit breaker activé sur 100% des appels inter-services et base de données — cascade de défaillances éliminée, validé par 3 AZ failure simulations en production
- AZ failover testé et validé : le système survit à la perte d'une AZ avec < 30 secondes de dégradation (vs 2h15 avant), sans perte de transaction
- MTTR réduit de 2h15 à 12 minutes pour les incidents d'infrastructure, grâce aux runbooks et à l'expérience acquise lors des Game Days
- 15 vulnérabilités de résilience découvertes et corrigées grâce aux chaos experiments : connexions non-poolées (3 services), timeouts manquants (5 appels), retries infinies (4 services), circuit breaker mal configuré (2 services), failover DNS non testé (1 service)
- Graceful degradation : le mode dégradé a été activé 3 fois en production réelle, traitant 95% des transactions en mode normal et les 5% restants en mode différé — au lieu de 0% de transactions traitées sans graceful degradation
- Confiance de l'équipe en astreinte : 90% se sentent "prêts" pour un incident majeur (vs 30% avant), mesuré par un survey anonyme après le 3ème Game Day

### Leçons apprises
- Les resilience patterns (circuit breaker, retry avec backoff, bulkhead, timeout) doivent être implémentés AVANT les chaos experiments — injecter des pannes dans un système sans protection est du sabotage, pas du chaos engineering. Chez StreamPay, les resilience patterns ont pris 6 semaines d'implémentation avant le premier chaos experiment.
- Les Game Days sont l'investissement le plus rentable en résilience — ils révèlent les failles que les tests automatisés ne trouvent pas : procédures manquantes (qui appelle qui en SEV1 ?), runbooks incomplets (le runbook Redis ne couvrait pas le cas de saturation graduelle), communication défaillante (le canal Slack d'incident n'existait pas), et le stress réel du troubleshooting sous pression.
- Le graceful degradation est la différence entre une panne totale et une dégradation acceptable — il doit être designé, implémenté et testé pour chaque service critique. Chez StreamPay, le mode dégradé du service de fraude a sauvé 95% des transactions lors d'un incident réel, transformant une panne potentielle de 180K EUR en un impact de 9K EUR.
- Les chaos experiments en production nécessitent un cadre strict — kill switch accessible en 1 clic, heures creuses uniquement, SRE superviseur dédié, rollback automatique si un SLI dépasse 2x la borne. Sans ce cadre, les chaos experiments en production sont un risque inacceptable pour une fintech régulée.

---

## Cas 3 : Implémentation d'error budgets et SLO-driven development

### Contexte
HealthAPI, plateforme d'API de données de santé (45 personnes), expose des APIs REST consommées par 80 applications tierces (hôpitaux, pharmacies, assurances, startups healthtech). L'entreprise est un acteur structurant de l'écosystème e-santé français, certifiée HDS et connectée au Health Data Hub. L'infrastructure tourne sur AWS ECS avec PostgreSQL RDS (multi-AZ), Redis ElastiCache, et un API gateway Kong. Le SLA contractuel est de 99.9% de disponibilité (43.2 min de downtime autorisé/mois), avec des pénalités financières progressives en cas de non-respect. L'équipe technique comprend 10 développeurs, 2 SREs et un responsable conformité. Le monitoring actuel utilise Datadog avec 200+ alertes configurées par les SREs successifs.

### Problème
L'entreprise est prise dans un cercle vicieux classique des organisations sans SLO : l'équipe produit pousse pour des features (3 clients majeurs attendent une intégration avec le DMP), l'équipe SRE freine pour la fiabilité ("on ne peut pas déployer, le système est trop fragile"), et le CTO arbitre au cas par cas sans données objectives, créant de la frustration des deux côtés. Les développeurs déploient sans considérer l'impact sur la fiabilité (une migration de schéma a causé 2h de downtime), et les SRE bloquent des releases "par précaution" (3 releases retardées d'une semaine sans justification quantifiée). Résultat paradoxal : le SLA de 99.9% est respecté sur le papier (99.92% mesuré) mais 3 clients majeurs ont renouvelé avec une pénalité pour les incidents passés, et le NPS des intégrateurs est de -10. L'équipe SRE (2 personnes) est en burnout chronique, traitant 25 pages PagerDuty par semaine dont 80% sont des faux positifs (CPU spike, memory warning) qui ne correspondent à aucun impact utilisateur.

### Approche
1. **SLIs centrés utilisateur** : Définition de 4 SLIs basés sur l'expérience utilisateur réelle (pas sur les métriques d'infrastructure) en atelier de 2 jours avec les SREs, les développeurs et le product manager : availability (% requêtes retournant 2xx/3xx en < 3s, mesuré au niveau du load balancer — exclut les requêtes de monitoring internes), latency (p95 < 500ms, mesuré de la réception de la requête au renvoi de la réponse), correctness (% de réponses avec données valides et à jour, vérifié par des synthetic checks comparant les réponses API avec les données sources), freshness (données < 5 min de retard par rapport à la source, mesuré par un synthetic check qui écrit une donnée et vérifie sa disponibilité via l'API). Les SLIs sont implémentés comme des métriques Datadog custom alimentées par les logs du load balancer et les synthetic checks.
2. **SLOs et error budgets** : SLO de 99.95% sur une fenêtre glissante de 30 jours pour chaque SLI — plus ambitieux que le SLA contractuel de 99.9% pour garder une marge de sécurité. Error budget calculé : 0.05% x 30 jours x 24h x 60min = 21.6 minutes d'erreur autorisées/mois. Dashboard error budget visible par tous (product, engineering, management, et même les clients enterprise via un portail de status), rafraîchi toutes les 5 minutes. Le dashboard affiche : budget restant en minutes et en pourcentage, tendance sur 7 jours (en consommation accélérée ou stable), et projection de fin de mois. Le CTO a imposé que le dashboard error budget soit le premier slide de chaque comité engineering hebdomadaire.
3. **Error budget policies** : Politique formalisée dans un document de 2 pages signé par le CTO, VP Product et VP Engineering, présentée en all-hands : Budget > 50% → ship features normalement, les releases à risque modéré sont autorisées ; Budget 25-50% → review supplémentaire des releases par un SRE, pas de changements d'infrastructure ni de migrations de schéma ; Budget < 25% → feature freeze, 100% du temps engineering consacré à la fiabilité, postmortem obligatoire pour chaque incident même mineur ; Budget épuisé → incident review avec le CTO, plan de remédiation formalisé avant toute nouvelle feature, communication proactive aux clients affectés. La politique inclut une clause d'exception : le VP Engineering peut autoriser un déploiement critique même en feature freeze, à condition de documenter le risque accepté.
4. **Burn rate alerting** : Remplacement des 200+ alertes statiques par 8 alertes burn rate (2 par SLI : fast burn 14.4x/1h et slow burn 3x/6h). Le fast burn page le on-call via PagerDuty (impact majeur immédiat), le slow burn notifie le canal Slack #reliability (dégradation subtile nécessitant une investigation dans les heures suivantes). Runbook associé à chaque alerte avec les étapes de diagnostic et de remédiation. Suppression de 192 alertes non-actionnables — chaque suppression est documentée avec la raison ("CPU > 80% ne corrèle pas avec un impact utilisateur sur les 6 derniers mois"). Les 8 alertes restantes sont testées mensuellement via des synthetic failures pour vérifier qu'elles se déclenchent correctement.

### Résultat
- Pages on-call réduites de 25/semaine à 4/semaine — 100% actionnables, chaque page correspond à une consommation anormale d'error budget (donc un impact utilisateur réel)
- Disponibilité mesurée passée de 99.92% à 99.97% — l'error budget crée un cadre de priorisation clair qui force l'investissement proactif en fiabilité avant que le budget ne soit épuisé
- Feature velocity augmentée de 20% — les releases ne sont plus bloquées "par précaution" par les SRE mais par des données objectives (budget restant), éliminant les frictions politiques
- Dialogue product/engineering objectivé et apaisé : "il reste 65% d'error budget, on peut prendre le risque de cette migration" vs l'ancien "on ne devrait pas déployer, c'est trop risqué"
- Zéro pénalité SLA sur les 12 derniers mois (vs 3 pénalités l'année précédente), soit 75K EUR de pénalités évitées
- Burnout SRE résolu : satisfaction de l'équipe SRE passée de 3/10 à 8/10, les deux SREs passent 60% de leur temps sur des projets d'amélioration au lieu de répondre à des alertes

### Leçons apprises
- L'error budget est l'outil le plus puissant pour aligner product et engineering — il transforme la fiabilité d'un sujet de conflit subjectif en un sujet de données objectives. Chez HealthAPI, le premier mois avec les error budgets a éliminé 100% des discussions "devrait-on déployer" — la réponse est dans le dashboard.
- Le burn rate alerting élimine la fatigue d'alerte tout en améliorant la détection — moins d'alertes mais chaque alerte compte. La clé est que le burn rate mesure l'impact sur l'error budget (donc sur l'utilisateur), pas sur l'infrastructure. Un CPU à 95% qui ne dégrade pas la latence ne page plus jamais.
- L'error budget policy doit être formalisée et signée par le management — sans engagement exécutif, elle est ignorée sous pression business. Chez HealthAPI, la première activation du feature freeze a été un moment critique : le VP Product voulait forcer un déploiement, le VP Engineering a tenu bon en brandissant le document signé. Ce précédent a établi la crédibilité de tout le système.
- Les SLIs doivent être centrés sur l'expérience utilisateur, pas sur l'infrastructure — un SLI "CPU < 80%" ne mesure pas la satisfaction utilisateur. Chez HealthAPI, la migration des SLIs d'infrastructure vers des SLIs utilisateur a révélé que 60% des alertes précédentes ne correspondaient à aucun impact réel, expliquant les 80% de faux positifs.
