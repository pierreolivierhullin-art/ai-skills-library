# Études de cas — Backend Development & Database Management

## Cas 1 : Optimisation d'une base PostgreSQL pour un SaaS analytics

### Contexte
InsightBoard, SaaS B2B d'analytics marketing, gère 500M de lignes d'événements dans une base PostgreSQL 15 hébergée sur AWS RDS (db.r6g.2xlarge, 8 vCPU, 64 Go RAM). L'entreprise est positionnée sur le segment des PME e-commerce françaises, avec une croissance de 30% par an sur un marché dominé par Google Analytics et Amplitude. L'application utilise Prisma comme ORM avec un backend Node.js (Fastify) et un cache Redis ElastiCache. 50 clients génèrent chacun 500K-5M d'événements par mois. Les dashboards client affichent des agrégations sur des périodes variables (7j, 30j, 90j, 12 mois) avec des filtres par source de trafic, type d'événement et segment utilisateur. L'équipe technique comprend 8 développeurs, 1 DBA à mi-temps et 2 SREs.

### Problème
Les dashboards deviennent inutilisables aux heures de pointe (10h-12h et 14h-16h) : les requêtes d'agrégation sur 90 jours prennent 15-45 secondes, provoquant des timeouts côté frontend et des plaintes récurrentes de 15 clients majeurs. L'EXPLAIN ANALYZE révèle des sequential scans sur les tables d'événements malgré la présence d'index B-tree sur `tenant_id` et `created_at` — le planner PostgreSQL considère le sequential scan plus efficace car les index couvrent moins de 30% de la table (statistiques non mises à jour et work_mem insuffisante). Le connection pool (PgBouncer en mode session) sature avec 200 connexions actives aux heures de pointe, alors que le RDS n'en supporte que 250. Le coût RDS a doublé en 6 mois (scaling vertical de db.r6g.xlarge à db.r6g.2xlarge) sans amélioration significative des performances — le goulot d'étranglement est l'I/O, pas le CPU.

### Approche
1. **Partitionnement par date** : Conversion de la table `events` (500M lignes) en table partitionnée par mois (`PARTITION BY RANGE (created_at)`). Chaque partition contient ~40M lignes, une taille qui tient confortablement dans le buffer pool. PostgreSQL élimine automatiquement les partitions non pertinentes lors des requêtes filtrées par date (partition pruning), réduisant le volume de données scannées de 10-100x selon la période. La migration a été réalisée via `pg_partman` avec un script de migration online qui a copié les données par tranches de 1M de lignes sur 72 heures, sans downtime — les écritures continuaient sur la table originale et étaient rejouées via un trigger de redirection.
2. **Index couvrants et partiels** : Création d'index covering (`CREATE INDEX ... INCLUDE (value, metadata)`) pour les requêtes de dashboard, éliminant les heap fetches et réduisant les I/O de 80%. Index partiels sur les événements récents (`WHERE created_at > NOW() - INTERVAL '90 days'`) pour les requêtes les plus fréquentes — ces index sont 4x plus petits que les index complets et tiennent intégralement en mémoire. Mise à jour de `default_statistics_target` à 500 pour les colonnes filtrées fréquemment, et `work_mem` augmenté à 256MB pour les requêtes analytiques.
3. **Vues matérialisées** : Création de 3 vues matérialisées pré-agrégées (daily_metrics, weekly_metrics, monthly_metrics) par tenant et par type d'événement, réduisant les données à interroger de 500M lignes à 2M lignes d'agrégats. Rafraîchissement incrémental toutes les 15 minutes via pg_cron avec `REFRESH MATERIALIZED VIEW CONCURRENTLY` (sans lock). Les dashboards interrogent les vues matérialisées au lieu des tables brutes, avec un fallback sur les données brutes uniquement pour les requêtes ad hoc nécessitant un drill-down au niveau événement. Un monitoring Datadog alerte si le rafraîchissement prend plus de 5 minutes.
4. **Migration vers Supavisor** : Remplacement de PgBouncer (mode session, inefficace pour le pooling) par Supavisor pour un connection pooling plus efficace avec support du prepared statement protocol et du mode transaction pooling. Configuration en mode transaction pooling avec un pool de 50 connexions backend pour 500 connexions client — chaque requête Prisma occupe une connexion backend uniquement pendant l'exécution. Supavisor gère également le routing read replica : les requêtes de dashboard (read-heavy) sont dirigées vers 2 read replicas RDS, déchargeant le primaire de 70% du trafic.

### Résultat
- Temps de réponse dashboard passé de 15-45s à 200ms-1.2s (divisé par 30), avec un p99 < 2s même aux heures de pointe
- Sequential scans éliminés — 100% des requêtes dashboard utilisent des index ou des vues matérialisées, validé par un audit EXPLAIN ANALYZE hebdomadaire automatisé
- Connexions backend réduites de 200 à 50 grâce au pooling optimisé, avec une capacité de supporter 1000 connexions client simultanées
- Coût RDS réduit de 40% : retour à une instance db.r6g.xlarge (scaling down) plus 2 read replicas db.r6g.large, pour un coût total inférieur à la seule instance 2xlarge précédente
- Capacité à supporter 2x plus de clients sans modification d'infrastructure, validé par un test de charge simulant 100 clients avec des patterns d'usage réels
- Zéro plainte client sur les performances des dashboards depuis la migration (6 mois), contre 15 tickets par mois avant

### Leçons apprises
- Le partitionnement par date est le premier levier pour les tables d'événements à forte volumétrie — il doit être prévu dès la conception du schéma, pas après. La migration d'une table de 500M lignes vers une table partitionnée a pris 3 jours d'ingénierie et 72h de migration — un coût qui aurait été de 2h si le partitionnement avait été prévu initialement.
- Les vues matérialisées transforment les performances des dashboards analytics, mais le rafraîchissement doit être soigneusement planifié pour ne pas surcharger la base. Chez InsightBoard, le `REFRESH CONCURRENTLY` consomme 20% du CPU du primaire pendant 3 minutes — le scheduler pg_cron est configuré pour éviter les heures de pointe.
- EXPLAIN ANALYZE est l'outil le plus sous-utilisé — l'analyser systématiquement sur les 10 requêtes les plus fréquentes avant d'investir dans le scaling vertical. Chez InsightBoard, 80% du gain de performance provient de 3 index correctement conçus, pas du hardware.
- Le connection pooling en mode transaction est drastiquement plus efficace que le mode session pour les applications avec des requêtes courtes — chez InsightBoard, le ratio est de 10:1 (500 connexions client pour 50 backend), là où le mode session exigeait un ratio 1:1.

---

## Cas 2 : Architecture backend multi-tenant avec Supabase et RLS

### Contexte
TeamBoard, startup SaaS de gestion de projets (12 personnes, 4 développeurs), lance un MVP avec Supabase comme backend-as-a-service pour accélérer le time-to-market. L'entreprise, fondée il y a 18 mois, cible les petites équipes tech (5-20 personnes) avec un positionnement "Trello + Notion simplifié". L'application Next.js utilise le client Supabase directement depuis le frontend pour les opérations CRUD via les auto-generated REST APIs et les Realtime subscriptions. 200 équipes (tenants) utilisent la plateforme avec un total de 3000 utilisateurs. Le schéma de base comprend 18 tables (projects, tasks, comments, files, team_members, etc.) avec des foreign keys entre elles. L'équipe n'a pas de DBA ni de spécialiste sécurité — les 4 développeurs sont full-stack juniors à intermédiaires.

### Problème
Après 6 mois de croissance rapide et d'ajout de features sous pression, un client (une startup fintech) signale qu'il voit les projets d'une autre équipe dans son dashboard — un incident de sécurité de type fuite de données cross-tenant. L'investigation révèle que les RLS policies sont incomplètes : la table `comments` n'a aucune policy (oubli lors d'un sprint de 2 semaines où 3 tables ont été ajoutées sans RLS), et la policy de la table `tasks` vérifie `auth.uid() = user_id` au lieu de `team_id = (SELECT team_id FROM team_members WHERE user_id = auth.uid())` — un utilisateur pouvait donc voir les tâches de n'importe quelle équipe s'il connaissait l'ID. Le client Supabase côté frontend a un accès direct à la base sans couche de validation intermédiaire, rendant les RLS policies la seule ligne de défense. Le client fintech menace de résilier son abonnement (leur plus gros client à 500 EUR/mois) et de signaler l'incident à la CNIL car des données personnelles sont impliquées.

### Approche
1. **Audit RLS complet** : Création d'un script SQL automatisé (`rls_audit.sql`) qui vérifie que chaque table accessible via l'API Supabase a au moins une RLS policy active pour SELECT, INSERT, UPDATE et DELETE. Le script compare la liste des tables avec `pg_policies` et génère un rapport des écarts. Résultat de l'audit initial : 5 tables sur 18 sans policies complètes (comments, files, notifications, activity_log, tags). Correction immédiate en 48h avec une policy par défaut restrictive (`USING (false)`) appliquée d'abord, puis remplacée par la policy correcte après test. L'audit est désormais exécuté automatiquement à chaque migration Supabase.
2. **Pattern de tenant isolation** : Création d'une fonction PostgreSQL `current_tenant_id()` qui extrait le `team_id` de l'utilisateur authentifié via une jointure avec `team_members`. Toutes les RLS policies utilisent cette fonction comme filtre unique, garantissant une logique d'isolation homogène sur toutes les tables. Un trigger `BEFORE INSERT` injecte automatiquement le `tenant_id` sur chaque insertion, empêchant un développeur d'oublier de le spécifier dans le code applicatif. Un trigger `BEFORE UPDATE` empêche la modification du `tenant_id` d'une ressource existante. Ce pattern a été documenté dans un ADR et un template de migration Supabase qui génère automatiquement la RLS policy et les triggers pour chaque nouvelle table.
3. **Edge Functions comme API layer** : Migration des opérations critiques (création de projet, invitation de membres, modification de permissions, suppression de données) depuis les appels Supabase directs vers des Edge Functions Deno. Les Edge Functions utilisent le `service_role` key (qui bypass le RLS) avec des validations métier explicites avant chaque opération — vérification des permissions, validation des inputs (Zod), rate limiting, et audit logging. Cette couche intermédiaire permet d'exprimer des règles métier complexes (ex : "seul un admin peut inviter un membre externe") que le RLS PostgreSQL ne peut pas implémenter. Les opérations CRUD simples (lecture de tâches, mise à jour de statut) restent en accès direct Supabase pour la performance.
4. **Tests RLS automatisés** : Suite de tests pgTAP qui vérifie l'isolation des données avec un jeu de données de test comprenant 3 tenants (A, B, C) et des utilisateurs avec différents rôles. 47 assertions vérifient qu'un utilisateur du tenant A ne peut JAMAIS lire, modifier ou supprimer des données du tenant B — y compris via des jointures complexes et des sous-requêtes. Ces tests s'exécutent dans le pipeline CI à chaque migration et prennent 12 secondes. Un test de "negative permission" vérifie spécifiquement les scénarios d'attaque identifiés lors de l'incident initial (manipulation d'ID dans les URL, requêtes cross-tenant via les Realtime subscriptions).

### Résultat
- Fuite de données cross-tenant éliminée — zéro incident en 8 mois depuis la remédiation, validé par un pentest externe trimestriel
- 100% des tables avec RLS policies complètes et testées, avec un audit automatisé qui bloque toute migration ajoutant une table sans policy
- Tests RLS dans le CI : 47 assertions vérifiant l'isolation à chaque migration, exécutés en 12 secondes
- Confiance accrue des clients enterprise — le mécanisme d'isolation est auditable et documenté dans un whitepaper sécurité partagé avec les prospects
- Temps de développement pour les nouvelles features réduit de 20% grâce aux patterns standardisés (tenant isolation automatique via triggers et templates de migration)
- Le client fintech est resté et a upgradé son plan, citant les mesures correctives comme preuve de maturité de l'équipe

### Leçons apprises
- Le RLS Supabase est puissant mais fragile — un oubli de policy sur une seule table expose toutes les données de cette table. L'automatisation de la vérification est obligatoire, pas optionnelle. Chez TeamBoard, l'audit automatisé a détecté 2 oublis supplémentaires dans les 3 mois suivants, prouvant que l'erreur humaine est récurrente.
- Le client Supabase en frontend ne doit JAMAIS être la seule couche de sécurité — les Edge Functions ajoutent une validation métier que le RLS ne peut pas exprimer (permissions complexes, workflows multi-étapes, rate limiting). La règle chez TeamBoard : toute opération qui modifie des permissions ou supprime des données passe par une Edge Function.
- Les tests d'isolation doivent être traités comme des tests de sécurité critiques — ils s'exécutent à chaque migration, pas uniquement en release. Un test qui échoue bloque le merge, sans exception.
- La réponse à l'incident (communication transparente, correction en 48h, pentest externe, documentation publique) a transformé une crise en opportunité — le client fintech est devenu une référence commerciale et 3 prospects enterprise ont cité la transparence de TeamBoard comme facteur de décision.

---

## Cas 3 : Migration d'ORM et optimisation N+1 pour une API GraphQL

### Contexte
SocialHub, plateforme de gestion de réseaux sociaux (30 personnes), permet aux community managers de gérer plusieurs comptes sociaux depuis un dashboard unifié. L'entreprise sert 2000 clients (agences et marques) sur le marché européen, en compétition avec Hootsuite et Buffer. L'API expose une API GraphQL (Apollo Server 4) avec TypeORM sur PostgreSQL 15, déployée sur AWS ECS Fargate. L'API sert 10K requêtes/minute avec une latence p95 de 2.5 secondes — bien au-dessus de l'objectif de 500ms. Le frontend React (Create React App) effectue des requêtes GraphQL imbriquées pour charger les dashboards : comptes sociaux, posts, métriques, commentaires, avec des profondeurs d'imbrication allant jusqu'à 4 niveaux. L'équipe backend comprend 5 développeurs, dont 2 juniors qui ont rejoint récemment.

### Problème
L'analyse des query plans révèle un problème massif de N+1 queries — le cauchemar classique de GraphQL. Une seule requête GraphQL "tableau de bord" génère jusqu'à 450 requêtes SQL (1 pour les 15 comptes + 15x10 pour les posts + 150x2 pour les métriques et commentaires). TypeORM charge les relations en lazy loading par défaut, et chaque resolver GraphQL déclenche des requêtes indépendantes sans aucune batching. Le pool de connexions PostgreSQL (20 connexions) sature 3 fois par jour pendant les heures de pointe, provoquant des timeouts en cascade et des pages d'erreur 504. Les développeurs juniors aggravent le problème en ajoutant des resolvers sans comprendre l'impact SQL. Le coût RDS a triplé en un an (upgrade vers db.r6g.2xlarge + 3 read replicas) sans résoudre le problème fondamental. Un client majeur (agence gérant 50 comptes) menace de churner car son dashboard met 8 secondes à charger.

### Approche
1. **DataLoader pattern** : Implémentation de DataLoader (Facebook) pour chaque entité (Account, Post, Metric, Comment). Les requêtes N+1 sont automatiquement regroupées en batch queries — au lieu de 150 requêtes `SELECT * FROM metrics WHERE post_id = ?`, DataLoader émet une seule requête `SELECT * FROM metrics WHERE post_id IN (?, ?, ...)`. Un DataLoader par requête GraphQL (scope per-request via le context Apollo) pour éviter les problèmes de cache cross-request et les fuites de données entre utilisateurs. L'implémentation a pris 1 semaine et a été le quick win le plus impactant du projet.
2. **Migration TypeORM → Drizzle ORM** : Migration progressive de TypeORM vers Drizzle ORM pour le type-safety complet et le contrôle explicite des jointures. Drizzle génère des requêtes SQL prévisibles (pas de magie lazy loading) et permet les jointures explicites avec `db.query.accounts.findMany({ with: { posts: { with: { metrics: true } } } })`. La migration a été réalisée table par table sur 8 semaines : chaque resolver est migré, testé avec des tests d'intégration comparant les résultats TypeORM et Drizzle, puis basculé. Un wrapper de compatibilité a permis aux deux ORMs de coexister pendant la transition sans duplication de code.
3. **Query complexity analysis** : Implémentation d'un plugin Apollo Server qui calcule la complexité de chaque requête GraphQL selon un scoring pondéré (chaque field = 1 point, chaque relation = 5 points, chaque liste non paginée = 10 points) et rejette les requêtes dépassant un seuil (score > 500). Cela empêche les requêtes excessivement imbriquées qui pourraient générer des milliers de requêtes SQL. Les clients recevant un rejet obtiennent un message d'erreur explicite avec une suggestion de pagination. Un query depth limit de 5 niveaux a également été implémenté pour prévenir les attaques par requêtes récursives.
4. **Pagination par curseur** : Remplacement de la pagination offset (`LIMIT 20 OFFSET 100`) par la pagination par curseur (`WHERE id > :cursor ORDER BY id LIMIT 20`) pour les listes de posts et commentaires. Performance O(1) au lieu de O(n) — la page 500 est aussi rapide que la page 1, contrairement à l'offset qui scanne et ignore les n premières lignes. La pagination utilise des curseurs opaques encodés en base64 contenant l'ID et le timestamp de tri, conformes à la spécification Relay Connection. Le frontend a été adapté pour utiliser le pattern `fetchMore` d'Apollo Client avec le merge automatique des pages.

### Résultat
- Requêtes SQL par requête GraphQL réduites de 450 à 4-8 (divisé par 60), mesuré par un logging automatique du nombre de requêtes par request
- Latence p95 passée de 2.5s à 180ms (divisé par 14), avec un p99 < 400ms même pour les dashboards complexes
- Saturation du connection pool éliminée — connexions actives réduites de 180 à 35 en pic, avec une capacité de headroom de 70%
- Coût RDS réduit de 30% (downgrade d'instance vers db.r6g.xlarge et suppression de 2 read replicas sur 3)
- Migration Drizzle complétée en 8 semaines sans downtime (migration table par table avec tests de comparaison)
- Le client majeur (agence 50 comptes) a vu son dashboard passer de 8s à 600ms — contrat renouvelé pour 2 ans avec upgrade de plan

### Leçons apprises
- Le N+1 query est le problème de performance #1 des APIs GraphQL — le DataLoader pattern doit être implémenté dès le premier jour, pas après. Chez SocialHub, DataLoader seul a divisé le nombre de requêtes SQL par 30, avant même la migration d'ORM.
- Les ORMs avec lazy loading par défaut (TypeORM, Sequelize) sont dangereux avec GraphQL — préférer des ORMs explicites (Drizzle, Prisma) qui forcent la déclaration des relations à charger. Le lazy loading de TypeORM est un piège invisible : le code fonctionne correctement mais génère des centaines de requêtes.
- La pagination par curseur est dramatiquement plus performante que l'offset pour les grands datasets — le surcoût d'implémentation (2 jours) est minimal comparé au gain. Pour les datasets > 10K lignes, l'offset ne devrait jamais être utilisé.
- La query complexity analysis est un filet de sécurité indispensable pour les APIs GraphQL publiques — sans elle, un seul client malveillant ou un bug frontend peut saturer la base de données avec une requête exponentiellement coûteuse.
