# Études de cas — Code Excellence

## Cas 1 : Réduction de la dette technique d'un legacy codebase

### Contexte
FinApp, startup fintech de 35 personnes, maintient une application de gestion de trésorerie développée en 4 ans, ciblant les DAF et trésoriers de PME et ETI françaises. Le codebase TypeScript/React/Node.js compte 180K lignes de code avec une couverture de tests de 22%, principalement concentrée sur des tests unitaires superficiels de composants React. La vélocité de l'équipe (8 développeurs répartis en 2 squads) a chuté de 40% en 18 mois malgré l'ajout de 2 développeurs. Chaque nouvelle feature nécessite des modifications dans 15-20 fichiers en moyenne, signe d'un couplage excessif. Le stack technique comprend React 17 (non migré vers la v18), Express.js, Prisma, PostgreSQL, et un déploiement Vercel + Railway. Le turnover développeur est de 30% annuel, chaque départ aggravant la perte de connaissance tacite sur le codebase.

### Problème
Le code accumule une dette technique massive quantifiée par SonarQube à 120 jours-développeur de remédiation : 47 God Classes (> 500 lignes, la plus grosse faisant 2800 lignes), duplication de logique métier dans 3 couches différentes (API routes, services, composants React — le calcul de TVA est implémenté à 4 endroits avec des résultats divergents), et zéro test sur le module de calcul de trésorerie prévisionnelle (le plus critique, générant 60% de la valeur perçue par les clients). Les bugs de régression augmentent de 20% par trimestre, avec 3 incidents de calcul incorrect signalés par des clients en 2 mois. Le onboarding d'un nouveau développeur prend 6 semaines avant le premier commit productif — le développeur passe les 4 premières semaines à essayer de comprendre les dépendances implicites. Le CTO estime que 60% du temps de développement est consacré à la compréhension du code existant plutôt qu'à la création de valeur.

### Approche
1. **Diagnostic par hotspots** : Analyse de churn via `git log --numstat` (fichiers les plus modifiés sur 12 mois) croisée avec la complexité cyclomatique (SonarQube). Identification de 12 fichiers "hotspots" responsables de 65% des bugs — des fichiers modifiés 200+ fois par an avec une complexité cyclomatique > 40. Ces 12 fichiers représentent seulement 3% du codebase mais concentrent l'essentiel de la douleur. La visualisation en bubble chart (axe X = fréquence de modification, axe Y = complexité, taille = nombre de bugs) a convaincu le management d'investir dans le refactoring. Priorité absolue donnée à ces 12 fichiers.
2. **Tests de caractérisation** : Avant tout refactoring, écriture de 200+ tests de caractérisation sur les modules critiques (calcul de trésorerie, moteur de pricing, réconciliation bancaire) pour capturer le comportement existant exact, y compris les bugs connus qui sont devenus des "features" (certains clients dépendent des résultats de calcul actuels). Chaque test documente le comportement observé, pas le comportement attendu. Mutation testing avec Stryker pour valider la qualité des tests : mutation score initial de 35% (les tests existants ne détectaient que 35% des mutations de code), cible fixée à 80%. L'investissement en tests de caractérisation a pris 3 semaines avec 2 développeurs dédiés.
3. **Refactoring incrémental** : Application systématique du Boy Scout Rule (chaque PR laisse le code plus propre) + sessions dédiées de 20% du sprint réservées exclusivement au refactoring (pas de features, pas de bugfixes — un créneau protégé validé par le product manager). Extraction des 47 God Classes en modules cohérents (principe SRP) — la classe `TransactionService` de 2800 lignes a été décomposée en 8 modules : `CashFlowCalculator`, `ReconciliationEngine`, `TaxComputer`, etc. Introduction de Value Objects (Money, AccountId, DateRange, TaxRate) pour remplacer les primitives (les montants étaient des `number` sans devise associée, source de bugs de conversion). Mise en place du pattern Ports & Adapters pour isoler la logique métier des dépendances externes (ORM, API bancaires, services de change).
4. **Quality gates en CI** : Configuration de SonarQube avec des portes strictes sur le nouveau code uniquement (pour ne pas bloquer les PRs existantes) : zéro nouveau code smell bloquant, couverture minimum de 80% sur le nouveau code, complexité cyclomatique < 10 par fonction, et duplication < 3%. Un bot GitHub commente automatiquement chaque PR avec le delta de dette technique (ajoutée vs remboursée). Les PR qui augmentent la dette technique nette nécessitent l'approbation d'un tech lead.

### Résultat
- Couverture de tests passée de 22% à 68% en 6 mois (90%+ sur la logique métier critique), avec des tests qui détectent réellement les régressions
- Mutation score passé de 35% à 78%, prouvant que les tests valident le comportement, pas seulement la couverture de lignes
- Bugs de régression réduits de 45% par trimestre — les incidents de calcul incorrect signalés par les clients sont tombés à zéro
- Vélocité de l'équipe remontée de 40% (retour au niveau initial), puis augmentée de 15% supplémentaires au trimestre suivant grâce à la meilleure modularité
- Onboarding réduit de 6 semaines à 2 semaines grâce à la meilleure lisibilité et à la documentation implicite des tests de caractérisation
- Les 12 fichiers hotspots refactorés en modules de < 200 lignes avec responsabilités claires — le fichier le plus modifié génère désormais 0 bug par trimestre au lieu de 8

### Leçons apprises
- L'analyse de churn x complexité est le meilleur prioriseur de refactoring — cibler les fichiers qui changent souvent ET qui sont complexes, pas la dette technique "théorique" identifiée par SonarQube. Chez FinApp, les 12 hotspots (3% du codebase) représentaient 65% des bugs et 50% du temps de debugging.
- Les tests de caractérisation avant refactoring sont non-négociables — sans eux, le refactoring introduit plus de bugs qu'il n'en corrige. Chez FinApp, 3 bugs ont été introduits pendant le refactoring et détectés immédiatement par les tests de caractérisation, évitant des incidents en production.
- Le 20% du sprint dédié au refactoring est un investissement rentable dès le 2ème trimestre en vélocité récupérée. La clé est de protéger ce créneau — chez FinApp, les premières tentatives sans créneau protégé échouaient car le refactoring était systématiquement sacrifié sous pression de deadline.
- Les Value Objects (Money, DateRange) éliminent des classes entières de bugs — chez FinApp, 40% des bugs de calcul provenaient de confusions entre centimes et euros, ou entre dates UTC et dates locales. Les Value Objects rendent ces erreurs impossibles à la compilation.

---

## Cas 2 : Transformation TDD dans une équipe produit

### Contexte
CloudSecure, éditeur SaaS de cybersécurité (60 personnes, 15 développeurs répartis en 3 squads), développe un dashboard de monitoring de sécurité en temps réel pour les SOC (Security Operations Centers) d'entreprises mid-market. Le frontend est en React/TypeScript, le backend en Go, avec une pipeline de données Kafka pour l'ingestion d'événements de sécurité. L'entreprise est en forte croissance (x2 en ARR sur l'année) et recrute activement. L'équipe ship des features rapidement — 4 releases par semaine — mais accumule des bugs critiques en production : 12 incidents P1 en 3 mois, principalement sur la logique de détection d'alertes (faux positifs non filtrés, alertes manquées) et le scoring de risque (scores incohérents entre le dashboard et les API). Chaque incident P1 coûte en moyenne 8h d'investigation et de correction, plus une communication client chronophage.

### Problème
L'équipe pratique le "test-after" — les tests sont écrits après le code, souvent sous pression de deadline en fin de sprint, et se limitent à valider les happy paths. Le module de scoring de risque (200 règles métier codées dans 15 fichiers Go) a une couverture de 40% mais un mutation score de seulement 15% — les tests existent mais ne valident rien de significatif (assertions trop lâches, mocks excessifs qui ne testent que le wiring). Les code reviews prennent 3 jours en moyenne car les PRs font 800-1200 lignes sans tests pertinents, rendant la relecture exhaustive. Les développeurs seniors passent 40% de leur temps en firefighting (investigation d'incidents) au lieu de mentorer et architecturer. Le VP Engineering estime que le coût des bugs (investigation + correction + communication client) représente 25% du budget engineering.

### Approche
1. **Formation TDD immersive** : Workshop de 3 jours animé par un coach externe spécialisé en TDD (formateur certifié avec 10 ans d'expérience). Programme : jour 1 — cycle Red-Green-Refactor sur des katas classiques (Bowling Game, Roman Numerals) en Go et TypeScript ; jour 2 — katas plus complexes (Mars Rover, Banking) avec introduction du outside-in TDD (London school) adapté au développement produit ; jour 3 — application sur le codebase réel de CloudSecure, avec un exercice de live-coding sur le module de scoring de risque. Les 15 développeurs participent en 2 groupes. Budget formation : 15K EUR, amorti en 2 mois par la réduction des incidents.
2. **Pair programming systématique** : 2 semaines de pair programming obligatoire entre développeurs formés et non-formés, avec rotation quotidienne des binômes. Un développeur senior "TDD champion" désigné par squad (3 au total) pour accompagner la transition sur la durée. Les TDD champions consacrent 30% de leur temps au mentorat pendant les 2 premiers mois, avec un créneau quotidien de "TDD office hours" pour répondre aux questions. Un canal Slack #tdd-help est créé pour les demandes asynchrones. Le pair programming est progressivement réduit à 2 sessions par semaine après 1 mois.
3. **Trunk-based development** : Migration du feature branching (branches de 5-10 jours, source de PRs massives) vers le trunk-based development avec branches < 1 jour et feature flags (LaunchDarkly). Les PRs passent de 800+ lignes à 150-200 lignes, rendant la review faisable en 30 minutes. Chaque PR contient les tests associés — la règle "no test, no merge" est appliquée automatiquement par un check CI qui vérifie que chaque PR ajoute ou modifie au moins un fichier de test. La transition a nécessité 3 semaines d'accompagnement et une refonte du pipeline CI (de 25 minutes à 8 minutes pour supporter les merges fréquents).
4. **Mutation testing en CI** : Intégration de Stryker (TypeScript) et go-mutesting (Go) dans le pipeline CI avec un seuil minimum de mutation score de 70% sur le nouveau code. Les PRs qui baissent le mutation score global sont bloquées avec un message explicatif. Un dashboard Grafana affiche l'évolution du mutation score par module et par squad, visible par tous. Les squads avec le meilleur mutation score sont reconnues en weekly engineering all-hands — une forme de gamification qui a accéléré l'adoption.

### Résultat
- Incidents P1 réduits de 12 à 2 par trimestre (divisé par 6), avec un coût d'investigation réduit de 8h à 2h en moyenne
- Mutation score du module de scoring passé de 15% à 82%, prouvant que les tests valident réellement le comportement métier
- Temps de code review réduit de 3 jours à 4 heures (PRs plus petites, tests comme documentation du comportement attendu)
- Les développeurs reportent une confiance accrue : 85% se sentent "en sécurité" pour refactorer du code existant (vs 20% avant), mesuré par un survey anonyme trimestriel
- Temps de développement par feature initialement +30% (apprentissage TDD, courbe de 6-8 semaines), puis -15% après 3 mois (moins de bugs, moins de rework, moins de firefighting)
- ROI estimé à 180K EUR/an (réduction des incidents + réduction du temps de review + réduction du rework), pour un investissement initial de 35K EUR (formation + productivité réduite pendant l'apprentissage)

### Leçons apprises
- Le TDD n'est pas une pratique de test mais une pratique de design — il force à penser aux interfaces et aux comportements avant l'implémentation. Chez CloudSecure, les modules développés en TDD ont une complexité cyclomatique moyenne de 4 vs 12 pour les modules legacy, preuve que le TDD produit un design plus simple.
- Le pair programming est le vecteur de transmission le plus efficace pour le TDD — les formations seules ne suffisent pas, la pratique encadrée est indispensable. Les développeurs qui ont fait 2 semaines de pair programming atteignent l'autonomie TDD en 1 mois vs 3 mois pour ceux formés sans pairing.
- Le trunk-based development est le catalyseur naturel du TDD : des PRs petites et fréquentes nécessitent des tests solides pour maintenir la confiance. Inversement, le TDD rend le trunk-based development possible en garantissant que chaque petit changement est testé.
- La résistance au changement est réelle — 3 développeurs sur 15 étaient ouvertement sceptiques au début. Le point de bascule a été le premier mois sans incident P1, qui a converti les sceptiques en ambassadeurs. Patience et résultats mesurables sont les meilleurs arguments.

---

## Cas 3 : Mise en place d'une culture de code review efficace

### Contexte
EduTech, scale-up EdTech de 45 personnes (18 développeurs répartis en 3 squads : Learner, Instructor, Platform), développe une plateforme d'apprentissage en ligne pour les organismes de formation professionnelle. Le stack technique comprend Next.js (App Router), tRPC, Prisma, PostgreSQL, et un déploiement Vercel. L'entreprise est en forte croissance (60% par an) et recrute 6 développeurs dans les 3 prochains mois. Le processus de code review existe depuis la création mais est devenu dysfonctionnel avec la croissance : les PRs stagnent 4-5 jours, les reviewers se concentrent sur le style et le formatting plutôt que la logique métier, et les développeurs juniors (6 sur 18) reçoivent des feedback peu constructifs et parfois humiliants ("c'est pas comme ça qu'on fait ici", "pourquoi tu n'as pas fait X ?" sans explication).

### Problème
Le temps moyen de cycle (du premier commit au merge) est de 7 jours, dont 4-5 jours d'attente en review — les développeurs ouvrent une PR puis passent à autre chose, perdant le contexte quand les commentaires arrivent. Les reviewers approuvent sans commentaire ("LGTM" sans avoir lu le code) sur 40% des PRs, par manque de temps ou de motivation. L'analyse des reviews sur 3 mois montre que 70% des commentaires portent sur le style (indentation, nommage, imports) et 5% seulement sur la logique métier ou la sécurité. 3 développeurs juniors envisagent de quitter l'entreprise (signalé en 1-on-1 à leur manager), citant les reviews comme source principale de frustration et de sentiment d'exclusion. Les bugs en production augmentent de 25% par trimestre car la review ne détecte que des problèmes cosmétiques tandis que les erreurs de logique, les failles de sécurité et les régressions passent inaperçues.

### Approche
1. **Refonte du processus** : Limitation des PRs à 400 lignes de diff maximum (exception documentée au-delà, nécessitant l'approbation du tech lead). Assignation automatique des reviewers par rotation via CODEOWNERS avec un algorithme d'équilibrage de charge (chaque développeur reçoit un nombre équivalent de reviews par semaine). SLA de review : première réponse sous 4 heures ouvrées — un bot Slack rappelle les PRs en attente toutes les 2 heures. Les PRs sans review après 8h sont escaladées au tech lead. La configuration Danger.js en CI bloque les PRs > 400 lignes avec un message explicatif et une suggestion de découpage.
2. **Checklist de review structurée** : Introduction d'une checklist PR couvrant 10 critères organisés par priorité : logique métier (le code fait-il ce que la spec demande ?), sécurité (injection, XSS, IDOR, RLS), performance (N+1, pagination, index manquants), tests (cas nominaux + edge cases + tests négatifs), typage strict (pas de `any`, types discriminants), error handling (erreurs explicites, pas de catch silencieux), accessibilité (WCAG AA pour les composants UI), rétrocompatibilité API, documentation inline (pour la logique non triviale), nommage et lisibilité. Chaque reviewer doit cocher chaque critère explicitement — un check CI vérifie que la checklist est complétée avant le merge.
3. **AI-assisted first pass** : Déploiement de CodeRabbit comme premier reviewer automatique. L'IA détecte les problèmes de style, les code smells, les potentiels bugs et les violations de patterns en 2-3 minutes. Les commentaires de style (formatting, imports, nommage) sont gérés à 90% par l'IA + un linter Biome configuré en pre-commit. Les reviewers humains se concentrent sur la logique métier, le design, les questions architecturales et le mentorat des juniors. Le ratio de commentaires humains pertinents (logique et design) est passé de 30% à 75% après le déploiement de CodeRabbit.
4. **Culture de feedback constructif** : Formation de 2 heures animée par un coach en communication, obligatoire pour les 18 développeurs. Règle cardinale : chaque commentaire critique doit être accompagné d'une suggestion concrète (code example ou lien vers la documentation) ou d'une question ouverte ("As-tu envisagé X ? Voici pourquoi ça pourrait être mieux..."). Introduction de "comment types" avec des préfixes : `[blocking]` pour les problèmes qui doivent être corrigés, `[suggestion]` pour les améliorations optionnelles, `[learning]` pour les commentaires pédagogiques sans action requise. Un feedback 360 anonyme mensuel permet aux développeurs de noter la qualité des reviews reçues.

### Résultat
- Cycle time réduit de 7 jours à 1.5 jour (divisé par 4.7), libérant un contexte-switching significatif
- Temps de première réponse passé de 4-5 jours à 3 heures en moyenne, avec 90% des PRs reviewées sous le SLA de 4h
- PRs "LGTM sans commentaire" réduites de 40% à 8% — les reviewers s'engagent réellement grâce à la checklist
- Bugs détectés en review augmentés de 3x (logique et sécurité vs style), avec 12 vulnérabilités de sécurité détectées en review en 3 mois
- eNPS développeur passé de -5 à +32, mesuré par un survey anonyme trimestriel
- Zéro départ dans les 6 mois suivants — les 3 juniors qui envisageaient de partir citent les reviews comme source d'apprentissage dans le survey de satisfaction

### Leçons apprises
- La taille des PRs est le facteur #1 de qualité de review — au-delà de 400 lignes, la qualité de relecture chute drastiquement (études Microsoft et Google confirment). Chez EduTech, le taux de bugs détectés en review a doublé après la limitation à 400 lignes, simplement parce que les reviewers lisent réellement le code.
- L'IA comme premier reviewer libère les humains pour la réflexion de haut niveau — mais ne remplace jamais le jugement humain sur la logique métier. Chez EduTech, CodeRabbit détecte 90% des problèmes de style mais 0% des erreurs de logique métier. Le vrai gain est le temps libéré pour les commentaires à haute valeur.
- Le feedback constructif est une compétence qui s'apprend — investir dans la formation transforme la culture d'équipe en quelques semaines. Le passage de "c'est pas comme ça" à "[suggestion] As-tu envisagé X ? Voici pourquoi..." change radicalement le vécu des juniors.
- Le SLA de review (4h) est le levier le plus sous-estimé — chez EduTech, 60% du cycle time était du temps d'attente en review. Réduire ce temps a un impact immédiat sur la vélocité sans aucun changement dans le code ou les tests.
