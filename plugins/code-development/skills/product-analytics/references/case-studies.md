# Études de cas — Product Analytics & Instrumentation

## Cas 1 : Implémentation d'un tracking plan structuré

### Contexte
TaskFlow, SaaS de gestion de tâches B2B (30 personnes, 8 développeurs), compte 15K utilisateurs actifs mensuels répartis dans 800 équipes. L'entreprise est positionnée sur le segment des startups et PME tech, en compétition avec des acteurs comme Linear, Asana et Monday.com, avec un différenciateur sur la simplicité et l'intégration native avec les outils de développement (GitHub, GitLab). L'application React/Node.js utilise Google Analytics (GA4) pour le trafic et quelques événements Mixpanel ajoutés ad hoc par différents développeurs au fil des 3 années de développement, sans coordination ni convention. L'équipe produit (3 PMs) demande régulièrement des données qui n'existent pas ou sont contradictoires entre GA4 et Mixpanel. Un data analyst a été recruté il y a 6 mois mais passe la majorité de son temps à nettoyer les données au lieu de les analyser.

### Problème
Le tracking est chaotique et inutilisable en l'état : 180 événements existent dans Mixpanel dont 60% ne sont plus émis (code mort laissé par d'anciens développeurs), 30% utilisent des naming conventions différentes (`button_click`, `ButtonClicked`, `btn-click`, `click_button`) rendant impossible l'agrégation, et les propriétés sont inconsistantes (un même événement `task_created` a `user_id` dans une page, `userId` dans une autre, et `uid` dans une troisième). Le PM principal demande "quel est notre taux d'activation ?" — personne ne peut répondre car l'événement d'activation n'existe pas et la définition même d'activation n'a jamais été formalisée. Le data analyst passe 70% de son temps à nettoyer les données (déduplication, normalisation, correction de types) au lieu de les analyser, générant une frustration visible et un risque de départ. Le coût combiné de GA4 Premium et Mixpanel est de 2500 EUR/mois pour des données largement inutilisables.

### Approche
1. **Audit et nettoyage** : Inventaire complet des 180 événements existants via l'API Mixpanel, croisé avec une recherche dans le codebase (grep des appels `track()`, `analytics.track()`, `mixpanel.track()`). Classification en 3 catégories : 35 événements utiles et correctement implémentés (gardés tels quels), 45 utiles mais mal nommés ou avec des propriétés inconsistantes (à corriger et migrer), 100 inutiles — code mort, événements de debug oubliés, duplications (à supprimer). Suppression du code de tracking mort en 15 PRs ciblées. L'audit a pris 1 semaine avec le data analyst et un développeur senior.
2. **Tracking plan structuré** : Conception d'un tracking plan de 50 événements alignés sur les parcours utilisateur clés (onboarding, gestion quotidienne, collaboration, facturation) avec la convention Object-Action (Title Case) : `Task Created`, `Project Viewed`, `Subscription Started`, `Invite Sent`, `Integration Connected`, etc. Chaque événement documenté dans un spreadsheet collaboratif avec : propriétés (nom en snake_case, type TypeScript, valeurs attendues avec exemples), déclencheur précis (quelle action utilisateur, côté client ou serveur), owner squad, et priorité P1/P2/P3. Le tracking plan a été co-conçu en atelier de 2 jours avec les 3 PMs, le data analyst et 2 développeurs seniors. La définition d'activation a été formalisée pour la première fois : "complétion de 3 tâches dans un projet en J1".
3. **Schéma validé en CI** : Le tracking plan est stocké en JSON Schema dans le repo (`/tracking/schema.json`). Un middleware de validation vérifie chaque événement émis en développement et en staging — un événement non conforme au schéma (nom incorrect, propriété manquante, type invalide) déclenche un warning en dev et une erreur bloquante en staging. Le pipeline CI échoue si un événement ne respecte pas le schéma, empêchant les régressions de tracking. Un script de migration automatisé vérifie que tout appel `track()` dans le codebase correspond à un événement du tracking plan. Les ajouts d'événements nécessitent une modification du JSON Schema en PR, revue par le data analyst.
4. **Migration vers PostHog** : Remplacement de GA4 + Mixpanel par PostHog (self-hosted sur AWS, données en EU) comme outil unique. Event tracking, funnel analysis, retention, session replay, et feature flags dans un seul outil — supprimant la fragmentation entre 2 outils. Implémentation hybride : tracking client-side (SDK PostHog React) pour les interactions UX (clics, navigation, scroll depth), tracking server-side (SDK PostHog Node.js) pour les événements business critiques (création de tâche, paiement, invitation), garantissant une fiabilité de 100% indépendamment des adblockers. Un proxy reverse (Next.js API route `/ingest`) masque les appels PostHog client-side, contournant 95% des adblockers.

### Résultat
- 50 événements propres et documentés (vs 180 chaotiques) — 100% conformes au schéma, avec zéro régression de tracking en 6 mois
- Le data analyst consacre 80% de son temps à l'analyse stratégique (vs 30% avant) — funnels, cohortes, segmentation, rapports hebdomadaires pour le product board
- Temps de réponse aux questions produit passé de "3-5 jours de requêtes ad hoc" à "15 minutes via les dashboards self-service" — les PMs sont autonomes sur 90% de leurs questions
- Taux d'activation défini et mesuré pour la première fois : 32% (complétion de 3 tâches en J1), avec un objectif de 45% fixé pour le trimestre suivant et un plan d'action associé
- Coût analytics réduit de 40% (un outil PostHog self-hosted au lieu de GA4 + Mixpanel, les données restant en EU pour la conformité RGPD)
- Zéro événement non-conforme en production grâce à la validation CI — le tracking plan est garanti par le code, pas par la discipline humaine

### Leçons apprises
- Le tracking plan est le fondement de tout — sans lui, chaque événement ajouté augmente le chaos. Investir 2 semaines de planification (atelier + documentation + schéma) économise des mois de nettoyage ultérieur. Chez TaskFlow, les 180 événements chaotiques ont coûté 6 mois de temps data analyst en nettoyage avant de devenir exploitables.
- La validation de schéma en CI est le garde-fou le plus efficace — elle empêche la régression du tracking plan automatiquement. Sans elle, le tracking plan se dégrade en 3 mois sous la pression des sprints (un développeur pressé ajoute un `track('click')` au lieu de `track('Button Clicked')` et personne ne le remarque).
- PostHog self-hosted offre un excellent rapport fonctionnalités/coût pour les startups — analytics, session replay, feature flags et expérimentation dans un seul outil, avec les données en EU. Le proxy reverse pour contourner les adblockers augmente la couverture de données de 30%.
- Le tracking server-side pour les événements business est non-négociable — les adblockers bloquent 35% des événements client-side chez TaskFlow. Un paiement ou une activation non trackée rend les métriques produit inexactes et les décisions qui en découlent erronées.

---

## Cas 2 : Culture d'expérimentation A/B testing

### Contexte
ShopDirect, e-commerce D2C de produits cosmétiques (50 personnes), réalise 12M EUR de CA annuel avec un site Next.js et un trafic de 500K visiteurs uniques par mois. L'entreprise est positionnée sur le segment premium de la cosmétique naturelle, avec un panier moyen de 65 EUR et un taux de conversion de 2.8%. L'équipe fait des "tests A/B" depuis 6 mois avec Google Optimize (déprécié puis arrêté) puis Optimizely (côté client), mais les résultats sont décevants et remettent en question l'investissement : sur 20 tests lancés, 15 sont déclarés "non-conclusifs", 3 ont été stoppés prématurément par le product manager ("le variant B semble gagner"), et seulement 2 ont été correctement menés à terme. L'équipe data comprend un data analyst junior qui gère les tests en parallèle de ses autres responsabilités.

### Problème
L'analyse des 20 tests par un consultant data révèle des problèmes méthodologiques graves qui invalident la quasi-totalité des résultats : aucun calcul de sample size préalable (tests lancés avec 500 visiteurs quand 5000 sont statistiquement nécessaires pour détecter un effet de 10%), peeking systématique (le PM vérifie les résultats quotidiennement et arrête le test dès que p < 0.05 — cette pratique d'arrêt prématuré gonfle le taux de faux positifs à 30%+), pas de guardrail metrics (un test a augmenté le taux de clic "Ajouter au panier" de 20% mais réduit le taux de conversion final de 15% — un résultat net négatif déployé en production pendant 3 semaines avant détection), et pas de suivi post-déploiement (impossible de mesurer l'impact réel d'un test "gagnant" après son déploiement à 100%). Le coût d'Optimizely est de 3000 EUR/mois pour des résultats largement non fiables.

### Approche
1. **Framework d'expérimentation rigoureux** : Template obligatoire pour chaque test, revu par le data analyst avant le lancement : hypothèse formalisée en If/Then/Because ("Si nous ajoutons des avis clients sur la page produit, alors le taux de conversion augmentera de 8%, car la preuve sociale réduit l'anxiété d'achat"), primary metric (taux de conversion checkout), guardrail metrics (revenue par session, taux de retour, NPS), MDE (Minimum Detectable Effect) souhaité (8%), sample size calculé via le calculateur Evan Miller (5200 visiteurs par variant pour un power de 80% et un alpha de 5%), et durée estimée (18 jours à 290 visiteurs/jour par variant). Le template est stocké dans Notion avec un historique de tous les tests passés.
2. **Migration vers Statsig** : Remplacement d'Optimizely par Statsig (warehouse-native, connecté au data warehouse Snowflake de ShopDirect). Assignment server-side via le SDK Next.js pour la consistance (plus de flickering visible côté client qui biaisait les résultats). Bayesian analysis pour des résultats plus intuitifs : probabilité de victoire (ex : "92% de probabilité que le variant B améliore la conversion") au lieu de p-values incompréhensibles pour les non-statisticiens. Guardrail metrics automatisées : si une guardrail se dégrade de >5%, le test est automatiquement stoppé et une alerte est envoyée.
3. **Séquentiel testing** : Adoption du sequential testing (GroupSequential) qui permet de vérifier les résultats à intervalles réguliers prédéfinis (J3, J7, J14, J21) sans inflater le taux de faux positifs — chaque vérification utilise une borne de décision ajustée (alpha spending function). Les tests sont "monitorés" en continu avec des bornes de futilité : si à mi-parcours la probabilité de détecter un effet est < 10%, le test est arrêté pour futilité (économie de trafic). Cette approche réduit la durée médiane des tests de 30% tout en maintenant la rigueur statistique.
4. **Culture d'expérimentation** : Chaque squad doit avoir au moins 1 test actif en permanence — l'expérimentation est un KPI de squad. Résultats partagés en weekly all-hands avec apprentissages (y compris les tests négatifs, présentés positivement comme des "apprentissages qui ont sauvé X EUR"). "Test of the Month" pour l'insight le plus impactant (positif ou négatif), voté par l'équipe. Un channel Slack #experiments partage les résultats en temps réel. Le CEO participe activement aux discussions, envoyant le signal que l'expérimentation est une priorité stratégique.

### Résultat
- Tests conclusifs passés de 25% (5/20) à 70% (14/20) — le sample size correct dès le départ élimine les tests sous-dimensionnés
- Taux de faux positifs réduit de ~30% à < 5% (fin du peeking grâce au sequential testing avec bornes prédéfinies)
- 5 tests gagnants implémentés en 6 mois avec un impact cumulé de +18% sur le taux de conversion (de 2.8% à 3.3%)
- Revenue uplift mesuré et validé post-déploiement : +1.8M EUR annualisé attribuable aux tests A/B, soit un ROI de 50x sur le coût des outils et du temps d'analyse
- Guardrail metrics : zéro test déployé avec une régression non détectée sur le revenue par session — les 2 tests avec régression de guardrail ont été stoppés automatiquement
- Temps moyen d'un test réduit de 6 semaines à 3 semaines grâce au sequential testing (arrêt précoce quand le résultat est clair ou futile)

### Leçons apprises
- Le calcul de sample size AVANT le lancement est la différence entre un test scientifique et une opinion déguisée en données — 80% des tests "non-conclusifs" chez ShopDirect échouaient simplement par manque de trafic. Un test sous-dimensionné ne prouve rien, même s'il affiche un résultat.
- Le sequential testing est le meilleur compromis entre rigueur et vitesse — il permet de monitorer les résultats à intervalles réguliers sans inflater les erreurs. La borne de futilité est particulièrement précieuse : arrêter un test voué à l'échec après 1 semaine au lieu de 3 libère du trafic pour d'autres tests.
- Partager les tests perdants est aussi important que les tests gagnants — les apprentissages négatifs évitent de répéter les mêmes erreurs. Chez ShopDirect, un test négatif ("les pop-ups de réduction augmentent le taux de conversion immédiat mais réduisent le taux de retour de 20%") a empêché 3 squads de faire la même erreur.
- L'assignment server-side est indispensable pour les tests e-commerce — le flickering côté client (le visiteur voit brièvement la page originale avant le variant) biaise les résultats et dégrade l'expérience. Chez ShopDirect, la migration vers l'assignment server-side a augmenté la fiabilité des résultats de 15%.

---

## Cas 3 : Migration vers une analytics privacy-first conforme RGPD

### Contexte
SantéPlus, plateforme de téléconsultation (60 personnes, 500K utilisateurs inscrits, 80K consultations/mois), est un acteur majeur de la santé numérique en France. L'entreprise traite des données de santé soumises au RGPD renforcé et aux recommandations spécifiques de la CNIL sur les données de santé. L'application web React utilise Google Analytics (GA4), Hotjar (session replay), et 8 scripts de remarketing tiers (Google Ads, Meta Pixel, Criteo, etc.). Un CMP (Consent Management Platform) Axeptio est installé mais mal configuré : une analyse technique révèle que les scripts GA4 et Hotjar se chargent via le tag manager avant le consentement dans certains parcours (page d'accueil et landing pages marketing), une violation flagrante. L'équipe technique comprend 12 développeurs, un data analyst et un DPO à mi-temps.

### Problème
Un contrôle CNIL déclenché par une plainte d'un utilisateur révèle une non-conformité : les cookies analytics et marketing sont déposés avant le consentement utilisateur (violation de l'article 82 de la loi Informatique et Libertés et de l'article 5 du RGPD). L'entreprise reçoit une mise en demeure publique avec un délai de 3 mois pour se mettre en conformité, sous peine d'amende pouvant atteindre 4% du CA. La publicité de la mise en demeure cause un damage réputationnel immédiat : 3 articles de presse, 15% d'augmentation des demandes de suppression de compte. De plus, 35% du trafic est bloqué par les adblockers (GA4 et Hotjar sont dans les listes de blocage standard), rendant les données incomplètes. L'équipe produit prend des décisions sur 65% des données réelles — une marge d'erreur inacceptable pour des décisions d'investissement en acquisition et en product development. Le taux de consentement analytics est de 45% en moyenne, aggravant encore la perte de données.

### Approche
1. **Audit complet des traceurs** : Inventaire de tous les scripts, cookies et pixels via un audit technique en 3 étapes : scan automatisé Cookiebot (détection de tous les cookies et scripts), analyse manuelle du code source (grep des appels à des domaines tiers dans le HTML et le JavaScript), et vérification réseau via les DevTools Chrome sur les 20 parcours utilisateur principaux. Résultat : 23 cookies dont 8 déposés avant consentement (GA4, Hotjar, 6 pixels marketing). Classification : 4 strictement nécessaires (session, CSRF, langue, préférences cookie — exemptés de consentement), 6 analytics (GA4, Hotjar, Mixpanel — soumis à consentement), 13 marketing/remarketing (Google Ads, Meta, Criteo, etc. — soumis à consentement). L'audit a pris 1 semaine et a impliqué le DPO, un développeur et le data analyst.
2. **PostHog self-hosted + exemption CNIL** : Remplacement de GA4 par PostHog self-hosted sur l'infrastructure propre de l'entreprise (Docker sur AWS eu-west-3 Paris, données en EU exclusivement). Configuration conforme aux lignes directrices CNIL pour l'exemption analytics first-party (délibération CNIL du 2 juin 2023) : pas de cross-site tracking (domaine first-party uniquement), pas d'identifiant persistant au-delà de 13 mois, données anonymisées automatiquement après 25 mois, pas de transfert de données hors UE, pas de recoupement avec d'autres traitements. Cette configuration précise permet l'exemption de consentement pour l'analytics — 100% des visiteurs sont trackés sans bandeau cookie pour l'analytics. La mise en conformité a été validée par le DPO et un cabinet d'avocats spécialisé RGPD.
3. **Server-side tracking** : Migration du tracking client-side vers le server-side pour les événements business critiques. Le SDK PostHog côté serveur (Node.js) enregistre les conversions (consultation terminée), les inscriptions (compte créé), les abonnements (paiement effectué), et les événements de parcours de soin (ces données ne doivent jamais transiter par un script tiers). Le tracking server-side est invisible aux adblockers et garantit une couverture de 100% des événements business. Pour le tracking client-side résiduel (interactions UX, scroll, clics), un proxy reverse via une route API Next.js (`/api/ph-ingest`) masque les appels PostHog, contournant 95% des adblockers.
4. **Suppression des scripts marketing** : Suppression de 8 scripts de remarketing client-side (Google Ads pixel, Meta Pixel, Criteo tag, etc.) remplacés par les Conversions APIs server-to-server de Meta (CAPI) et Google (Enhanced Conversions API) qui envoient les événements de conversion directement depuis le serveur sans utiliser de cookies tiers. Les 5 scripts restants (widget de chat Intercom, outil de support, Axeptio CMP, service de visioconférence, et CDN) sont classés comme strictement nécessaires ou conditionnés au consentement via Axeptio correctement reconfiguré (chargement bloqué par défaut, activation uniquement après consentement explicite).

### Résultat
- Conformité CNIL validée — la mise en demeure a été levée après l'audit de remediation conduit par un expert indépendant mandaté par la CNIL, avec zéro non-conformité résiduelle
- Couverture de données passée de 65% à 97% (server-side + exemption CNIL = tracking sans consentement pour l'analytics, insensible aux adblockers)
- Cookies tiers réduits de 23 à 4 (strictement nécessaires uniquement sans consentement), éliminant 19 sources potentielles de non-conformité
- PostHog self-hosted coûte 60% de moins que GA4 Premium + Hotjar combinés (1K EUR/mois vs 2.5K EUR/mois), avec des fonctionnalités supplémentaires (session replay, feature flags)
- Taux de consentement analytics : non applicable (exemption CNIL) — 100% des visiteurs trackés en analytics first-party, vs 45% avant avec le consentement
- Décisions produit basées sur 97% des données au lieu de 65% — la précision des métriques de conversion et de rétention est significativement améliorée, permettant des décisions d'investissement plus fiables

### Leçons apprises
- L'exemption CNIL pour l'analytics first-party est un avantage stratégique majeur — elle permet de tracker 100% des visiteurs sans consentement, à condition de respecter les conditions strictes (self-hosted, first-party, pas de cross-site, anonymisation). C'est un avantage concurrentiel par rapport aux entreprises qui dépendent de GA4 et ne peuvent tracker que 45-60% de leurs visiteurs.
- Le server-side tracking est la solution définitive aux adblockers et à la dépréciation des cookies tiers — pour les événements business critiques (conversions, inscriptions, paiements), il doit être la méthode par défaut, pas un fallback. Chez SantéPlus, 35% des inscriptions n'étaient pas trackées avant la migration server-side.
- Les scripts marketing tiers sont la principale source de non-conformité RGPD et de fuite de données — les Conversions APIs server-to-server offrent les mêmes fonctionnalités (attribution, optimisation des campagnes) sans cookies tiers. La migration a pris 2 semaines et n'a pas dégradé les performances des campagnes publicitaires.
- La mise en demeure CNIL, bien que douloureuse (3 mois de stress, coût juridique de 25K EUR, damage réputationnel), a été le catalyseur d'une transformation qui aurait dû être faite proactivement. Le coût de la conformité proactive (PostHog + migration server-side) est de 15K EUR — 10x moins cher que la remédiation sous pression.
