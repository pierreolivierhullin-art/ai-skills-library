# Études de cas — Monitoring & Observability

## Cas 1 : Déploiement d'OpenTelemetry pour une architecture microservices

### Contexte
LogiSoft, éditeur SaaS de logistique (80 personnes, 20 développeurs), opère 18 microservices sur Kubernetes pour une plateforme de gestion des expéditions utilisée par 300 transporteurs et 2000 e-commerçants. L'entreprise traite 50K commandes/jour et est positionnée comme le leader français du TMS (Transport Management System) en mode SaaS. Chaque service utilise un outil de monitoring différent selon l'époque de sa création et les préférences du développeur : 5 services avec Datadog APM, 4 avec New Relic, 6 avec du logging brut vers CloudWatch, et 3 sans aucune instrumentation (les plus anciens). L'équipe SRE (3 personnes) navigue entre 4 dashboards différents pour diagnostiquer un incident, sans aucune vue unifiée de la plateforme. Le coût total de monitoring atteint 8K EUR/mois répartis entre 3 vendors.

### Problème
Un incident majeur (perte de 2300 commandes pendant 45 minutes un vendredi soir) prend 4 heures à diagnostiquer car l'équipe ne peut pas tracer une requête de bout en bout. Le service de routing des commandes appelle le service d'inventaire qui appelle le service de pricing — mais les traces s'arrêtent à chaque frontière de service car les outils ne propagent pas le même context de trace (Datadog utilise `x-datadog-trace-id`, New Relic utilise `newrelic-trace-id`, CloudWatch n'a aucune propagation). Le MTTD (Mean Time to Detect) est de 25 minutes car les alertes ne sont pas corrélées entre les vendors. Le MTTR est de 3.5 heures car le diagnostic nécessite de reconstituer manuellement la chaîne d'appels entre les 4 dashboards. La root cause (un timeout du service de pricing dû à un cache Redis expiré) n'a été identifiée qu'après 3 heures de debugging. Les 2300 commandes perdues ont coûté 180K EUR en indemnisations clients et 15 points de NPS.

### Approche
1. **OpenTelemetry comme couche unique** : Migration vers le SDK OpenTelemetry pour tous les 18 services, remplaçant les agents propriétaires Datadog et New Relic. Auto-instrumentation pour les frameworks HTTP (Express, Fastify), gRPC, les clients PostgreSQL et Redis, éliminant 80% du travail d'instrumentation. Injection de custom spans sur la logique métier critique : traitement de commande (avec l'ID commande comme attribut), calcul de route optimale, vérification de stock en temps réel, et calcul de tarification. Chaque span contient les attributs métier (`order_id`, `carrier_id`, `customer_id`) permettant de filtrer les traces par entité business. La migration a été réalisée service par service sur 6 semaines, en commençant par le service de routing (le plus critique).
2. **OTel Collector en gateway mode** : Déploiement d'un OTel Collector en mode gateway (3 replicas pour la haute disponibilité) qui reçoit toutes les traces, métriques et logs des 18 services. Le Collector enrichit les données (ajout de metadata K8s : pod name, namespace, node, deployment version), filtre le bruit (suppression des health checks, des requêtes de monitoring internes), et exporte vers un backend unique : Grafana Cloud (Tempo pour les traces, Mimir pour les métriques, Loki pour les logs). Le Collector utilise le batch processor pour optimiser les envois réseau et le memory limiter pour prévenir les OOM en cas de pic de télémétrie.
3. **Tail-based sampling** : Configuration du sampling dans le Collector via le tail_sampling processor : 100% des traces avec erreurs (status code >= 400) sont conservées, 100% des traces lentes (> 2s de durée totale) sont conservées, 100% des traces contenant des span events d'exception sont conservées, et 5% des traces normales sont échantillonnées aléatoirement pour la baseline. Réduction de 80% du volume de données envoyées à Grafana Cloud tout en conservant 100% du signal utile pour le debugging. Le coût de stockage Grafana Cloud est calibré sur le volume post-sampling.
4. **Correlation IDs systématiques** : Injection d'un `trace_id` et d'un `request_id` dans chaque log line via le contexte OpenTelemetry, automatiquement par le SDK sans modification du code applicatif. Un clic dans Grafana sur un log affiche la trace complète (tous les services traversés), et vice versa — clic sur un span affiche les logs émis pendant ce span. Le `trace_id` est également propagé dans les messages Kafka (header custom), permettant de tracer une commande de bout en bout même à travers les communications asynchrones. Un dashboard Grafana "Incident Investigation" combine trace, logs et métriques sur une timeline unique pour chaque `trace_id`.

### Résultat
- MTTD réduit de 25 min à 3 min (alertes sur les traces d'erreur avec attribution automatique du service responsable)
- MTTR réduit de 3.5h à 35 min (trace end-to-end visible en un clic, plus besoin de naviguer entre 4 dashboards)
- Coûts monitoring réduits de 8K EUR à 3.2K EUR/mois (un seul vendor Grafana Cloud, sampling intelligent réduisant le volume de 80%)
- 100% des services instrumentés avec propagation de contexte — zéro trou dans les traces, y compris à travers les messages Kafka
- Onboarding monitoring d'un nouveau service : 15 minutes (auto-instrumentation OTel + templates Grafana), contre 2 jours auparavant (configuration manuelle vendor-specific)
- L'incident type "commandes perdues" est désormais détecté en 3 minutes et résolu en 20 minutes — comparé aux 4h25 de l'incident initial

### Leçons apprises
- OpenTelemetry élimine le vendor lock-in et unifie l'instrumentation — c'est le standard incontournable pour toute nouvelle architecture. Chez LogiSoft, la migration a pris 6 semaines mais a éliminé la dette technique de monitoring accumulée sur 4 ans.
- Le tail-based sampling est crucial pour le rapport coût/signal — sans lui, le volume de données explose et les coûts deviennent prohibitifs. Chez LogiSoft, le volume brut aurait coûté 12K EUR/mois sur Grafana Cloud ; avec le sampling, il coûte 3.2K EUR/mois sans perte de signal utile.
- La corrélation traces-logs-métriques dans un seul outil transforme le diagnostic d'incident — passer de 3 dashboards à 1 divise le MTTR par un facteur significatif. L'investissement dans le dashboard "Incident Investigation" unifié a été le plus rentable du projet.
- L'enrichissement des spans avec des attributs métier (`order_id`, `carrier_id`) est ce qui rend les traces réellement utiles pour le debugging — sans eux, les traces sont des lignes techniques incompréhensibles pour les développeurs qui ne connaissent pas l'infrastructure.

---

## Cas 2 : Implémentation d'une stratégie SLO-driven

### Contexte
BookAPI, plateforme de réservation en ligne (60 personnes), expose une API REST servant 15K requêtes/minute pour un réseau de 500 hôtels et 2000 restaurants partenaires. L'entreprise est un acteur majeur de la réservation en ligne en France, avec une forte saisonnalité (trafic x3 en été et pendant les fêtes). L'équipe utilise Datadog avec 200+ alertes configurées accumulées au fil des années par différents SREs et développeurs. Les développeurs sont en astreinte (on-call) par rotation d'une semaine, avec 3 squads de 5 développeurs chacune. Le stack technique comprend Node.js, PostgreSQL, Redis, Elasticsearch, déployé sur AWS EKS.

### Problème
L'alert fatigue est critique et menace la viabilité du programme d'astreinte : les on-call reçoivent 30-50 pages par semaine (PagerDuty), dont 85% sont des faux positifs ou des alertes non-actionnables (CPU spike transitoire, memory usage en croissance lente sans impact, error rate à 1.1% au lieu du seuil de 1%). Un développeur senior a démissionné en citant explicitement le stress de l'astreinte comme raison principale — son départ a aggravé la charge sur les restants. Les alertes sont basées sur des seuils statiques (CPU > 80%, memory > 90%, error rate > 1%) qui ne reflètent pas l'impact réel sur les utilisateurs : un CPU à 85% pendant 30 secondes ne dégrade aucune expérience utilisateur. Paradoxalement, un vrai incident (latence dégradée de 300ms à 4s sur le parcours de réservation pendant 2h) n'a été détecté que par un tweet client viral — aucune des 200+ alertes ne l'avait capté car la dégradation était progressive et ne dépassait aucun seuil statique. L'incident a coûté 25K EUR en réservations perdues.

### Approche
1. **Identification des SLIs critiques** : Atelier de 2 jours avec product managers, engineering leads et le VP Engineering pour identifier les 5 parcours utilisateur critiques : recherche de disponibilité, réservation, paiement, annulation, et consultation de réservation. Pour chaque parcours : définition d'un SLI d'availability (% de requêtes réussies avec status 2xx en < 3s, mesuré côté serveur au niveau du load balancer) et d'un SLI de latency (p95 < 800ms, mesuré de l'entrée de la requête au retour de la réponse). Les SLIs sont implémentés comme des métriques Datadog custom, calculées à partir des logs ALB et des traces OpenTelemetry.
2. **Définition des SLOs** : SLOs définis sur des fenêtres glissantes de 30 jours après analyse de 6 mois de données historiques : availability 99.9% (43 min d'erreur tolérées/mois, soit un error budget de 43.2 minutes), latency p95 99.5% (< 800ms pour 99.5% des requêtes). Les SLOs sont calibrés pour être ambitieux mais atteignables — les données historiques montraient une availability moyenne de 99.7%, le SLO de 99.9% force l'amélioration sans être irréaliste. Error budgets calculés automatiquement et affichés sur un dashboard Datadog visible par tous (engineering, product, management).
3. **Burn rate alerting** : Remplacement de toutes les 200+ alertes statiques par 10 alertes burn rate (2 par parcours critique). Fast burn alert : budget consommé 14.4x plus vite que prévu sur 1h (page PagerDuty immédiate — correspond à une dégradation sévère qui consommerait tout le budget en 2 jours). Slow burn alert : budget consommé 3x plus vite sur 6h (notification Slack channel #reliability — correspond à une dégradation subtile qui consommerait le budget en 10 jours). Les 190 alertes restantes sont supprimées ou converties en alertes informatives non-paginantes. Chaque alerte burn rate a un runbook associé détaillant le diagnostic et les actions de remédiation.
4. **Error budget policies** : Trois zones définies avec des actions claires, formalisées dans un document signé par le VP Engineering et le VP Product : Budget > 50% restant → ship features normalement, optimiser la performance, les changements risqués sont autorisés ; Budget 25-50% → mode prudent, review supplémentaire pour les changements à risque, pas de migration de base de données ni de changement d'infrastructure ; Budget < 25% → feature freeze, 100% du temps engineering consacré à la fiabilité, postmortem obligatoire pour chaque incident, et revue d'architecture. Le dashboard error budget est projeté sur un écran dans l'open space engineering.

### Résultat
- Pages on-call réduites de 35/semaine à 3/semaine (divisé par 12) — 100% actionnables, chaque page correspond à un impact réel sur les utilisateurs
- MTTD pour les vrais incidents réduit de "aléatoire" (parfois détecté par Twitter) à 5 minutes — le burn rate détecte les dégradations progressives que les seuils statiques manquent
- Satisfaction on-call (survey interne) passée de 2/10 à 7.5/10, et le programme d'astreinte n'est plus un frein au recrutement
- Disponibilité réelle améliorée de 99.7% à 99.95% — les incidents sont détectés et résolus plus vite, et l'error budget policy force l'investissement proactif en fiabilité
- Dialogue product/engineering objectivé : "il nous reste 40% d'error budget ce mois, on peut shipper cette feature risquée" — la fiabilité est devenue une donnée, pas une opinion
- Zéro incident non détecté depuis l'implémentation (8 mois), contre 3 incidents détectés par les clients sur les 6 mois précédents

### Leçons apprises
- Les SLOs transforment le dialogue entre product et engineering — l'error budget rend la fiabilité tangible et négociable. Chez BookAPI, le VP Product utilise désormais le dashboard error budget dans ses sprints de priorisation, un changement culturel majeur.
- Le burn rate alerting élimine les faux positifs car il mesure l'impact cumulé, pas les pics instantanés — un CPU à 90% pendant 10 secondes ne page plus, mais une latence à 1.5s pendant 30 minutes page immédiatement car elle consume le budget.
- Les error budget policies doivent être formalisées et acceptées par toute l'organisation — sans engagement du management, elles sont ignorées sous pression business. Chez BookAPI, la première activation du feature freeze (budget < 25%) a été respectée malgré une deadline commerciale, ce qui a établi la crédibilité du process.
- La suppression de 190 alertes sur 200 est émotionnellement difficile — chaque alerte a été créée par quelqu'un qui pensait qu'elle était importante. Un audit de 2 semaines avec chaque squad a été nécessaire pour valider la suppression et rassurer les équipes.

---

## Cas 3 : Culture postmortem et amélioration continue

### Contexte
StreamFlow, plateforme de streaming vidéo B2B (45 personnes, 12 développeurs), fournit une infrastructure de diffusion en direct pour 150 clients (événements corporate, formations en ligne, webinars). L'entreprise traite 500 streams simultanés en pic avec une audience cumulée de 50K spectateurs. Le stack technique comprend une architecture microservices (8 services) en Go et Node.js, déployée sur AWS EKS, avec MediaLive pour l'encodage vidéo et CloudFront pour la distribution. L'entreprise subit 2-3 incidents majeurs par mois (perte de service > 15 min). L'équipe traite les incidents en mode pompier — le problème est résolu, un email laconique est envoyé ("le service a été restauré"), et on passe à autre chose sans analyse structurée.

### Problème
Analyse sur 6 mois réalisée par le nouveau VP Engineering : 40% des incidents sont des récurrences de problèmes déjà rencontrés, ce qui représente un gaspillage estimé à 120 heures d'engineering par trimestre. La base de données de cache Redis sature tous les 3-4 semaines (même cause — croissance du volume de sessions sans ajustement du maxmemory — jamais corrigée durablement car le workaround consiste à faire un `FLUSHALL` manuel). Les migrations de base de données cassent la production une fois par mois (process non standardisé, pas de validation en staging). Il n'existe aucun postmortem documenté, aucune action corrective suivie, et les développeurs en astreinte appliquent les mêmes workarounds manuels à chaque occurrence. Le MTTR médian est de 45 minutes car le diagnostic repart de zéro à chaque incident, même pour des problèmes connus. 3 clients importants ont menacé de résilier leur contrat en citant la fiabilité insuffisante.

### Approche
1. **Processus d'incident structuré** : Définition de niveaux de sévérité formalisés (SEV1: perte totale de service affectant > 50% des utilisateurs, SEV2: dégradation majeure affectant > 10% des utilisateurs, SEV3: dégradation mineure ou un client isolé). Pour chaque SEV1/SEV2 : un incident commander est désigné automatiquement (le on-call senior), un canal Slack dédié est créé par un bot (#inc-YYYYMMDD-description), et une timeline d'incident est documentée en temps réel dans le canal. Un bot PagerDuty notifie automatiquement le VP Engineering pour les SEV1 et crée un Google Doc de timeline pré-rempli avec le template d'incident. L'incident commander est formé via un workshop de 4h sur la gestion d'incident (communication, priorisation, escalade).
2. **Postmortems blameless systématiques** : Postmortem obligatoire pour tout SEV1/SEV2 dans les 48h suivant la résolution. Template structuré : timeline avec timestamps exacts (extraite du canal Slack), impact quantifié (nombre d'utilisateurs affectés, durée, revenus perdus), root cause analysis avec la technique des 5 Whys (minimum 3 niveaux de profondeur), what went well (ce qui a fonctionné dans la réponse), what can be improved (ce qui a ralenti la résolution), et action items avec owner, deadline et priorité. Le postmortem est présenté en réunion d'équipe par l'incident commander — le focus est sur les systèmes et les processus, jamais sur les individus. La phrase "qui a causé l'incident ?" est explicitement bannie au profit de "comment le système a permis que cela arrive ?".
3. **Action items trackés** : Chaque action item du postmortem est créé comme ticket Linear avec une deadline, un owner nommé, et un label "postmortem". Un dashboard "Postmortem Actions" agrège tous les tickets et est revu chaque lundi en weekly engineering meeting par le VP Engineering. Les actions non complétées après 2 sprints sont escaladées au VP Engineering qui en fait un sujet de 1-on-1 avec le squad lead concerné. Le taux de complétion des actions est un KPI de l'équipe engineering, suivi mensuellement. Les actions sont classées en 3 catégories : prevention (empêcher la récurrence), detection (détecter plus vite), et mitigation (réduire l'impact).
4. **Runbooks opérationnels** : Création d'un runbook pour chaque type d'incident récurrent, stocké dans le repo engineering-docs et lié aux alertes PagerDuty. Le runbook décrit : symptômes observables (quelles métriques sont en alerte, quels logs chercher), procédure de diagnostic (étapes précises avec les commandes à exécuter), procédure de résolution (fix temporaire + fix permanent), et actions préventives déjà en place. Chaque alerte PagerDuty contient un lien direct vers le runbook correspondant. Les runbooks sont maintenus à jour par les on-call : après chaque incident, le on-call vérifie si le runbook était précis et le met à jour si nécessaire.

### Résultat
- Incidents récurrents réduits de 40% à 8% des incidents totaux, libérant 100+ heures d'engineering par trimestre
- MTTR médian réduit de 45 min à 18 min grâce aux runbooks (diagnostic plus rapide — le on-call n'a plus besoin de deviner, il suit la procédure)
- 25 postmortems documentés en 6 mois — base de connaissances consultable et searchable, utilisée comme matériel d'onboarding pour les nouveaux développeurs
- 92% des action items complétés dans les délais (vs 0% avant — il n'y avait aucun tracking), les 8% restants étant des actions dépendantes d'un tiers (AWS support)
- Redis : scaling automatique (autoscaling ElastiCache) implémenté suite à un action item postmortem — plus de saturation depuis 5 mois, éliminant l'incident le plus fréquent
- Migrations : pipeline zero-downtime standardisé (expand-contract) implémenté suite à un action item postmortem — zéro incident de migration depuis 4 mois

### Leçons apprises
- Le postmortem blameless est le mécanisme d'apprentissage le plus puissant d'une organisation d'ingénierie — sans lui, les erreurs se répètent indéfiniment. Chez StreamFlow, la simple mise en place de postmortems a réduit les incidents récurrents de 40% à 8% en 6 mois, sans aucun changement technique majeur.
- Tracker les action items est aussi important que les identifier — un postmortem sans suivi est un exercice de documentation inutile. Chez StreamFlow, la revue hebdomadaire des actions par le VP Engineering a transformé les postmortems de "théâtre de la rigueur" en véritable moteur d'amélioration.
- Les runbooks transforment le temps de résolution pour les problèmes connus — l'investissement de 2 heures de rédaction économise des heures de diagnostic sur chaque occurrence. Chez StreamFlow, le runbook Redis a été utilisé 4 fois avant que le fix définitif ne soit implémenté, économisant environ 6 heures de debugging.
- La culture blameless doit être activement défendue par le management — sans cela, les postmortems dégénèrent en sessions de blame déguisé. Chez StreamFlow, le VP Engineering coupe immédiatement toute question orientée "qui" et redirige vers "comment le système a permis cela", créant un environnement psychologiquement sûr pour la transparence.
