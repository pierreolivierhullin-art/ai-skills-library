# Études de cas — Data Engineering

## Cas 1 : Migration d'un ETL legacy vers une stack moderne ELT avec dbt et Snowflake

### Contexte
Groupe Lefèvre Industries, ETI industrielle de 2 800 salariés basée à Lyon, spécialisée dans la fabrication de composants aéronautiques de haute précision pour Airbus, Safran et Dassault Aviation. L'entreprise occupe une position de tier-2 sur un marché en forte croissance (+7 % par an), mais fait face à une pression croissante des donneurs d'ordres sur la traçabilité et le reporting de production en temps réel. Le système décisionnel repose depuis 12 ans sur une chaîne ETL Informatica PowerCenter alimentant un data warehouse Oracle on-premise hébergé dans un datacenter interne. L'équipe data de 6 personnes (3 développeurs ETL seniors, 1 DBA Oracle, 1 analyste BI et 1 responsable data) passe 70 % de son temps en maintenance corrective des pipelines existants, laissant très peu de bande passante pour les nouveaux projets demandés par les métiers.

### Problème
Les jobs ETL cumulent un temps d'exécution moyen de 9 h 40 par nuit, avec un taux d'échec de 18 % nécessitant des reprises manuelles quotidiennes. Le coût annuel de licences Informatica et Oracle atteint 380 000 EUR, tandis que le délai moyen de livraison d'un nouveau rapport est de 14 semaines, ce qui provoque une frustration croissante chez les directeurs de production et le CFO. La dette technique accumulée rend toute évolution structurelle quasi impossible : les mappings ETL contiennent plus de 3 200 transformations imbriquées, dont la logique n'est documentée nulle part. Trois développeurs clés détiennent à eux seuls la connaissance critique de ces flux, créant un risque opérationnel majeur identifié par l'audit interne. Le COMEX a fixé un ultimatum : réduire les coûts d'infrastructure data de 40 % et le délai de livraison de rapports de 50 % sous 18 mois, sous peine de reconsidérer la stratégie data du groupe.

### Approche
1. **Audit et cartographie des flux** : Inventaire exhaustif des 247 jobs ETL existants sur une période de 6 semaines, mobilisant 2 développeurs seniors et 1 consultant externe spécialisé Informatica. Classification par criticité métier (P1 à P4) et identification des 83 transformations réellement utilisées par les consommateurs finaux. Les 164 jobs restants ont été identifiés comme obsolètes ou redondants, résultat d'une accumulation de 12 ans sans rationalisation. Cette phase a produit une matrice de traçabilité source-cible qui a servi de blueprint pour la migration.
2. **Architecture cible ELT** : Déploiement de Snowflake comme entrepôt cloud après un benchmark de 4 semaines contre BigQuery et Redshift, Fivetran pour l'ingestion des 12 sources (ERP SAP, CRM Salesforce, MES Wonderware, GMAO, pointeuse, 7 fichiers plats partenaires), et dbt Core pour les transformations versionnées sous Git avec 100 % de couverture de tests. L'architecture a été validée par un comité technique incluant le DSI, le RSSI et un architecte cloud externe. Le choix de Snowflake a été déterminé par sa capacité de séparation compute/stockage, essentielle pour gérer les pics de requêtes en fin de mois.
3. **Migration incrémentale par domaine** : Migration progressive sur 5 vagues de 6 semaines chacune, en commençant par le domaine finance (le plus documenté et le mieux testé), suivi de la production, de la qualité, des achats et enfin du commercial. Chaque vague incluait une exécution parallèle ancien/nouveau système pendant 2 semaines avec comparaison automatisée des résultats via un framework de tests de réconciliation développé en interne. Les écarts supérieurs à 0,01 % déclenchaient une investigation systématique. Cette approche a permis de détecter et corriger 47 bugs latents dans les anciens pipelines, dont certains faussaient les données depuis plusieurs années.
4. **Décommissionnement et formation** : Extinction progressive d'Informatica filiale par filiale, avec un point de non-retour fixé à la validation par les métiers de chaque vague. Formation de l'équipe data complète à dbt et SQL analytique (40 h par personne, réparties sur 8 semaines en alternance avec le travail opérationnel), complétée par une certification Snowflake pour les 3 développeurs seniors. Mise en place de revues de code data hebdomadaires calquées sur les pratiques DevOps, avec pull requests obligatoires et documentation automatique via dbt docs, assurant ainsi une transmission de connaissance qui élimine le risque de dépendance aux personnes clés.

### Résultat
- Temps d'exécution nocturne réduit de 9 h 40 à 1 h 15 (baisse de 87 %), avec une fenêtre de traitement terminée avant 2 h du matin contre 11 h auparavant
- Taux d'échec des pipelines passé de 18 % à 2,3 %, avec des alertes automatiques et un temps moyen de résolution tombé de 3 h à 20 min
- Coût annuel d'infrastructure réduit de 380 000 EUR à 145 000 EUR (économie de 62 %), soit un ROI du projet de migration atteint en 14 mois
- Délai de livraison d'un nouveau rapport ramené de 14 semaines à 3 semaines, permettant de résorber un backlog de 23 demandes métier en 6 mois
- Couverture de tests automatisés sur les modèles dbt atteignant 100 %, contre 0 % de tests automatisés sur l'ancien système Informatica
- Satisfaction de l'équipe data mesurée par le NPS interne passée de -15 à +62, avec zéro départ sur les 18 mois suivant la migration

### Leçons apprises
- La migration incrémentale par domaine métier réduit considérablement le risque par rapport à une approche big-bang, mais elle nécessite une discipline rigoureuse de gestion de la coexistence temporaire des deux systèmes pour éviter les incohérences de données entre consommateurs
- L'exécution parallèle ancien/nouveau système est indispensable pour garantir la confiance des métiers dans les données migrées — sans cette preuve concrète, les responsables métier retardent systématiquement la validation par peur de perdre leur source de vérité habituelle
- Investir dans la formation de l'équipe existante plutôt que recruter des profils spécialisés assure une meilleure continuité opérationnelle et préserve la connaissance métier accumulée, à condition de prévoir un accompagnement par un expert externe pendant les 3 premiers mois
- Documenter les bugs découverts dans l'ancien système pendant la migration et les partager avec les métiers est un puissant levier d'adhésion : cela démontre que la migration n'est pas seulement un projet technique mais une amélioration concrète de la fiabilité des données sur lesquelles ils prennent des décisions

---

## Cas 2 : Pipeline de streaming temps réel avec Kafka pour un moteur de recommandation e-commerce

### Contexte
ModaStream, pure player e-commerce français de mode et accessoires, 45 millions d'EUR de chiffre d'affaires annuel, 1,2 million de visiteurs uniques mensuels. L'entreprise se positionne sur le segment premium-accessible et fait face à une concurrence féroce de Zalando, Vestiaire Collective et de marketplaces asiatiques sur le prix. La différenciation repose sur la personnalisation de l'expérience client et la pertinence des recommandations. Le système de recommandation produits fonctionne en batch quotidien sur la base des données de navigation de la veille, alimentant un modèle collaborative filtering entraîné chaque nuit sur un serveur GPU dédié. L'équipe data compte 4 data engineers, 2 data scientists et 1 ML engineer, opérant sur une stack AWS (RDS PostgreSQL, S3, EC2) sans infrastructure de streaming.

### Problème
Le moteur de recommandation batch génère des suggestions obsolètes : un client ayant acheté un article le matin se voit encore recommander ce même article le soir, créant une expérience perçue comme générique et non personnalisée. Le taux de clic sur les recommandations stagne à 2,1 % et le panier moyen associé aux suggestions est de 38 EUR, bien en dessous de la cible de 55 EUR fixée par la direction e-commerce. Les pics de trafic lors des ventes flash (x8 le trafic normal, soit jusqu'à 2 800 sessions simultanées) provoquent des pertes de données de navigation estimées à 15 % des événements, car le système de collecte batch n'est pas dimensionné pour absorber ces volumes. Le CTO a identifié ce chantier comme la priorité technique n°1 pour le prochain semestre, avec un budget de 180 000 EUR et une échéance ferme de 5 mois avant les soldes d'été.

### Approche
1. **Collecte événementielle temps réel** : Déploiement d'Apache Kafka (cluster de 6 brokers sur AWS MSK) avec un schéma registry Avro pour capturer les événements de navigation, ajout panier, achat et recherche, soit environ 12 millions d'événements par jour en régime normal. L'implémentation a nécessité une refonte complète du tracking front-end, passant d'un système de tags Google Analytics à un event bus JavaScript maison émettant vers un collecteur Kafka Connect. Le choix de Kafka MSK plutôt que Kinesis a été motivé par la portabilité multi-cloud et la richesse de l'écosystème de connecteurs, après un POC comparatif de 3 semaines.
2. **Traitement stream avec Kafka Streams** : Développement de 5 micro-services de transformation en Java calculant en temps réel les profils de session utilisateur (catégories visitées, fourchette de prix, marques consultées, taux de scroll, temps passé par page) avec une fenêtre glissante de 30 minutes et une latence cible inférieure à 500 ms. Chaque micro-service a été conçu comme un composant indépendant déployé sur Kubernetes, permettant un scaling horizontal granulaire. L'équipe data science a collaboré étroitement avec les data engineers pour définir les 25 features temps réel les plus prédictives, en s'appuyant sur une analyse de feature importance réalisée sur 6 mois de données historiques.
3. **Feature store temps réel** : Alimentation d'un feature store Redis (cluster de 3 nœuds avec 64 Go de RAM) contenant les 25 features clés par utilisateur, consommé directement par le modèle de recommandation via une API de scoring avec un SLA de réponse à 50 ms au p99. Le modèle de recommandation a été adapté pour exploiter un mix de features temps réel (session courte) et batch (historique d'achat long terme), nécessitant un re-training complet du modèle et un A/B test de 4 semaines contre l'ancien système. Le feature store inclut un mécanisme de fallback vers les features batch en cas de latence excessive du pipeline streaming.
4. **Observabilité et résilience** : Mise en place d'un monitoring Prometheus/Grafana sur le consumer lag, le throughput et la latence end-to-end, avec alertes automatiques Slack/PagerDuty et mécanisme de replay depuis les topics Kafka en cas d'incident. Un runbook détaillé de 15 scénarios d'incident a été rédigé et testé via des exercices de chaos engineering mensuels (injection de pannes réseau, arrêt de brokers, saturation mémoire Redis). La rétention Kafka a été configurée à 7 jours pour permettre un replay complet en cas de corruption de données dans le feature store.

### Résultat
- Latence de recommandation passée de 24 h (batch J-1) à 800 ms en moyenne, avec un p99 à 1,2 seconde même en période de pointe
- Taux de clic sur les recommandations augmenté de 2,1 % à 5,8 % (+176 %), dépassant la cible initiale de 4,5 %
- Panier moyen des achats issus de recommandations passé de 38 EUR à 62 EUR (+63 %), générant un revenu incrémental estimé à 2,1 millions d'EUR annualisés
- Zéro perte d'événement mesurée lors des 3 dernières ventes flash (pics à 2 400 événements/seconde), contre 15 % de perte auparavant
- Coût d'infrastructure streaming de 3 200 EUR/mois, largement compensé par le revenu incrémental, soit un ROI supérieur à 50x
- Temps moyen de détection d'incident sur le pipeline réduit à 90 secondes grâce au monitoring proactif, contre des pertes silencieuses non détectées auparavant

### Leçons apprises
- Le passage au temps réel ne concerne pas uniquement la technologie : le modèle de recommandation lui-même doit être repensé pour exploiter des features de session courte — un simple branchement de features temps réel sur un modèle conçu pour le batch dégrade paradoxalement les performances
- Un schéma registry strict dès le départ évite des semaines de debugging liées à des incompatibilités de format entre producteurs et consommateurs ; toute modification de schéma doit passer par un processus de revue formelle avec validation de la rétrocompatibilité
- Le dimensionnement du cluster Kafka doit anticiper les pics de charge avec un facteur x10 par rapport au trafic nominal pour absorber les ventes flash sans dégradation — un sous-dimensionnement initial peut provoquer des pertes de données irrécupérables pendant les moments commerciaux les plus critiques
- Prévoir un mécanisme de fallback automatique vers le système batch en cas de défaillance du pipeline streaming est indispensable en production : les premiers mois, 3 incidents ont nécessité ce fallback, évitant à chaque fois une dégradation visible de l'expérience utilisateur

---

## Cas 3 : Implémentation d'une architecture Data Lakehouse pour un groupe média

### Contexte
Groupe Médias Confluence, conglomérat média français regroupant 3 chaînes TV, 8 stations radio, 12 sites web éditoriaux et une plateforme de streaming comptant 850 000 abonnés. Le groupe emploie 2 300 collaborateurs et réalise 310 millions d'EUR de chiffre d'affaires, dont 45 % en revenus publicitaires, dans un secteur en pleine recomposition sous la pression des GAFAM et des plateformes de streaming internationales. La capacité à monétiser les audiences cross-canal constitue l'avantage concurrentiel principal face aux pure players digitaux. Le groupe génère quotidiennement 2,4 To de données hétérogènes (logs de streaming, données publicitaires programmatiques, audiences TV Médiamétrie, contenus éditoriaux, données CRM abonnés, interactions réseaux sociaux). Deux silos coexistent : un data lake S3 de 480 To sous-exploité administré par l'équipe infrastructure, et un data warehouse Redshift saturé géré par l'équipe BI de 5 personnes.

### Problème
Les analystes média attendent en moyenne 4 jours pour obtenir une vue consolidée des audiences cross-canal, rendant impossible toute optimisation en temps réel de la programmation ou de la régie publicitaire. Le data lake contient 18 mois de données brutes sans catalogage ni gouvernance, rendant 60 % des données inexploitables faute de documentation, de schéma cohérent ou de contrôle de qualité. Le coût combiné S3 + Redshift atteint 52 000 EUR par mois, avec un taux d'utilisation effectif de Redshift de seulement 35 % en raison de requêtes mal optimisées et de tables non partitionnées. La régie publicitaire estime perdre 1,8 million d'EUR par an en revenus publicitaires non optimisés faute de données audience consolidées en temps quasi réel. Le CDO récemment recruté a reçu mandat du COMEX de résoudre cette situation en 12 mois maximum.

### Approche
1. **Architecture lakehouse sur Delta Lake** : Migration vers une architecture lakehouse unifiée avec Delta Lake sur S3, organisée en trois couches (bronze/silver/gold) et orchestrée par Apache Spark sur Databricks, permettant de combiner stockage économique et requêtes analytiques performantes. Le choix de Databricks a été fait après un POC de 6 semaines comparant Databricks, Snowflake et une solution open source Spark/Iceberg, Databricks l'emportant sur la facilité de gestion du streaming et l'intégration native de Delta Lake. L'architecture a été conçue pour supporter à la fois des requêtes interactives (latence < 10 secondes) et des traitements batch lourds (enrichissement ML des profils audience), grâce à la séparation des clusters compute.
2. **Ingestion multi-format unifiée** : Développement de connecteurs standardisés pour les 7 types de sources (API Médiamétrie avec ingestion quotidienne, flux publicitaires programmatiques en JSON temps réel via Kafka, logs Kafka du player streaming à 800 événements/seconde, fichiers CSV CRM, données semi-structurées réseaux sociaux, flux XML de programmation TV, données Parquet issues de l'ancien data lake). Chaque connecteur inclut une validation de schéma à l'ingestion, un mécanisme de dead-letter queue pour les événements malformés, et un monitoring de fraîcheur avec alerte si le délai d'ingestion dépasse le SLA défini par source. L'équipe de 3 data engineers a développé un framework d'ingestion réutilisable en Python/PySpark réduisant le temps de développement d'un nouveau connecteur de 3 semaines à 3 jours.
3. **Couche sémantique et catalogage** : Déploiement d'Unity Catalog pour le catalogage automatique des 340 tables, avec lignage de données end-to-end couvrant le chemin complet de la source brute au KPI affiché dans le dashboard. Contrôle d'accès par rôle métier (éditorial, régie pub, direction générale, direction des programmes) implémenté via des politiques RBAC granulaires au niveau colonne pour les données sensibles (PII abonnés, détail des revenus publicitaires par annonceur). Documentation automatique des métriques d'audience (audience cumulée, part de marché, durée d'écoute, reach, GRP) avec formules de calcul validées par la direction des études et alignées sur les standards Médiamétrie. Un glossaire métier de 180 termes a été co-construit avec les 4 directions métier en 6 ateliers.
4. **Optimisation des performances et des coûts** : Mise en place du Z-ordering sur les colonnes de date et de canal, partitionnement par média et par jour, configuration de l'auto-scaling Databricks avec plafond budgétaire mensuel de 28 000 EUR et alertes à 80 % de consommation. Migration des 35 requêtes récurrentes les plus coûteuses vers des tables matérialisées Delta avec rafraîchissement incrémental, réduisant le temps d'exécution moyen de ces requêtes de 12 minutes à 8 secondes. Mise en place d'un processus de revue mensuelle des coûts par cluster avec identification des requêtes les plus consommatrices et optimisation proactive (réécriture de requêtes, ajustement du partitionnement, archivage des données froides vers S3 Glacier).

### Résultat
- Délai d'obtention d'une vue audience cross-canal réduit de 4 jours à 35 minutes, permettant des ajustements de programmation et de régie en quasi temps réel
- Taux de données exploitables dans le lac passé de 40 % à 94 % grâce au catalogage et à la validation de qualité, ouvrant la voie à des analyses auparavant impossibles (corrélation audience TV / engagement digital)
- Coût mensuel d'infrastructure réduit de 52 000 EUR à 31 000 EUR (économie de 40 %), malgré un volume de données multiplié par 2 sur la période
- Nombre de requêtes analytiques exécutées par les équipes métier multiplié par 6 (de 180 à 1 100 par semaine), signe d'une démocratisation effective de l'accès aux données
- Revenus publicitaires incrémentaux estimés à 1,2 million d'EUR sur les 12 premiers mois grâce à une meilleure segmentation audience cross-canal proposée aux annonceurs
- Temps de développement d'un nouveau pipeline d'ingestion réduit de 3 semaines à 3 jours grâce au framework réutilisable, accélérant significativement la capacité d'intégration de nouvelles sources

### Leçons apprises
- L'architecture en couches bronze/silver/gold impose une discipline de transformation qui résout naturellement les problèmes de qualité des données accumulés dans un data lake non gouverné, mais cette discipline doit être accompagnée de règles strictes sur ce qui a le droit de passer d'une couche à l'autre — sans validation automatisée inter-couches, la couche gold finit par hériter des problèmes de la couche bronze
- Le catalogage et le lignage ne sont pas des fonctionnalités optionnelles : sans eux, un lakehouse reproduit les mêmes travers qu'un data lake classique — l'investissement initial dans Unity Catalog a représenté 20 % du budget projet mais a conditionné 80 % de l'adoption par les métiers
- L'implication des équipes métier dans la définition de la couche gold (métriques, dimensions, grain) est le facteur déterminant du taux d'adoption ; les 6 ateliers de co-construction avec les directions métier ont consommé 15 jours de travail mais ont évité des mois de va-et-vient post-déploiement
- La gestion proactive des coûts cloud est indispensable dès le jour 1 : sans plafond budgétaire et revue mensuelle, les coûts Databricks ont tendance à dériver rapidement (+30 % le deuxième mois dans notre cas), notamment à cause de clusters laissés actifs par les data scientists après leurs expérimentations
