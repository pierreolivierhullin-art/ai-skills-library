# Études de cas — BI, Reporting & Data Governance

## Cas 1 : Mise en place d'un framework de gouvernance des données avec data catalog et monitoring qualité

### Contexte
Assurances Atlantique, compagnie d'assurance mutualiste couvrant 1,6 million d'assurés en IARD et prévoyance, 3 100 collaborateurs, présente dans 14 départements du Grand Ouest avec un réseau de 85 agences. L'entreprise réalise 2,3 milliards d'EUR de primes encaissées et se positionne comme le 3e acteur mutualiste régional, dans un secteur soumis à une pression réglementaire croissante (Solvabilité II, DORA, RGPD) et à une transformation digitale accélérée par la concurrence des insurtechs. L'infrastructure IT repose sur un mainframe AS/400 pour les systèmes cœur métier (gestion des contrats, sinistres), complété par 23 applications métier alimentant un data warehouse Teradata de 4,8 To. L'équipe data de 8 personnes (3 développeurs ETL, 2 analystes BI, 1 DBA, 1 architecte data, 1 data analyst actuariat) opère sans politique formelle de gouvernance des données ; les définitions de KPIs varient selon les directions et aucun catalogue de données n'existe.

### Problème
Le régulateur (ACPR) a émis 3 recommandations lors du dernier contrôle sur place portant sur la traçabilité des données utilisées dans les calculs de solvabilité, avec un délai de mise en conformité de 12 mois sous peine de sanctions financières. En interne, 34 % des rapports réglementaires (QRT Solvabilité II, états ACPR, reporting RGPD) nécessitent des corrections manuelles avant soumission, mobilisant 2 ETP pendant les 3 semaines de chaque clôture trimestrielle. Les équipes actuarielles estiment passer 6 heures par semaine à vérifier la cohérence des données entre systèmes, un temps improductif qui retarde les travaux de provisionnement et de tarification. Cinq définitions concurrentes du "taux de sinistralité" coexistent dans l'organisation (direction technique, direction financière, direction commerciale, actuariat, contrôle de gestion), générant des écarts allant jusqu'à 3,2 points sur un même exercice. Le dernier audit SOX a relevé 8 faiblesses matérielles liées à l'absence de contrôles automatisés sur les données alimentant les états financiers.

### Approche
1. **Organisation de la gouvernance** : Nomination d'un Chief Data Officer rattaché à la direction générale (recrutement externe d'un profil combinant expertise data et connaissance du secteur assurantiel), création d'un comité data governance mensuel réunissant les 7 directeurs métier avec un pouvoir décisionnel réel sur les définitions de métriques et les priorités de qualité. Désignation de 14 data owners (un par domaine métier : sinistres IARD, sinistres prévoyance, souscription, provisions techniques, réassurance, etc.) responsables de la définition des règles métier, et de 28 data stewards responsables de la qualité opérationnelle au quotidien. Un RACI détaillé a été formalisé pour chaque type de décision data (création de règle, exception, escalade), évitant les zones grises qui paralysent habituellement les organisations de gouvernance. Le CDO dispose d'un budget propre de 450 000 EUR et d'une équipe dédiée de 3 personnes.
2. **Déploiement du data catalog** : Implémentation d'un data catalog sur DataHub (choisi après un benchmark de 8 semaines contre Alation, Collibra et Atlan, DataHub l'emportant sur le rapport fonctionnalités/coût et la flexibilité open source) recensant les 1 240 tables du data warehouse, avec documentation collaborative des métadonnées métier par les data stewards, lignage technique automatisé depuis les pipelines dbt couvrant le chemin complet de la source au rapport, et glossaire métier unifié de 320 termes validés individuellement par les data owners lors de 12 ateliers de 2 heures. Le glossaire a résolu le problème des 5 définitions concurrentes du taux de sinistralité en imposant une formule unique validée par la direction technique et le comité d'audit. Chaque terme du glossaire inclut la formule de calcul, la source de vérité, le data owner responsable et la date de dernière validation.
3. **Framework de qualité des données** : Mise en place de Great Expectations pour le monitoring automatisé de la qualité avec 850 règles de validation couvrant la complétude, l'unicité, la fraîcheur, la cohérence inter-tables et la conformité aux plages de valeurs métier, exécutées à chaque chargement de données. Les règles ont été co-définies avec les actuaires et les contrôleurs de gestion pour refléter les exigences réglementaires Solvabilité II. Un score de qualité composite (0 à 100) a été défini par domaine, pondérant les dimensions de qualité selon leur criticité métier (la complétude des données sinistres pèse plus lourd que celle des données marketing). Les anomalies détectées génèrent automatiquement un ticket Jira assigné au data steward concerné avec un SLA de résolution de 24 h pour les anomalies critiques et 5 jours pour les anomalies mineures.
4. **Tableaux de bord de gouvernance** : Création d'un dashboard de pilotage multi-niveaux affichant le score de qualité par domaine (de 0 à 100) avec tendance sur 12 mois glissants, le taux de documentation du catalog (objectif 95 %), le nombre d'anomalies détectées et leur délai de résolution, le taux de respect des SLA de correction, et le lignage des données réglementaires critiques. Ce dashboard est présenté mensuellement au comité data governance pour les actions correctives et trimestriellement au COMEX et au comité d'audit pour le pilotage stratégique. Un rapport spécifique "conformité réglementaire" a été développé pour faciliter les échanges avec l'ACPR, documentant de manière exhaustive la traçabilité des données utilisées dans les calculs de solvabilité.

### Résultat
- Taux de correction manuelle des rapports réglementaires réduit de 34 % à 4 % en 9 mois, libérant 1,5 ETP par clôture trimestrielle pour des travaux d'analyse à valeur ajoutée
- Score moyen de qualité des données passé de 61/100 à 91/100 sur les domaines critiques (sinistres, provisions, solvabilité), avec un objectif de 95/100 à 18 mois
- Temps de vérification des données actuarielles réduit de 6 h à 45 min par semaine (baisse de 87 %), accélérant le cycle de provisionnement de 5 jours par trimestre
- Zéro recommandation du régulateur sur la traçabilité des données lors du contrôle suivant, le rapport de l'ACPR soulignant explicitement la qualité du dispositif de gouvernance mis en place
- Taux de documentation du data catalog atteignant 92 % des 1 240 tables (contre 0 % au départ), avec une cible de 98 % à fin d'année
- Réduction de 65 % des tickets d'incident liés à des incohérences de données entre directions, passant de 23 tickets par mois à 8

### Leçons apprises
- La gouvernance des données ne peut pas être un projet purement IT : le rattachement du CDO à la direction générale et l'implication des directeurs métier au comité de gouvernance sont des conditions sine qua non de réussite — un CDO rattaché à la DSI n'aurait pas eu l'autorité nécessaire pour arbitrer les conflits de définition entre directions
- Démarrer le data catalog par les données réglementaires (celles à fort enjeu de conformité) crée un effet de levier qui facilite l'extension aux autres domaines : la pression du régulateur fournit une motivation externe puissante qui coupe court aux résistances internes au changement
- Le score de qualité affiché par domaine introduit une dynamique de comparaison saine entre directions et accélère la prise en charge des anomalies — cependant, il faut veiller à ce que cette compétition reste constructive en valorisant les progressions plutôt que les scores absolus, sous peine de décourager les domaines partant de plus loin
- Le glossaire métier est le livrable le plus sous-estimé mais le plus impactant du programme : sa construction est politiquement sensible (chaque direction doit accepter que sa définition n'est pas forcément la bonne), mais une fois validé, il élimine des centaines d'heures de débats récurrents et constitue le socle de confiance de tout le dispositif

---

## Cas 2 : Migration d'une BI artisanale Excel vers une plateforme self-service Metabase/Looker

### Contexte
NovaTech Solutions, ESN française de 650 collaborateurs spécialisée en transformation digitale, 78 millions d'EUR de chiffre d'affaires, implantée sur 5 sites (Paris, Lyon, Nantes, Bordeaux, Lille). L'entreprise opère dans un marché du conseil IT très concurrentiel où la rentabilité par projet et l'optimisation du staffing sont les principaux leviers de marge. La gestion financière et le pilotage commercial reposent sur un écosystème de 87 fichiers Excel partagés sur un drive réseau, maintenus par 5 contrôleurs de gestion dont 2 juniors arrivés dans l'année. Les macros VBA critiques ont été développées par un contrôleur de gestion parti 2 ans plus tôt, sans documentation. Le directeur financier et les 12 directeurs de practice consultent des extractions Excel envoyées par email chaque vendredi, avec un délai d'obsolescence de 5 jours dès réception.

### Problème
Les contrôleurs de gestion consacrent 60 % de leur temps à la production de rapports et seulement 40 % à l'analyse à valeur ajoutée, un ratio inversé par rapport aux benchmarks du secteur (35 % production / 65 % analyse). Trois erreurs de formule Excel non détectées au T2 ont conduit à une surestimation du taux de marge projet de 2,3 points, faussant les décisions de staffing sur 4 projets majeurs et entraînant un manque à gagner estimé à 180 000 EUR. Le délai de clôture mensuelle est de 12 jours ouvrés, contre une cible à 5 jours fixée par le CFO et attendue par les directeurs de practice pour piloter leur activité. La traçabilité des calculs est quasi inexistante : en cas de question du commissaire aux comptes, il faut en moyenne 3 heures pour reconstituer la chaîne de calcul d'un indicateur. Deux contrôleurs de gestion ont exprimé leur intention de quitter l'entreprise, lassés du travail répétitif de production de rapports, menaçant la continuité opérationnelle de la fonction finance.

### Approche
1. **Cartographie et rationalisation des besoins** : Audit approfondi des 87 fichiers Excel mené conjointement par le contrôle de gestion et un consultant data pendant 4 semaines, pour identifier les 23 rapports réellement utilisés (consultation documentée) et les 156 indicateurs calculés. Une série de 6 ateliers de priorisation avec les directeurs de practice et la direction financière a permis de réduire le périmètre à 42 indicateurs essentiels, en distinguant les indicateurs de pilotage (consultés hebdomadairement) des indicateurs de clôture (mensuels) et des indicateurs stratégiques (trimestriels). Les 114 indicateurs supprimés étaient soit redondants, soit plus consultés depuis plus de 6 mois, soit calculés différemment par plusieurs personnes. Cette rationalisation a été le moment le plus difficile politiquement, certains directeurs de practice s'accrochant à "leurs" indicateurs spécifiques.
2. **Architecture data et modélisation** : Construction d'un modèle dimensionnel en étoile centré sur les axes projet, collaborateur, client et période dans PostgreSQL, alimenté par des pipelines dbt depuis les 4 sources (ERP Sage X3, CRM HubSpot, outil de staffing StaffingBridge, pointage des temps Timmi), avec rafraîchissement toutes les 4 heures en journée et un rafraîchissement complet nocturne. Le modèle a été conçu avec 8 tables de faits et 12 tables de dimensions, documentées intégralement dans dbt avec tests de cohérence inter-tables. La modélisation a été validée par les 5 contrôleurs de gestion et le CFO lors de 3 revues formelles, garantissant que les formules de calcul des indicateurs sont identiques à celles utilisées historiquement dans Excel (quand elles étaient correctes) ou corrigées de manière documentée.
3. **Déploiement Metabase self-service** : Configuration de Metabase avec 8 dashboards thématiques (P&L mensuel, suivi staffing et TJMO, pipeline commercial, rentabilité par practice, suivi trésorerie, analyse client et concentration, benchmark consultants et pyramide, indicateurs RH et turnover), formation de 35 utilisateurs clés en 3 vagues de 4 heures sur chaque site. Le choix de Metabase (plutôt que Looker, Power BI ou Tableau) a été motivé par la simplicité d'utilisation pour des profils non techniques, le coût de licence nul (version open source) et la possibilité pour les contrôleurs de gestion de créer eux-mêmes de nouvelles questions sans intervention technique. Chaque dashboard a été conçu avec un "executive summary" en haut de page (3-4 KPIs en grand format) suivi du détail, pour répondre au besoin de lecture rapide des directeurs de practice.
4. **Gouvernance des métriques et adoption** : Mise en place d'un dictionnaire de métriques intégré à Metabase (définition, formule, source, responsable, date de dernière validation) accessible depuis chaque graphique, certification officielle des dashboards validés par le contrôle de gestion (badge "certifié CDG" visible sur le dashboard), et process de demande d'évolution via un formulaire standardisé Jira avec SLA de 5 jours ouvrés. Un "bureau des métriques" mensuel de 1 heure réunit le CFO, 2 contrôleurs de gestion et 2 directeurs de practice pour arbitrer les demandes d'évolution, ajouter de nouveaux indicateurs ou retirer ceux devenus obsolètes. Un plan de décommissionnement progressif des fichiers Excel a été établi sur 3 mois, avec suppression des accès au drive réseau une fois la migration validée domaine par domaine.

### Résultat
- Part du temps des contrôleurs de gestion consacré à l'analyse passée de 40 % à 75 % (production de rapports réduite de 60 % à 25 %), alignant NovaTech sur les benchmarks du secteur
- Délai de clôture mensuelle réduit de 12 jours à 4,5 jours ouvrés, en dessous de la cible de 5 jours fixée par le CFO
- Zéro erreur de calcul détectée sur les 6 mois suivant le déploiement (contre 3 erreurs majeures au T2 précédent), grâce aux tests automatisés dbt et à la source de vérité unique
- Taux d'adoption de Metabase à 83 % des utilisateurs cibles après 3 mois (mesure : connexion hebdomadaire), avec un pic à 91 % en période de clôture
- Les 2 contrôleurs de gestion menaçant de partir ont confirmé leur engagement, citant la transformation de leur rôle comme facteur déterminant de leur décision de rester
- Économie annuelle estimée à 65 000 EUR (temps de production de rapports économisé + élimination des erreurs + réduction du risque de turnover)

### Leçons apprises
- La rationalisation des indicateurs (de 156 à 42) est l'étape la plus difficile politiquement mais la plus déterminante : chaque indicateur supprimé est une source de confusion en moins et un calcul de moins à maintenir — prévoir 2 à 3 ateliers de plus que prévu pour les négociations, et s'appuyer sur des données d'usage réelles (logs de consultation) pour objectiver les discussions
- La certification officielle des dashboards par le contrôle de gestion crée un cercle vertueux de confiance qui accélère l'abandon des fichiers Excel : les directeurs de practice acceptent de lâcher leur fichier Excel uniquement s'ils ont la garantie que le dashboard a été validé avec le même niveau de rigueur
- Prévoir un budget de 20 % du projet pour l'accompagnement post-déploiement (support, évolutions mineures, formation de rattrapage, communication interne) est essentiel pour maintenir l'adoption dans la durée — dans notre cas, un relâchement de l'accompagnement au mois 4 a provoqué une baisse de 8 points du taux d'adoption, corrigée en 3 semaines par une campagne de réactivation
- Le décommissionnement des fichiers Excel doit être progressif mais ferme : fixer une date de suppression définitive des accès au drive réseau crée l'urgence nécessaire pour que les derniers réfractaires migrent — sans cette contrainte, la coexistence des deux systèmes peut durer indéfiniment et miner la crédibilité du projet

---

## Cas 3 : Projet de Master Data Management pour un groupe industriel multi-entités

### Contexte
Groupe Fonderies Réunies, groupe industriel de 5 600 collaborateurs réparti en 4 filiales (fonderie, usinage, traitement de surface, assemblage) opérant sur 11 sites en France et en Belgique. Le groupe réalise 420 millions d'EUR de chiffre d'affaires avec 2 200 clients actifs, principalement dans les secteurs automobile, ferroviaire et énergie. Issu d'une stratégie de croissance externe menée sur 15 ans (4 acquisitions successives), le groupe n'a jamais harmonisé ses systèmes d'information : chaque filiale dispose de son propre ERP (2 SAP S/4HANA, 1 Oracle E-Business Suite, 1 Sage X3) et de ses propres référentiels clients, fournisseurs et articles. Le directeur général, arrivé 2 ans plus tôt, a fait de l'intégration opérationnelle des filiales sa priorité stratégique, avec l'objectif de dégager 15 millions d'EUR de synergies sur 3 ans, dont une part significative repose sur la consolidation des données de référence.

### Problème
Un même client peut exister sous 3 à 5 identifiants différents selon les filiales, rendant impossible une vue consolidée du chiffre d'affaires client groupe — un handicap majeur pour la force commerciale groupe créée récemment pour développer les ventes croisées entre filiales. L'écart entre le CA client consolidé calculé manuellement (opération mobilisant 3 personnes pendant 2 semaines chaque trimestre) et les données systèmes atteint 8,7 millions d'EUR (2,1 % du CA), un niveau d'imprécision inacceptable pour le COMEX et le conseil d'administration. Les doublons fournisseurs génèrent 340 000 EUR de surcoûts annuels par perte de pouvoir de négociation (un même fournisseur négocié séparément par 3 filiales au lieu de bénéficier d'un effet volume groupe), et 12 % des commandes inter-filiales subissent des erreurs de référencement article entraînant des retards de production moyens de 2,3 jours. Le directeur des achats groupe estime à 1,2 million d'EUR le potentiel de synergies achats bloqué par l'absence de référentiel fournisseur unifié.

### Approche
1. **Diagnostic et périmètre MDM** : Analyse approfondie de la qualité des données de référence sur les 4 ERP, menée par une équipe de 2 consultants MDM et 4 data stewards internes (1 par filiale) sur 8 semaines. Identification de 4 800 doublons clients (taux de 38 %), 1 200 doublons fournisseurs (taux de 22 %) et 15 000 articles sans correspondance inter-filiales (sur un catalogue total de 42 000 références). L'analyse a également révélé des problèmes de complétude critique : 28 % des fiches clients n'ont pas de numéro SIRET, 35 % des fiches fournisseurs ont une adresse incomplète, et les classifications articles diffèrent totalement entre filiales (4 taxonomies incompatibles). Priorisation du projet sur 3 domaines dans l'ordre suivant : clients (impact commercial immédiat), fournisseurs (potentiel de synergies achats), articles (complexité technique la plus élevée).
2. **Plateforme MDM et golden records** : Déploiement d'Informatica MDM comme hub centralisé après un benchmark de 6 semaines contre Stibo STEP et Semarchy, Informatica l'emportant sur la maturité des algorithmes de matching et l'intégration native avec les ERP SAP. Configuration d'algorithmes de matching probabiliste multi-critères (raison sociale normalisée, adresse géocodée, numéro SIRET, téléphone, email) avec un score de confiance de 0 à 100 pour constituer les golden records — une fiche de référence unique par entité. Chaque golden record est validé par les data stewards métier de chaque filiale concernée via un workflow de validation à 4 yeux (proposition automatique, validation data steward filiale d'origine, validation data steward filiale cible, validation finale par le data owner domaine). Le déploiement a été réalisé en 4 mois, avec un environnement de recette mobilisant 8 utilisateurs métier pendant 6 semaines de tests.
3. **Règles de gouvernance et workflows** : Définition de 45 règles de qualité sur les données de référence (complétude SIRET obligatoire pour tout client France, format adresse normalisé selon la norme AFNOR, classification article unifiée selon une taxonomie groupe à 4 niveaux co-construite avec les 4 directeurs techniques de filiale), workflows de création/modification/désactivation avec validation par le data steward du domaine concerné et propagation automatique vers les 4 ERP en temps réel via des connecteurs API. Un comité de gouvernance MDM mensuel réunit les 4 data stewards, le data owner clients, le data owner fournisseurs, le data owner articles et le directeur des achats groupe pour traiter les cas litigieux, faire évoluer les règles et suivre les indicateurs de qualité. Les règles de qualité sont bloquantes à la saisie : il est impossible de créer un nouveau client ou fournisseur sans respecter les critères minimaux de complétude, ce qui empêche la réintroduction de données de mauvaise qualité.
4. **Réconciliation et nettoyage initial** : Campagne de dédoublonnage en 3 phases successives menée sur 4 mois par 8 data stewards (2 par filiale) avec l'appui de 2 consultants. Phase 1 : traitement automatique pour les correspondances avec un score de confiance supérieur à 95 % (2 100 fusions clients réalisées en 2 semaines). Phase 2 : traitement assisté pour les scores entre 80 et 95 % (1 800 cas traités manuellement en 6 semaines, chaque cas nécessitant une vérification métier). Phase 3 : traitement entièrement manuel pour les scores inférieurs à 80 % (900 cas complexes traités en 8 semaines, impliquant souvent des appels aux clients pour confirmer l'identité). L'activation des contrôles de qualité bloquants à la saisie dans chaque ERP a été réalisée en big-bang le même jour sur les 4 filiales, après une période de fonctionnement en mode "alerte sans blocage" de 4 semaines pour permettre aux utilisateurs de s'adapter.

### Résultat
- Nombre de doublons clients réduit de 4 800 à 120 (taux de dédoublonnage de 97,5 %), les 120 restants étant des cas en cours d'investigation nécessitant une vérification terrain
- Vue consolidée du CA client groupe fiable à 99,6 % (écart résiduel de 1,7 million d'EUR contre 8,7 millions précédemment), permettant pour la première fois un classement client groupe exploitable par la force commerciale
- Surcoûts fournisseurs liés aux doublons réduits de 340 000 EUR à 45 000 EUR par an (économie de 87 %), avec un potentiel de synergies achats additionnelles de 800 000 EUR identifié grâce à la visibilité groupe sur les volumes
- Taux d'erreur de référencement article inter-filiales passé de 12 % à 1,8 %, réduisant les retards de production associés de 2,3 jours à 0,4 jour en moyenne
- Temps de consolidation trimestrielle du CA client réduit de 2 semaines (3 personnes) à 2 heures (automatisé), libérant 6 semaines-homme par an pour l'analyse commerciale
- Première campagne de ventes croisées inter-filiales lancée avec succès 3 mois après la mise en production du MDM, générant 1,4 million d'EUR de commandes additionnelles sur le premier semestre

### Leçons apprises
- Le MDM est avant tout un projet organisationnel : 60 % de l'effort porte sur la définition des règles de gouvernance et l'alignement des filiales, 40 % sur la technologie — les filiales historiquement autonomes perçoivent le MDM comme une perte de souveraineté, ce qui nécessite un sponsorship fort de la direction générale et une communication transparente sur les bénéfices partagés
- Le nettoyage initial des données est un investissement massif mais ponctuel (4 mois, 8 data stewards + 2 consultants) ; la vraie valeur réside dans les contrôles de qualité bloquants qui empêchent la réintroduction de doublons — sans ces contrôles, le taux de doublons recommence à croître de 3 à 5 % par mois, annulant le bénéfice du nettoyage en moins d'un an
- Impliquer les directeurs de filiale dès le cadrage en leur montrant l'impact financier des doublons sur leur propre périmètre (et non uniquement sur le groupe) transforme des résistances potentielles en sponsorship actif — la présentation d'un "coût du non-MDM" chiffré par filiale a été le moment de bascule du projet
- Le choix entre une approche big-bang et une approche progressive pour l'activation des contrôles bloquants est critique : l'approche big-bang que nous avons choisie a créé 2 semaines de friction intense (hausse de 40 % des appels au support) mais a évité la dérive d'une coexistence prolongée entre les anciens et nouveaux processus, qui dans notre expérience ne se résout jamais d'elle-même
