# Études de cas — Éthique de l'IA & IA Responsable

## Cas 1 : Détection de biais dans un système IA de recrutement

### Contexte
TalentSphère, éditeur SaaS français de 320 salariés spécialisé dans les solutions RH, commercialise un module de présélection automatique des CV utilisé par 180 entreprises clientes. L'entreprise, fondée en 2017, s'est imposée comme le troisième acteur français du marché ATS (Applicant Tracking System) augmenté par l'IA, derrière deux concurrents américains. L'équipe technique comprend 14 data scientists, 8 ML engineers et un responsable éthique IA recruté en 2023. Le modèle de présélection repose sur un ensemble XGBoost entraîné sur 3,2 millions de CV historiques collectés auprès de 12 partenaires cabinets de recrutement, avec un pipeline d'inférence déployé sur AWS SageMaker. En 2024, une analyse interne déclenchée par un audit annuel de conformité RGPD révèle des écarts statistiques significatifs dans les taux de présélection selon le genre et l'origine géographique des candidats.

### Problème
Le taux de présélection des candidates féminines pour les postes techniques était inférieur de 34 % à celui des candidats masculins à compétences équivalentes. Les candidats issus de certains départements d'outre-mer affichaient un taux de rejet 28 % supérieur à la moyenne nationale. L'analyse approfondie révèle que ces biais s'amplifient depuis 18 mois, corrélés à l'introduction d'un nouveau jeu de données d'entraînement provenant d'un secteur industriel historiquement peu féminisé. Deux clients grands comptes -- un groupe du CAC 40 et une collectivité territoriale -- menacent de résilier leurs contrats (représentant 1,2 M€ de revenus annuels), invoquant le non-respect de leurs engagements RSE. Le Défenseur des droits ouvre une enquête préliminaire suite à la saisine d'une association de promotion de la diversité dans le numérique. L'équipe commerciale signale par ailleurs la perte de 5 appels d'offres majeurs où la question du biais algorithmique a été soulevée par les acheteurs.

### Approche
1. **Audit de biais algorithmique** : Déploiement d'un framework de détection couvrant 12 attributs protégés (genre, âge, origine, handicap, etc.), mobilisant 3 data scientists et 1 juriste pendant 6 semaines. Analyse de 450 000 décisions historiques sur 18 mois avec calcul des métriques de parité statistique et d'égalité des chances. L'audit utilise la bibliothèque Fairlearn de Microsoft ainsi que des outils internes de visualisation développés ad hoc, permettant de cartographier précisément les points de biais dans le pipeline de scoring.
2. **Ré-ingénierie des données d'entraînement** : Suppression des variables proxy corrélées aux attributs protégés (code postal, prénom, photo), identifiées par une analyse de corrélation de Pearson et de mutual information. Rééquilibrage du dataset par sur-échantillonnage stratifié et introduction de 85 000 profils synthétiques générés par des techniques de data augmentation utilisant des modèles génératifs (CTGAN). L'équipe a également mis en place un processus de validation croisée avec des experts RH pour s'assurer que les profils synthétiques restent réalistes et pertinents du point de vue métier.
3. **Implémentation d'un module de débiaisage** : Intégration d'un algorithme de post-processing (Equalized Odds) ajustant les seuils de décision par sous-groupe, déployé comme microservice indépendant dans le pipeline d'inférence. Mise en place de contraintes de fairness directement dans la fonction de coût du modèle via une régularisation de disparité. L'ensemble du module a été testé en parallèle du système existant pendant 4 semaines sur 20 % du trafic avant déploiement complet, assurant une transition sans régression de performance.
4. **Gouvernance et monitoring continu** : Création d'un comité éthique trimestriel incluant 3 représentants clients, 2 experts externes (un sociologue et un spécialiste du droit des discriminations) et 1 juriste spécialisé. Déploiement d'un tableau de bord temps réel surveillant 8 métriques de fairness avec alertes automatiques via Slack et PagerDuty lorsqu'un seuil de dérive est dépassé. Publication trimestrielle d'un rapport de transparence accessible aux clients, détaillant les performances de fairness par attribut protégé.

### Résultat
- Réduction de l'écart de présélection homme/femme de 34 % à 3,2 % en 4 mois
- Élimination du biais géographique (écart résiduel < 1,5 %)
- Rétention des 2 clients grands comptes et signature de 14 nouveaux contrats (+2,8 M€ ARR)
- Clôture de l'enquête du Défenseur des droits sans sanction
- Performance prédictive du modèle maintenue à 91,2 % de précision (vs 91,8 % avant débiaisage), un compromis jugé acceptable par le comité éthique
- Gain de 3 appels d'offres majeurs au second semestre 2024 où la démarche éthique a été un différenciateur décisif face aux concurrents

### Leçons apprises
- Les biais les plus dangereux proviennent souvent de variables proxy apparemment neutres (code postal, école, format du CV) plutôt que de variables directement discriminatoires : une analyse de corrélation systématique doit être intégrée au pipeline de feature engineering dès la conception
- Un audit de biais ponctuel est insuffisant : seul un monitoring continu garantit la pérennité des corrections face à la dérive des données, notamment lorsque les datasets d'entraînement sont régulièrement enrichis avec de nouvelles sources
- L'implication de parties prenantes externes (clients, juristes, société civile) dans la gouvernance éthique renforce la crédibilité et l'acceptabilité des solutions, mais nécessite un investissement en pédagogie pour rendre les métriques techniques accessibles aux non-spécialistes
- La transparence proactive sur les résultats d'audit de biais -- y compris lorsqu'ils sont défavorables -- construit une confiance durable avec les clients : TalentSphère a constaté que son rapport de transparence est devenu un argument commercial clé auprès des grandes entreprises sensibilisées à la diversité

---

## Cas 2 : Explicabilité d'un modèle de scoring crédit pour conformité RGPD

### Contexte
Banque Méridionale, établissement bancaire régional français comptant 2 400 collaborateurs et 1,1 million de clients particuliers, est implantée principalement dans le sud de la France avec 145 agences. Son positionnement de banque de proximité repose sur la relation client personnalisée, ce qui rend l'opacité algorithmique particulièrement problématique pour son identité de marque. L'équipe data science, composée de 9 personnes rattachées à la direction des risques, utilise depuis 2022 un modèle de machine learning (gradient boosting à 280 features) pour le scoring crédit immobilier, traitant environ 42 000 demandes de prêt par an. Le modèle a été développé en interne sur un stack Python/scikit-learn/LightGBM, déployé via une API REST hébergée on-premise. La CNIL effectue un contrôle dans le cadre de l'article 22 du RGPD relatif aux décisions automatisées, après une plainte collective de 28 clients relayée par une association de consommateurs.

### Problème
Le modèle affiche une précision de 94,3 % mais fonctionne comme une boîte noire : les conseillers ne peuvent expliquer aux clients les raisons d'un refus au-delà de formules génériques. Sur les 12 derniers mois, 1 870 réclamations clients portent sur l'opacité des décisions de crédit (hausse de 67 % vs N-1), dont 340 ont fait l'objet d'un recours formel auprès du médiateur bancaire. Le score de satisfaction client sur le processus de crédit a chuté de 7,2/10 à 5,4/10 en 18 mois. La CNIL exige sous 6 mois une mise en conformité avec obligation de fournir une explication individuelle, sous peine d'une amende pouvant atteindre 4 % du chiffre d'affaires (soit ~18 M€). Par ailleurs, l'entrée en vigueur progressive de l'AI Act (classification haut risque pour le scoring crédit) impose des exigences supplémentaires en matière de transparence et de documentation que la banque doit anticiper.

### Approche
1. **Cartographie des exigences réglementaires** : Analyse croisée du RGPD (art. 13-15, 22), de la directive crédit consommation et du futur AI Act (classification haut risque), conduite par un binôme juriste-data scientist sur 3 semaines. Définition de 4 niveaux d'explication : technique (data scientists), opérationnel (conseillers), client (langage naturel), régulateur (documentation formelle). Un benchmark des pratiques d'explicabilité de 8 banques européennes a nourri la définition des standards cibles, permettant d'identifier les meilleures pratiques en matière de communication des décisions automatisées.
2. **Implémentation SHAP et contrefactuels** : Déploiement de SHAP (SHapley Additive exPlanations) pour la décomposition feature-level de chaque décision, avec optimisation du temps de calcul par approximation TreeSHAP (passage de 12 secondes à 0,3 seconde par explication). Développement d'un module de contrefactuels basé sur DiCE (Diverse Counterfactual Explanations) indiquant les modifications minimales nécessaires pour obtenir un résultat favorable (ex. : « avec 8 000 EUR d'apport supplémentaire, votre dossier serait accepté »). Chaque explication est validée par un mécanisme de cohérence qui vérifie qu'aucune suggestion contrefactuelle ne contredit les politiques de risque de la banque.
3. **Interface d'explicabilité pour les conseillers** : Création d'un dashboard intégré au poste de travail affichant les 5 facteurs principaux de chaque décision, avec un score de confiance et des suggestions d'actions correctives. L'interface a été co-conçue avec un panel de 15 conseillers via 4 sprints de design thinking sur 6 semaines. Formation de 640 conseillers sur 3 semaines via un programme combinant e-learning (2h), ateliers pratiques (4h) et accompagnement en situation réelle par des data translators pendant 2 semaines.
4. **Documentation et traçabilité** : Génération automatique de fiches d'explication individuelles au format PDF pour chaque décision, intégrant les facteurs principaux, les pistes d'amélioration contrefactuelles et un QR code pointant vers une page d'information complémentaire. Constitution d'un registre d'explicabilité auditable conservant l'historique de 100 % des décisions sur 5 ans, conforme aux exigences d'archivage réglementaire. L'ensemble de la documentation technique a été structurée selon le format Model Card, facilitant la future mise en conformité AI Act.

### Résultat
- Conformité RGPD validée par la CNIL avec mention « bonnes pratiques » dans le rapport de contrôle
- Réduction des réclamations liées à l'opacité de 67 % en 8 mois (de 1 870 à 620 par an)
- Amélioration du taux de conversion crédit de 12 % grâce aux suggestions d'actions correctives
- Temps d'explication par le conseiller réduit de 25 minutes à 7 minutes par dossier refusé
- Score de satisfaction client sur le processus de crédit remonté de 5,4 à 7,8/10, dépassant le niveau pré-crise
- ROI du projet estimé à 2,1 M€/an en combinant la réduction des recours (économie juridique de 340 K€), l'amélioration de la conversion (1,4 M€ de prêts additionnels) et le gain de productivité des conseillers (360 K€)

### Leçons apprises
- L'explicabilité n'est pas un compromis avec la performance : le modèle a conservé 93,8 % de précision après simplification des features redondantes, contre 94,3 % initialement, une différence statistiquement non significative sur la population cible
- Les explications contrefactuelles (« que faudrait-il changer ? ») sont perçues comme nettement plus utiles par les clients que les explications causales (« pourquoi ce résultat ? ») : dans une enquête auprès de 500 clients, 89 % ont jugé les contrefactuels « très utiles » contre 34 % pour les explications causales seules
- La formation des utilisateurs métier est aussi critique que la technique : sans appropriation par les conseillers, les outils d'explicabilité restent sous-utilisés -- les 6 premières semaines ont montré un taux d'utilisation de seulement 45 % avant que l'accompagnement terrain ne le porte à 92 %
- L'explicabilité génère des insights métier inattendus : l'analyse SHAP a révélé que certaines features historiquement considérées comme essentielles par les analystes crédit avaient en réalité un impact marginal, conduisant à une révision de la politique de scoring traditionnelle

---

## Cas 3 : Évaluation d'impact IA pour un outil de diagnostic médical

### Contexte
MedVision Analytics, start-up deeptech française de 85 salariés spécialisée en imagerie médicale, développe un algorithme de détection précoce de rétinopathie diabétique à partir de photographies du fond d'oeil. Fondée en 2019 par deux ophtalmologues et un ingénieur en vision par ordinateur, l'entreprise a levé 18 M€ en série B auprès de fonds spécialisés medtech. L'équipe technique comprend 22 ingénieurs ML, 6 ophtalmologues-conseil et 3 spécialistes en affaires réglementaires. Le modèle, basé sur une architecture EfficientNet-V2 fine-tunée sur 480 000 images annotées par des ophtalmologues certifiés, est en phase de déploiement pilote dans 12 centres ophtalmologiques en Île-de-France, ciblant 45 000 patients diabétiques par an. Le marché européen de la détection automatisée de rétinopathie est estimé à 320 M€ et compte 4 compétiteurs principaux, dont 2 américains déjà certifiés FDA.

### Problème
L'ANSM (Agence nationale de sécurité du médicament) exige une évaluation d'impact algorithmique complète avant l'autorisation de mise sur le marché en tant que dispositif médical de classe IIa. Les tests internes révèlent une sensibilité de 96,1 % globale, mais tombant à 87,3 % sur les peaux foncées (phototypes V-VI), créant un risque de sous-diagnostic pour 22 % de la population cible. Ce biais est attribué à une sous-représentation des phototypes V-VI dans le dataset d'entraînement (8 % des images vs 22 % de la population diabétique française). Les associations de patients diabétiques, alertées par un article du Monde, expriment publiquement leurs inquiétudes, exerçant une pression médiatique croissante. Le délai réglementaire imposé est de 9 mois, et tout retard compromettrait le lancement commercial prévu pour le T1 2025, laissant le terrain aux concurrents américains.

### Approche
1. **Cadrage éthique et parties prenantes** : Constitution d'un comité d'évaluation pluridisciplinaire de 14 membres (ophtalmologues, patients, bioéthiciens, juristes santé, représentants associatifs), rémunérés pour leur participation afin de garantir un engagement soutenu. Cartographie de 23 risques éthiques selon la méthodologie ALTAI de la Commission européenne, priorisés par gravité et probabilité au cours de 3 ateliers structurés de 4 heures chacun. Un registre des risques a été constitué avec des responsables identifiés pour chaque risque et un plan de mitigation associé, revu mensuellement par le comité.
2. **Audit de performance différentielle** : Évaluation systématique sur 8 sous-populations (âge, sexe, phototype, comorbidités) par une équipe de 4 data scientists et 2 ophtalmologues sur 8 semaines. Collecte complémentaire de 12 000 images de fonds d'oeil de phototypes V-VI auprès de 4 centres hospitaliers partenaires en Afrique de l'Ouest et aux Antilles, dans le cadre d'accords de collaboration incluant un consentement éclairé spécifique. Ré-entraînement du modèle avec dataset enrichi, en utilisant des techniques de curriculum learning et de focal loss pour améliorer la sensibilité sur les sous-groupes sous-représentés.
3. **Protocole de supervision humaine** : Conception d'un workflow hybride IA-médecin avec double lecture obligatoire pour les cas limites (score de confiance < 85 %), développé en co-conception avec 8 ophtalmologues praticiens. Définition de seuils d'alerte spécifiques par sous-population, calibrés sur les courbes ROC respectives pour maintenir un taux de faux négatifs inférieur à 2 % dans chaque sous-groupe. Mise en place d'un mécanisme de recours permettant au patient de demander un examen complémentaire, avec un délai garanti de 48h pour obtenir une seconde lecture humaine.
4. **Documentation AIPD et suivi post-déploiement** : Rédaction d'un dossier d'évaluation d'impact de 180 pages conforme aux exigences de l'AI Act (annexe IV), structuré en 12 chapitres couvrant la description du système, l'analyse des risques, les mesures de mitigation et les indicateurs de suivi. Implémentation d'un système de pharmacovigilance algorithmique avec reporting trimestriel des incidents et des performances par sous-groupe, incluant un mécanisme d'alerte automatique si la sensibilité descend sous 93 % sur toute sous-population pendant une fenêtre glissante de 30 jours.

### Résultat
- Sensibilité sur phototypes V-VI remontée de 87,3 % à 94,8 % après ré-entraînement, écart résiduel < 1,5 points vs population générale
- Obtention du marquage CE classe IIa en 7 mois (2 mois avant le délai imposé)
- Déploiement étendu à 38 centres ophtalmologiques couvrant 120 000 patients/an
- Zéro incident de sous-diagnostic grave signalé sur les 9 premiers mois d'exploitation
- Le dossier d'évaluation d'impact a été cité comme référence par l'ANSM dans ses recommandations aux fabricants de dispositifs médicaux intégrant de l'IA, renforçant le positionnement de MedVision comme leader en IA responsable
- Signature de 3 partenariats internationaux (Allemagne, Espagne, Belgique) accélérée par la qualité de la documentation éthique, les partenaires citant la démarche AIPD comme facteur de confiance déterminant

### Leçons apprises
- L'évaluation d'impact doit commencer dès la phase de conception (by design) et non en fin de développement : les corrections tardives coûtent 3 à 5 fois plus cher, et MedVision estime qu'une intégration dès le début aurait économisé 4 mois et 600 K€ de retravail
- La diversité du dataset d'entraînement est un enjeu éthique fondamental en santé : un biais de représentation se traduit directement en inégalité d'accès aux soins, et les seuils réglementaires de performance doivent être respectés pour chaque sous-population, pas seulement en moyenne
- L'implication de patients et de représentants communautaires dans le comité d'évaluation a permis d'identifier des risques invisibles pour les seuls experts techniques (accessibilité linguistique, confiance culturelle envers l'IA, qualité du consentement éclairé dans les populations vulnérables)
- La documentation rigoureuse de la démarche éthique est un investissement commercial autant qu'un impératif réglementaire : dans le marché du dispositif médical, la traçabilité et la transparence deviennent des avantages concurrentiels mesurables, particulièrement dans le contexte de l'AI Act européen
