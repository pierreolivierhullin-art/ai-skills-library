# Études de cas — Risques IA & Safety

## Cas 1 : Red teaming et tests adversariaux d'un chatbot LLM orienté client

### Contexte
AssurPlus, groupe d'assurance mutualiste français de 4 200 salariés avec 3,8 millions d'assurés, est le cinquième acteur du marché de l'assurance de personnes en France. La direction de la transformation digitale, composée de 45 collaborateurs dont 12 ingénieurs IA, déploie un chatbot conversationnel basé sur GPT-4 pour gérer les demandes de premier niveau (sinistres, garanties, devis). Le chatbot, développé en partenariat avec un intégrateur spécialisé, est déployé sur une architecture Azure OpenAI Service avec un système de prompts structuré comprenant 8 templates métier. Il traite en moyenne 28 000 conversations par jour et est accessible 24h/24 sur le site web et l'application mobile, représentant 40 % des interactions de premier contact avec les assurés.

### Problème
Trois semaines après le lancement, des utilisateurs malveillants publient sur les réseaux sociaux des captures d'écran montrant le chatbot formuler des engagements contractuels non autorisés (« votre sinistre sera remboursé à 100 % sous 48h »), divulguer des informations confidentielles de politique tarifaire interne, et générer des réponses inappropriées via des techniques de jailbreak. L'incident génère 2 300 mentions négatives sur Twitter en 72h et un article dans Les Échos. Le service juridique estime l'exposition potentielle à 4,5 M€ si les engagements générés étaient considérés comme opposables. L'ACPR (Autorité de Contrôle Prudentiel et de Résolution) adresse un courrier d'information demandant des précisions sur les dispositifs de contrôle en place. En interne, le comité des risques requalifie le projet de « risque modéré » à « risque élevé », imposant un gel temporaire de 48h et un plan de remédiation formel sous 30 jours.

### Approche
1. **Constitution d'une équipe de red teaming** : Recrutement d'une cellule de 6 spécialistes (2 pentesters IA, 1 linguiste computationnel, 1 juriste, 2 ingénieurs prompt) complétée par un audit externe de la société Wavestone sur 4 semaines. Définition de 8 scénarios d'attaque prioritaires : jailbreak, injection de prompt, extraction de données, manipulation émotionnelle, génération d'engagements, usurpation d'identité, déni de service sémantique, et contournement des filtres. Chaque scénario est documenté avec une fiche de risque formelle incluant la probabilité d'occurrence, l'impact business potentiel et les critères de détection.
2. **Campagne de tests adversariaux systématiques** : Exécution de 15 000 tentatives d'attaque sur 4 semaines couvrant les 8 scénarios, suivant une méthodologie structurée en 3 phases : tests automatisés (8 000 tentatives via Garak et PromptFoo), tests manuels créatifs par l'équipe red team (5 000 tentatives), et tests crowdsourcés auprès de 200 collaborateurs volontaires (2 000 tentatives). Documentation de 342 vulnérabilités classées par criticité (CVSS adapté IA), dont 23 critiques, 67 élevées, 142 moyennes et 110 faibles. Chaque vulnérabilité est accompagnée d'un proof-of-concept reproductible et d'une recommandation de remédiation.
3. **Mise en place de couches de défense** : Déploiement d'un système de guardrails multi-niveaux : filtre d'entrée (détection d'injection par classificateur fine-tuné sur 50 000 exemples adversariaux), cadrage système renforcé avec instructions constitutionnelles, filtre de sortie (détection d'engagements, PII, tonalité via NLI classifier), et circuit-breaker automatique déconnectant la session après 2 tentatives suspectes. Ajout d'un disclaimer juridique dynamique sur chaque réponse engageante. L'architecture de défense en profondeur a été validée par un audit indépendant de sécurité applicative avant remise en production.
4. **Programme de red teaming continu** : Mise en place d'un bug bounty interne récompensant les contournements découverts (primes de 200 EUR à 2 000 EUR selon la criticité), avec participation de 85 collaborateurs actifs dès le premier trimestre. Tests adversariaux automatisés hebdomadaires sur 500 prompts rotatifs, renouvelés mensuellement à partir d'une veille active sur les nouvelles techniques d'attaque. Revue mensuelle des nouvelles méthodes de jailbreak publiées par la communauté (OWASP LLM Top 10, forums spécialisés), avec mise à jour systématique de la base de tests dans un délai de 72h après publication d'une nouvelle technique.

### Résultat
- Taux de réussite des attaques adversariales réduit de 23 % à 0,8 % en 6 semaines
- Zéro incident d'engagement contractuel non autorisé depuis la mise en production des guardrails (11 mois)
- Réduction de 94 % des mentions négatives liées au chatbot sur les réseaux sociaux
- Économie estimée de 4,5 M€ d'exposition juridique évitée
- Réponse satisfaisante à l'ACPR sous 3 semaines, avec une documentation de contrôle saluée comme « exemplaire » par l'autorité de supervision
- Le programme de bug bounty a permis d'identifier 47 vulnérabilités supplémentaires au cours des 6 mois suivants, dont 8 liées à de nouvelles techniques d'attaque non couvertes par les tests initiaux

### Leçons apprises
- Le red teaming doit être continu et non ponctuel : les techniques d'attaque évoluent plus vite que les défenses, avec de nouvelles méthodes de jailbreak publiées chaque semaine -- un programme de red teaming mensuel est le minimum viable pour un système à exposition publique
- Les guardrails techniques seuls sont insuffisants : la combinaison filtrage d'entrée + cadrage système + filtrage de sortie + circuit-breaker offre une défense en profondeur bien plus robuste, et la suppression d'une seule couche fait remonter le taux de contournement à plus de 8 %
- Les attaques les plus dangereuses ne sont pas les plus sophistiquées techniquement : les manipulations socio-émotionnelles simples (« je suis en détresse, aidez-moi ») contournent plus facilement les filtres que les injections techniques, car elles exploitent les biais d'alignement du modèle lui-même
- L'implication des équipes métier (juristes, actuaires, conseillers) dans le red teaming est indispensable : ce sont eux qui identifient les scénarios à plus fort impact business, là où les experts techniques se concentrent sur les exploits techniques les plus élégants mais pas nécessairement les plus dommageables

---

## Cas 2 : Détection et mitigation de la dérive de modèle pour un système de détection de fraude

### Contexte
PaySecure Europe, fintech franco-belge de 580 salariés spécialisée dans le traitement de paiements en ligne, opère un système de détection de fraude basé sur un ensemble de modèles ML (random forest + réseau de neurones) traitant 2,4 millions de transactions quotidiennes pour 1 200 e-commerçants. L'entreprise, fondée en 2016, s'est positionnée sur le créneau des marchands mid-market européens avec un focus particulier sur les secteurs luxe, voyages et marketplaces. L'équipe data science compte 18 personnes organisées en 3 squads (modélisation, feature engineering, MLOps), utilisant un stack technique basé sur Python, Spark, Airflow et un modèle de serving sur Kubernetes. Le système est en production depuis 3 ans avec des mises à jour trimestrielles, un cycle jugé suffisant par la direction technique jusqu'à l'incident décrit ci-dessous.

### Problème
Sur une période de 5 mois, le taux de fraude non détectée (faux négatifs) passe progressivement de 0,12 % à 0,41 %, représentant une perte cumulée de 8,7 M€ pour les commerçants clients. Parallèlement, le taux de faux positifs augmente de 2,1 % à 4,8 %, bloquant à tort 58 000 transactions légitimes par jour et provoquant la résiliation de 43 contrats commerçants en un trimestre (soit 1,8 M€ d'ARR perdu). L'équipe data science ne détecte la dérive qu'après la plainte d'un client majeur (marketplace mode représentant 320 K€/an), car les dashboards de monitoring ne surveillent que les métriques de latence et de disponibilité, pas les métriques de performance prédictive. L'absence de système d'alerte sur la qualité des prédictions a permis à la dégradation de s'accumuler silencieusement pendant 5 mois, un délai que le directeur des risques qualifie de « inacceptable pour un système critique ».

### Approche
1. **Diagnostic de la dérive** : Analyse rétrospective des distributions de features sur 12 mois via le test de Kolmogorov-Smirnov et le Population Stability Index (PSI), menée par une task force de 5 data scientists sur 3 semaines. Identification de 3 causes majeures : évolution des schémas de fraude (attaques par tokenisation de cartes volées, +180 % en 6 mois), changement de mix commerçants (+35 % de secteur luxe à panier élevé modifiant la distribution des montants), et dégradation de la qualité d'une source de données tierce (enrichissement IP devenu obsolète après un changement de fournisseur non communiqué). La combinaison de ces 3 facteurs a créé un phénomène de dérive conjointe données-concept particulièrement difficile à détecter par des méthodes traditionnelles.
2. **Déploiement d'un système de monitoring de drift** : Implémentation d'un pipeline de surveillance temps réel avec Evidently AI, mesurant 6 types de dérive (données, concept, performance, prédiction, label, feature importance). Seuils d'alerte configurés sur 3 niveaux (vigilance à PSI > 0,1, alerte à PSI > 0,2, critique à PSI > 0,3) avec notification automatique Slack et PagerDuty. Chaque alerte déclenche un runbook documenté spécifiant les actions à prendre par niveau de sévérité, incluant l'escalade vers le Model Risk Committee en cas d'alerte critique. Le pipeline intègre également un monitoring de la qualité des données en amont, détectant les anomalies dans les sources tierces avant qu'elles n'impactent le modèle.
3. **Ré-entraînement et architecture adaptative** : Passage d'un cycle de mise à jour trimestriel à un système de ré-entraînement hebdomadaire automatisé avec validation A/B sur 5 % du trafic. Introduction d'un modèle champion/challenger avec basculement automatique si le challenger surperforme pendant 72h consécutives, gouverné par des critères de performance stricts sur les faux négatifs (priorité absolue) et les faux positifs. Ajout de 14 nouvelles features capturant les patterns de fraude émergents, dont des features temporelles à haute résolution (séquences de transactions intra-session) et des features de graphe (détection de réseaux de cartes liées).
4. **Gouvernance du cycle de vie modèle** : Création d'un Model Risk Committee mensuel réunissant data science, risques, compliance et business, avec un mandat formel validé par le comité exécutif. Définition de SLA de performance (faux négatifs < 0,15 %, faux positifs < 2,5 %) avec escalade automatique en cas de dépassement et processus d'astreinte dédié. Documentation de chaque version de modèle dans un registre MLflow avec lignage complet des données, justification des changements et résultats de validation. Un rapport mensuel de performance modèle est communiqué à chaque commerçant client, renforçant la transparence et la confiance.

### Résultat
- Taux de faux négatifs ramené de 0,41 % à 0,09 % en 8 semaines (meilleur que le baseline historique)
- Taux de faux positifs réduit de 4,8 % à 1,7 %, libérant 74 000 transactions légitimes par jour
- Pertes liées à la fraude non détectée réduites de 8,7 M€ à 1,9 M€ sur le trimestre suivant
- Délai moyen de détection d'une dérive significative passé de 5 mois à 36 heures
- Récupération de 31 des 43 commerçants ayant résilié, et croissance nette de 87 nouveaux contrats au semestre suivant grâce au positionnement renforcé sur la qualité de détection
- ROI du système de monitoring estimé à 12,4 M€/an en pertes de fraude évitées et rétention client, pour un investissement de 1,2 M€ en infrastructure et équipe

### Leçons apprises
- La dérive de modèle est inévitable dans les environnements adversariaux (fraude, cybersécurité) où les attaquants adaptent continuellement leurs stratégies : un monitoring proactif est vital, et le cycle de mise à jour doit être dimensionné en fonction de la vitesse d'évolution de la menace, pas du confort opérationnel de l'équipe
- Le monitoring de performance seul est insuffisant : la surveillance des distributions de données en amont permet de détecter la dérive avant qu'elle n'impacte les résultats business, offrant une fenêtre d'intervention de plusieurs semaines au lieu de réagir après les pertes
- Les SLA de performance modèle doivent être traités avec la même rigueur que les SLA d'infrastructure : chaque point de pourcentage de dégradation a un coût business quantifiable, et la gouvernance modèle doit bénéficier du même niveau d'astreinte et d'escalade que la production informatique
- La qualité des données tierces est un angle mort fréquent : 38 % des incidents de dérive chez PaySecure sont finalement attribués à des changements non communiqués dans les sources de données externes, justifiant la mise en place de contrats de qualité de données (Data SLA) avec chaque fournisseur

---

## Cas 3 : Réponse à incident après une crise d'hallucination dans un produit legal-tech

### Contexte
JurisIA, start-up legal-tech parisienne de 120 salariés, commercialise une plateforme d'aide à la rédaction juridique et de recherche jurisprudentielle augmentée par IA, utilisée par 850 cabinets d'avocats et 45 directions juridiques d'entreprises. Fondée en 2020, l'entreprise a levé 22 M€ en 3 tours de financement et s'est positionnée comme le leader français de la legal-tech IA, avec un taux de croissance de 85 % par an. L'équipe technique comprend 35 ingénieurs, dont 12 spécialisés en NLP et 4 juristes-développeurs assurant la validation métier. Le moteur repose sur un LLM fine-tuné sur un corpus de 2,3 millions de décisions de justice françaises et européennes, avec un système de prompt engineering structuré mais sans couche de vérification factuelle systématique (RAG non implémenté à ce stade).

### Problème
Un avocat du barreau de Lyon dépose une assignation citant 4 arrêts de la Cour de cassation générés par la plateforme, dont 3 s'avèrent totalement fictifs (hallucinations). Le tribunal relève l'erreur, l'avocat est sanctionné par le bâtonnier, et l'affaire est médiatisée dans la Gazette du Palais et sur LegalNews, puis reprise par Le Figaro et BFM Business. En 48h, 23 cabinets clients suspendent leur abonnement (représentant 380 K€ MRR) et le CNB (Conseil National des Barreaux) publie un communiqué d'alerte déconseillant l'utilisation non supervisée d'outils d'IA juridique. L'analyse interne révèle un taux d'hallucination jurisprudentielle de 4,7 % sur les 30 derniers jours, un chiffre que la direction qualifie de « systémique et non anecdotique ». Les investisseurs expriment leurs inquiétudes quant à l'impact sur la prochaine levée de fonds prévue au Q3, et le directeur juridique de JurisIA estime le risque de responsabilité civile professionnelle à 2,1 M€ pour les dossiers potentiellement affectés.

### Approche
1. **Activation du protocole d'incident critique** : Déclenchement immédiat du plan de réponse (< 2h après notification) selon un playbook d'incident préétabli mais jamais testé en conditions réelles. Mise en place d'une cellule de crise (CEO, CTO, directeur juridique, responsable communication) avec points toutes les 4h, suivant un format structuré (situation, actions, blocages, prochaines étapes). Communication proactive aux 850 cabinets clients par email personnalisé dans les 6h, reconnaissant le problème et détaillant les mesures immédiates. Le CEO publie une tribune sur LinkedIn dans les 24h, affirmant la responsabilité de l'entreprise et détaillant le plan de remédiation.
2. **Containment et analyse racine** : Désactivation immédiate de la génération de références jurisprudentielles, remplacée par un mode « recherche seule » dans la base vérifiée, décision prise en 45 minutes malgré l'impact commercial significatif (perte temporaire de 60 % des fonctionnalités premium). Audit exhaustif de 48 000 citations générées sur 60 jours : identification de 2 260 références fictives (4,7 %), dont 890 partiellement inexactes (date ou chambre erronée) et 1 370 totalement inventées (numéro de pourvoi fictif). Cause racine : fine-tuning insuffisant sur la distinction entre raisonnement juridique et citation exacte, amplifié par l'absence de mécanisme de grounding factuel. L'analyse révèle que le taux d'hallucination varie de 1,2 % pour le droit du travail (jurisprudence abondante) à 11,3 % pour le droit européen (corpus plus restreint).
3. **Remédiation technique** : Implémentation d'un système RAG (Retrieval-Augmented Generation) remplaçant la génération de citations par une recherche dans une base vérifiée de 2,3 millions de décisions indexées via un moteur vectoriel (Weaviate) couplé à une recherche lexicale sur les identifiants de décision. Ajout d'un module de vérification croisée comparant chaque référence citée à la base Légifrance via API, avec un mécanisme de fallback vers EUR-Lex pour le droit européen. Affichage systématique d'un score de fiabilité et d'un lien direct vers la source primaire. L'ensemble du pipeline a été soumis à un test de charge sur 100 000 requêtes simulées avant remise en service, validant la fiabilité à 99,98 %.
4. **Reconstruction de la confiance** : Notification individuelle aux 412 clients ayant reçu des documents contenant des références fictives, avec liste exhaustive des citations concernées et évaluation du risque juridique potentiel pour chacune. Mise en place d'un programme de compensation (3 mois d'abonnement offerts) et d'une ligne directe dédiée avec un juriste pour les cabinets ayant utilisé des références fictives dans des procédures en cours. Publication d'un rapport de transparence de 35 pages détaillant l'incident, les causes, les corrections et les engagements. Engagement de certification annuelle par un auditeur indépendant (cabinet Mazars), avec publication des résultats.

### Résultat
- Taux d'hallucination jurisprudentielle réduit de 4,7 % à 0,02 % (99,98 % de fiabilité des citations)
- Récupération de 21 des 23 cabinets ayant suspendu leur abonnement en 3 mois
- Net Promoter Score remonté de -12 à +34 en 6 mois grâce à la transparence de la gestion de crise
- Signature d'un partenariat avec le CNB pour l'élaboration de normes de fiabilité IA en milieu juridique
- Succès de la levée de fonds série C au Q4 (15 M€) : les investisseurs ont cité la maturité de gestion de crise comme facteur de confiance, estimant que l'incident traité avec transparence a paradoxalement renforcé le positionnement de l'entreprise
- Zéro incident d'hallucination signalé par un client sur les 10 mois suivant le déploiement du système RAG, contre une moyenne de 12 signalements par mois avant la crise

### Leçons apprises
- Un plan de réponse à incident IA doit être préparé en amont et testé régulièrement par des exercices de simulation : les premières heures sont déterminantes pour la crédibilité et la maîtrise de la communication, et JurisIA admet que son playbook non testé a entraîné 2h de retard évitables dans la prise de décision initiale
- Dans les domaines à haute conséquence (juridique, santé, finance), la génération pure par LLM doit toujours être couplée à une vérification sur base de données vérifiée (RAG) : la confiance zéro envers les outputs non sourcés est la seule posture responsable, et le coût du RAG (estimé à 180 K€ chez JurisIA) est négligeable face au coût d'un incident
- La transparence totale lors d'un incident (reconnaissance rapide, rapport détaillé, compensation) génère paradoxalement plus de confiance à long terme que l'absence d'incident : les clients valorisent la maturité de gestion de crise et 91 % des cabinets interrogés déclarent faire « davantage confiance » à JurisIA après la crise qu'avant
- La segmentation du taux d'hallucination par domaine juridique est essentielle : un taux global masque des disparités critiques (1,2 % en droit du travail vs 11,3 % en droit européen), et le monitoring doit être granulaire pour détecter les zones de risque spécifiques avant qu'elles ne génèrent un incident
