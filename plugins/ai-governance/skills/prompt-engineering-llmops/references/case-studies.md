# Études de cas — Prompt Engineering & LLMOps

## Cas 1 : Implémentation d'une architecture RAG pour une base de connaissances d'entreprise

### Contexte
Groupe Véolia Technologie Interne (filiale fictive), direction des systèmes d'information d'un groupe industriel français de 12 000 salariés, gère une base documentaire de 340 000 documents techniques (procédures, normes, manuels, retours d'expérience) accumulés sur 15 ans. L'entreprise opère dans le secteur du traitement de l'eau et des déchets, avec 85 sites industriels répartis sur le territoire national, chacun soumis à des réglementations locales spécifiques. L'équipe DSI comprend 180 personnes, dont un pôle data/IA de 14 collaborateurs rattaché au directeur de l'innovation. La recherche documentaire existante repose sur un cluster Elasticsearch vieillissant (v6.8), couplé à un intranet SharePoint 2019 et 4 autres sources non indexées. Les ingénieurs terrain passent en moyenne 47 minutes par jour à rechercher des informations, avec un taux de satisfaction de la recherche interne de seulement 23 %, mesuré par une enquête annuelle auprès de 3 200 collaborateurs.

### Problème
Le moteur de recherche par mots-clés existant (Elasticsearch) ne couvre que 40 % des requêtes en langage naturel des techniciens, les forçant à reformuler leurs questions en combinaisons de mots-clés techniques. Les réponses sont des liens vers des documents de 50 à 200 pages sans pointage vers le passage pertinent, obligeant à une lecture fastidieuse pour trouver l'information recherchée. Le coût estimé de la perte de productivité liée à la recherche documentaire est de 6,2 M€/an, calculé sur la base du temps perdu par 4 800 utilisateurs réguliers valorisé au coût complet. Trois incidents techniques majeurs (coût total : 1,8 M€) ont été attribués à des procédures obsolètes ou introuvables au cours des 18 derniers mois, dont un déversement accidentel ayant entraîné une mise en demeure de la DREAL. La direction générale fixe un objectif de réduction de 60 % du temps de recherche sous 12 mois.

### Approche
1. **Architecture RAG multi-sources** : Conception d'un pipeline ingestion-indexation-retrieval-génération couvrant 7 sources (SharePoint, Confluence, SAP DMS, fichiers réseau, emails archivés, bases réglementaires, vidéos de formation transcrites), nécessitant le développement de 7 connecteurs d'ingestion spécifiques et un travail de normalisation des formats sur 6 semaines. Chunking hybride : découpage sémantique par paragraphe (400-600 tokens) avec chevauchement de 15 %, enrichi de métadonnées (date, auteur, classification, version, site industriel, domaine réglementaire). Vectorisation via un modèle d'embedding multilingue (e5-large-v2) avec index HNSW sur Qdrant, dimensionné pour 18 millions de chunks avec un temps de requête P99 < 50 ms.
2. **Stratégie de retrieval avancée** : Implémentation d'un retrieval hybride combinant recherche vectorielle (cosine similarity) et BM25 pondérés par Reciprocal Rank Fusion, après benchmark de 5 stratégies de fusion sur un set de validation de 800 requêtes annotées. Ajout d'un re-ranker cross-encoder (ms-marco-MiniLM) pour le top-20 des résultats, améliorant le NDCG@5 de 12 points. Filtrage contextuel par métier, site et niveau d'habilitation de l'utilisateur, implémenté via des metadata filters dans Qdrant pour respecter les contraintes de confidentialité industrielle. Recall@10 cible : 92 %. Le pipeline de retrieval complet a été optimisé pour un temps de réponse total < 2 secondes, incluant retrieval, re-ranking et génération.
3. **Prompt engineering et génération** : Conception de 12 templates de prompts spécialisés par type de requête (procédure, diagnostic, réglementation, historique d'incident), développés itérativement sur 4 semaines avec l'aide de 8 experts métier. Implémentation de Chain-of-Thought pour les questions complexes multi-documents, permettant au modèle de synthétiser des informations provenant de sources différentes en explicitant son raisonnement. Citation systématique des sources avec numéro de page et score de confiance, répondant à une exigence forte des ingénieurs qui refusent de faire confiance à une réponse non sourcée. Mécanisme de fallback vers un agent humain si le score de confiance est inférieur à 0,65, avec transfert du contexte de la conversation pour éviter la perte d'information.
4. **Déploiement et boucle de feedback** : Lancement progressif sur 3 sites pilotes (800 utilisateurs) pendant 8 semaines avant généralisation, avec un comité de pilotage hebdomadaire incluant des représentants terrain de chaque site. Collecte de feedback explicite (pouce haut/bas) et implicite (temps passé, clics sur sources, reformulations de requête), alimentant un dataset de 45 000 jugements de pertinence en 3 mois. Ré-indexation incrémentale quotidienne et ré-entraînement mensuel du re-ranker sur les données de feedback, créant une boucle d'amélioration continue qui a fait progresser le Recall@10 de 88 % au lancement à 93,4 % après 3 mois.

### Résultat
- Temps moyen de recherche documentaire réduit de 47 à 11 minutes par jour et par ingénieur
- Taux de satisfaction de la recherche passé de 23 % à 87 % (enquête sur 2 400 utilisateurs)
- Recall@10 mesuré à 93,4 % sur un benchmark de 1 500 questions annotées par des experts métier
- ROI estimé à 4,8 M€/an en gains de productivité et réduction des incidents liés à l'information
- Zéro incident technique lié à une procédure obsolète ou introuvable depuis le déploiement (vs 3 en 18 mois auparavant), soit une réduction du risque opérationnel majeur
- Taux d'adoption de 91 % des utilisateurs cibles après 6 mois, dépassant l'objectif initial de 70 %, grâce à l'implication des experts terrain dans la conception et le pilotage

### Leçons apprises
- La qualité du chunking et des métadonnées détermine 70 % de la performance d'un système RAG : investir massivement dans l'ingénierie de l'indexation avant d'optimiser la génération, car un RAG avec un excellent retrieval et un prompt basique surperforme systématiquement un RAG avec un retrieval médiocre et un prompt sophistiqué
- Le retrieval hybride (vectoriel + lexical + re-ranking) surpasse systématiquement le retrieval purement vectoriel, en particulier pour les requêtes contenant des identifiants techniques ou des codes normatifs (ex. : « NF EN 12566-3 ») qui nécessitent une correspondance exacte impossible par similarité sémantique seule
- Le feedback utilisateur implicite (patterns de navigation) est plus fiable que le feedback explicite (boutons) pour détecter les échecs de retrieval, car les utilisateurs ne signalent que 15 % des mauvaises réponses -- l'analyse des reformulations de requêtes s'est avérée le signal le plus prédictif d'un échec de retrieval
- L'implication des experts métier dès la phase de conception des prompts et du chunking est un facteur de succès critique : les 12 templates de prompts co-conçus avec les ingénieurs ont montré une performance 23 % supérieure aux prompts conçus par l'équipe IA seule, car ils capturent des conventions de langage et des logiques de raisonnement spécifiques au domaine

---

## Cas 2 : Pipeline d'évaluation LLM pour une plateforme d'orchestration multi-modèles

### Contexte
NeuralHub, start-up française de 95 salariés spécialisée dans l'orchestration de modèles d'IA générative, propose une plateforme SaaS permettant à ses 320 entreprises clientes de déployer des workflows combinant plusieurs LLM (GPT-4, Claude, Mistral, Llama) selon les cas d'usage. Fondée en 2022, l'entreprise a levé 14 M€ en série A et se positionne sur un marché en forte croissance estimé à 2,8 Md€ en Europe d'ici 2027. L'équipe technique de 52 personnes inclut 8 ML engineers spécialisés en évaluation, 6 ingénieurs infrastructure et 15 développeurs backend. La plateforme route 1,2 million de requêtes par jour vers le modèle optimal en fonction du coût, de la latence et de la qualité, mais le mécanisme de routage repose jusqu'ici sur des règles statiques définies manuellement par les clients, sans optimisation automatique.

### Problème
L'absence de pipeline d'évaluation standardisé entraîne des choix de routage sous-optimaux : 34 % des requêtes sont envoyées vers des modèles surdimensionnés (coût moyen 3,2x supérieur au nécessaire) tandis que 18 % sont dirigées vers des modèles insuffisants (qualité < seuil client). Les coûts d'inférence explosent à 420 K€/mois (+85 % en 6 mois) sans amélioration proportionnelle de la satisfaction. Les mises à jour fréquentes des modèles par les fournisseurs (OpenAI, Anthropic, Mistral) invalident les règles de routage existantes en quelques semaines, créant un travail de maintenance constant pour l'équipe. Trois clients enterprise (1,1 M€ ARR) menacent de partir vers des solutions mono-modèle plus prévisibles, argumentant que la complexité multi-modèle ne justifie pas son coût si le routage n'est pas intelligent. Le churn rate atteint 8 % par trimestre sur le segment enterprise, contre 3 % chez le concurrent principal.

### Approche
1. **Conception du framework d'évaluation** : Définition de 6 dimensions d'évaluation (exactitude factuelle, cohérence, pertinence, complétude, tonalité, sécurité) avec métriques spécifiques par dimension, formalisées dans un document de spécification de 45 pages validé par 12 clients pilotes. Création d'un benchmark propriétaire de 4 200 cas de test couvrant 14 domaines métier, annotés par 3 évaluateurs humains avec accord inter-annotateur > 0,85 (kappa de Cohen). Le benchmark est versionné et mis à jour mensuellement pour refléter l'évolution des capacités des modèles, avec un processus de curation impliquant 2 linguistes et 3 experts domaine.
2. **Pipeline d'évaluation automatisé** : Implémentation d'un système « LLM-as-Judge » utilisant Claude comme évaluateur calibré sur les annotations humaines (corrélation Spearman > 0,91), après benchmark de 4 modèles évaluateurs et calibration sur 2 000 exemples avec corrections humaines. Pipeline CI/CD dédié intégré à GitHub Actions : chaque nouveau modèle ou mise à jour de prompt est évalué automatiquement sur le benchmark complet en < 2h, avec rapport de régression généré automatiquement. Tableau de bord comparatif en temps réel des performances par modèle, domaine et type de tâche, accessible aux clients via une interface self-service permettant de visualiser le rapport qualité/coût de chaque modèle pour leurs cas d'usage spécifiques.
3. **Routage intelligent basé sur l'évaluation** : Développement d'un routeur ML entraîné sur 850 000 triplets (requête, modèle, score qualité) pour prédire le modèle optimal en < 5 ms, utilisant une architecture lightweight (distilled BERT à 22M paramètres) optimisée pour la latence de serving. Intégration de contraintes budgétaires par client (coût max par requête) et de SLA de qualité (score minimum par dimension), résolues par un algorithme d'optimisation sous contraintes en temps réel. Mécanisme de fallback automatique vers le modèle supérieur si le score prédit est en zone d'incertitude (marge de confiance < 15 %), évitant les dégradations de qualité perceptibles par l'utilisateur final.
4. **Boucle d'amélioration continue** : Échantillonnage quotidien de 500 requêtes production pour évaluation humaine par une équipe de 4 annotateurs dédiés, alimentant le recalibrage mensuel du LLM-as-Judge et la détection de dérive de l'évaluateur lui-même. A/B testing permanent comparant le routeur ML au routage par règles sur 10 % du trafic, avec critères d'arrêt statistiquement rigoureux (test séquentiel de Wald). Reporting hebdomadaire par client du ratio qualité/coût avec recommandations d'optimisation personnalisées, généré automatiquement par un pipeline analytique et revu par un customer success manager dédié aux comptes enterprise.

### Résultat
- Coûts d'inférence réduits de 420 K€ à 185 K€/mois (-56 %) à qualité constante ou supérieure
- Taux de requêtes surdimensionnées passé de 34 % à 7 %, requêtes sous-dimensionnées de 18 % à 2,3 %
- Rétention des 3 clients enterprise menacés et croissance ARR de +42 % sur le semestre suivant
- Temps d'évaluation d'un nouveau modèle réduit de 2 semaines (manuel) à 1h47 (automatisé)
- Churn rate enterprise réduit de 8 % à 2,1 % par trimestre, le routage intelligent devenant le principal argument de rétention cité par les clients
- Capacité de réaction aux mises à jour modèle fournisseur passée de 3-4 semaines (reconfiguration manuelle) à 24h (évaluation automatisée + re-routage), un avantage compétitif majeur dans un marché où les modèles évoluent mensuellement

### Leçons apprises
- L'évaluation LLM est un produit à part entière qui nécessite autant d'investissement que le système principal : les entreprises sous-estiment systématiquement le coût de construction et de maintenance d'un benchmark de qualité -- NeuralHub y consacre 15 % de ses ressources d'ingénierie, un ratio qu'elle juge minimal
- Le LLM-as-Judge est un accélérateur puissant mais ne remplace pas l'évaluation humaine : un échantillonnage humain régulier est indispensable pour détecter les dérives de l'évaluateur automatique lui-même, notamment lors des mises à jour du modèle évaluateur (2 incidents de dérive détectés en 6 mois)
- Le routage multi-modèles n'est rentable qu'à partir d'un volume suffisant (> 100 000 requêtes/jour) : en dessous, la complexité opérationnelle dépasse les économies réalisées -- NeuralHub recommande désormais explicitement le mono-modèle à ses clients à faible volume
- La transparence du benchmark et des métriques d'évaluation vis-à-vis des clients est un facteur de confiance déterminant : les clients enterprise exigent de comprendre comment les décisions de routage sont prises, et le dashboard self-service a réduit de 70 % les tickets de support liés aux interrogations sur la qualité des réponses

---

## Cas 3 : Optimisation du prompt engineering pour un système d'automatisation du support client

### Contexte
Voyagesia, agence de voyage en ligne française de 450 salariés, gère 8 500 demandes de support client par jour (email, chat, téléphone) avec une équipe de 120 conseillers. L'entreprise, fondée en 2012, réalise un chiffre d'affaires de 280 M€ et opère sur un marché de voyage en ligne très concurrentiel où la qualité du service client est un différenciateur critique. L'équipe technique comprend 35 développeurs et 5 data scientists, avec un stack technique reposant sur Zendesk, Salesforce et une infrastructure cloud GCP. En 2024, l'entreprise déploie un système d'automatisation basé sur Claude pour traiter les demandes de niveau 1 (modifications de réservation, informations bagages, réclamations simples), représentant 62 % du volume total. Le déploiement initial est réalisé en 6 semaines avec un prompt unique généraliste, sans optimisation spécifique par type de demande.

### Problème
Après 3 mois de production, le système automatisé affiche un taux de résolution au premier contact de seulement 41 % (objectif : 75 %), un taux d'escalade vers un agent humain de 52 %, et un score CSAT de 3,1/5 sur les interactions automatisées (vs 4,2/5 pour les interactions humaines). Les causes identifiées : prompts génériques ne capturant pas les spécificités métier, mauvaise gestion des cas ambigus, tonalité perçue comme robotique, et absence de personnalisation selon l'historique client. L'impact financier est double : le coût par interaction automatisée (0,85 EUR) est certes inférieur à l'interaction humaine (4,20 EUR), mais les escalades génèrent un surcoût de 6,50 EUR par cas (double traitement). Le ROI négatif du projet (-180 K€ sur le trimestre) menace sa pérennité, le directeur financier ayant fixé un ultimatum de 3 mois pour atteindre un point d'équilibre.

### Approche
1. **Audit et taxonomie des interactions** : Analyse de 25 000 conversations historiques (humaines et automatisées) par clustering sémantique utilisant un pipeline BERTopic personnalisé, sur une durée de 3 semaines impliquant 2 data scientists et 4 conseillers seniors. Identification de 47 intentions distinctes regroupées en 8 catégories (modification, annulation, information, réclamation, urgence, fidélité, technique, divers). Pour chaque intention, annotation de 200 exemples gold-standard par des conseillers seniors, incluant la réponse idéale, le ton attendu et les pièges à éviter, constituant un benchmark de 9 400 cas annotés servant de référence pour toutes les optimisations ultérieures.
2. **Ingénierie de prompts par couches** : Conception d'une architecture de prompts à 4 niveaux : (a) system prompt définissant la persona, les contraintes et les garde-fous (1 200 tokens), développé itérativement sur 12 versions, (b) contexte dynamique injecté par RAG (historique client, détails réservation, politique tarifaire applicable) via une intégration Zendesk/Salesforce temps réel, (c) few-shot examples spécifiques à l'intention détectée (3-5 exemples sélectionnés dynamiquement), (d) instructions de formatage et de tonalité adaptées au canal (chat vs email, avec des différences significatives en longueur et en formalisme). Versionnage systématique de chaque prompt dans un registre dédié (PromptLayer), avec traçabilité complète des modifications et de leur impact sur les métriques.
3. **Optimisation itérative par évaluation** : Mise en place d'un pipeline d'évaluation testant chaque variante de prompt sur le benchmark de 9 400 cas annotés, avec exécution automatisée via un job Airflow dédié. Métriques suivies : exactitude de la réponse, respect des politiques commerciales, tonalité (évaluée par LLM-as-Judge calibré sur 500 annotations humaines), et complétude. Exécution de 23 cycles d'optimisation sur 6 semaines avec des techniques de prompt chaining, self-consistency et structured output, documentant systématiquement les gains marginaux de chaque technique. Un comité de revue hebdomadaire réunissant data scientists et conseillers seniors valide chaque changement de prompt avant déploiement en production.
4. **Personnalisation et adaptation continue** : Implémentation d'un système de sélection dynamique de few-shot examples basé sur la similarité sémantique avec la requête entrante, utilisant un index vectoriel de 4 700 exemples annotés mis à jour quotidiennement. Ajustement du ton selon le segment client (premium vs standard) et le contexte émotionnel détecté par un classificateur de sentiment fine-tuné sur les données du domaine voyage. Boucle de feedback hebdomadaire intégrant les corrections des conseillers humains sur les cas escaladés pour enrichir les exemples de référence, créant un volant d'amélioration qui fait progresser la performance de 1,5 à 2 points de résolution par semaine au cours des 10 premières semaines.

### Résultat
- Taux de résolution au premier contact passé de 41 % à 78 % (+37 points en 10 semaines)
- Taux d'escalade réduit de 52 % à 19 %, libérant 38 conseillers pour les demandes complexes à forte valeur ajoutée
- Score CSAT des interactions automatisées remonté de 3,1 à 4,0/5 (écart avec le support humain réduit à 0,2 point)
- Économie de 1,4 M€/an en coûts opérationnels de support avec amélioration simultanée de la qualité de service
- ROI du projet passé de -180 K€/trimestre à +350 K€/trimestre, atteignant le point d'équilibre au mois 2 de l'optimisation et un ROI cumulé positif à partir du mois 5
- Temps de traitement moyen par interaction automatisée réduit de 4,2 minutes à 1,8 minute, améliorant l'expérience client perçue et la capacité de traitement du système

### Leçons apprises
- Le prompt engineering industriel est un processus d'ingénierie itératif, pas un art créatif : la rigueur du pipeline d'évaluation (benchmark, métriques, versionnage) détermine la vitesse de convergence vers la performance cible -- sans cette infrastructure, les 23 cycles d'optimisation auraient pris 6 mois au lieu de 6 semaines
- L'injection de contexte dynamique (historique client, détails de commande) via RAG a plus d'impact sur la qualité que le raffinement des instructions statiques : un prompt moyen avec un contexte riche surpasse un prompt excellent sans contexte, ce qui justifie de prioriser l'intégration des sources de données avant l'optimisation des formulations
- La sélection dynamique de few-shot examples par similarité est la technique ayant produit le gain marginal le plus important (+11 points de résolution), car elle permet au modèle de s'adapter implicitement à la diversité des situations sans multiplication des prompts -- une approche bien plus maintenable que la création de prompts spécifiques par intention
- L'implication des conseillers humains dans la boucle d'amélioration est le facteur différenciant entre un système qui stagne et un système qui s'améliore continuellement : les corrections apportées par les conseillers sur les cas escaladés constituent la source de données la plus précieuse pour enrichir le benchmark et les few-shot examples, créant un cercle vertueux entre humain et IA
